{"cells": [{"metadata": {"id": "90ace1c0-c50a-47f3-9b35-a8ebed5f473c"}, "cell_type": "markdown", "source": "# Create and Test Scoring Pipeline "}, {"metadata": {}, "cell_type": "markdown", "source": "\n\n\nBefore executing this notebook on IBM Cloud , you need to:<br>\n1) Insert a project token: Click on **More -> Insert project token** in the top-right menu section and run the cell <br>\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n2) Provide your IBM Cloud API key in the subsequent cell<br>\n3) You can then step through the notebook execution cell by cell, by selecting Shift-Enter. Or you can execute the entire notebook by selecting **Cell -> Run All** from the menu.<br>\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Insert IBM Cloud API key\nYour Cloud API key can be generated by going to the [API Keys section of the Cloud console](https://cloud.ibm.com/iam/apikeys). From that page, scroll down to the API Keys section, and click Create an IBM Cloud API key. Give your key a name and click Create, then copy the created key and paste it below. \n\nIf you are running this notebook on cloud pak for data on-prem, leave the ibmcloud_api_key field blank."}, {"metadata": {}, "cell_type": "code", "source": "ibmcloud_api_key = ''", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "try:\n    project\nexcept NameError:\n    # READING AND WRITING PROJECT ASSETS\n    import project_lib\n    project = project_lib.Project() ", "execution_count": 4, "outputs": []}, {"metadata": {"id": "f440ef13-4e1e-4621-8a71-2a2fcc8e1736"}, "cell_type": "markdown", "source": "### Introduction\n\nNow that we have built the machine learning models, stored and deployed them, we can use the models to score new data. \n\nIn the first part of the notebook we will:\n\n* Programmatically get the ID's for the deployment space and model deployment that were created in the **1-model-training** notebook\n* Promote assets required for scoring new data into the deployment space\n* Create a deployable function which will take raw data for scoring, prep it into the format required for the model and score it\n* Deploy the function\n* Create the required payload, invoke the deployed function and return predictions\n"}, {"metadata": {"id": "643ab3d9-1442-449d-9edf-0e3bc930fba2"}, "cell_type": "code", "source": "import pandas as pd\nimport datetime\nfrom ibm_watson_machine_learning import APIClient\nimport os\n\n\n\nif ibmcloud_api_key != '':\n    wml_credentials = {\n        \"apikey\": ibmcloud_api_key,\n        \"url\": 'https://' + os.environ['RUNTIME_ENV_REGION']  + '.ml.cloud.ibm.com'\n    }\nelse:\n    token = os.environ['USER_ACCESS_TOKEN']\n    wml_credentials = {\n        \"token\": token,\n        \"instance_id\" : \"openshift\",\n        \"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n        \"version\": \"3.5\"\n     }\nclient = APIClient(wml_credentials)\n\n", "execution_count": 5, "outputs": []}, {"metadata": {"id": "0dde17b7-0695-427f-ad75-23a3bfb56c78"}, "cell_type": "markdown", "source": "### User Inputs\n\nEnter the path to the csv file with raw data to be scored."}, {"metadata": {"id": "458028a5-ea9d-48d3-878d-e2c73980756d"}, "cell_type": "code", "source": "# specify the location of the csv file with raw data that we would like to score for\nfilename = 'Bill Payment View.csv'\nf = open(filename, 'w+b')\nf.write(project.get_file(filename).getbuffer())\nf.close()", "execution_count": 6, "outputs": []}, {"metadata": {"id": "e1ef21df-e3ca-4bbc-8840-76d35da8e717"}, "cell_type": "markdown", "source": "### Set up Deployment Space, Deployments and Assets\n\nThe following code programmatically gets the deployment space and the model deployment details which were created in **1-model-training**. <br>\n\nWe use the space name and default tags that were used when creating the deployments, as specified below. If multiple spaces with the same name exist, the code will take the space that was created most recently. Similarly, if multiple deployments within the selected space have the same tag, the most recently created deployment is used.\n\nAlternatively, you can manually enter the space and deployment guid's.\n\nThe code also promotes some assets into the deployment space; specifically, the dataset with raw data for scoring, the metadata that was stored in **1-model-training** and the transformer object that prepares the data. After being promoted into the deployment space, these assets are available and can be accessed by the deployed function."}, {"metadata": {"id": "80d87bea-df71-4e32-b972-07aa695d814c"}, "cell_type": "code", "source": "space_name = 'Utilities Payment Risk Prediction Space'\nmodel_name = 'utilities_payment_risk_prediction_model'\ndeployment_name = 'utilities_payment_risk_prediction_model_deployment'\n", "execution_count": 7, "outputs": []}, {"metadata": {"id": "041bfc6b-2511-4381-a34d-24689998876c"}, "cell_type": "markdown", "source": "Get the space we are working in (found using the name that were hardcoded in 1-model-training). If you would like to use a different space, manually set the space_id.\n\nSet the space as the default space for working."}, {"metadata": {"id": "529b4941-a3e7-49c0-b929-392f04cf95e2"}, "cell_type": "code", "source": "l_space_details = []\nl_space_details_created_times = []\nfor space_details in client.spaces.get_details()['resources']:\n    if space_details['entity']['name'] == space_name:\n        space_id=space_details['metadata']['id']\n\n# set this space as default space\nclient.set.default_space(space_id)", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "'SUCCESS'"}, "metadata": {}}]}, {"metadata": {"id": "1ef08492-cbce-402f-856d-b627913a7cee"}, "cell_type": "markdown", "source": "Get the deployment id. If there are multiple deployments with the same name in the same space, we take the latest."}, {"metadata": {"id": "76b92d11-7573-4904-a441-c74dc3a0deec"}, "cell_type": "code", "source": "l_deployment_details = []\nl_deployment_details_created_times = []\n\nfor deployment in client.deployments.get_details()['resources']:\n        \n\n        if deployment['entity']['name'] == deployment_name:            \n                l_deployment_details.append(deployment)\n                l_deployment_details_created_times.append(datetime.datetime.strptime(deployment['metadata']['created_at'],  '%Y-%m-%dT%H:%M:%S.%fZ'))\n                \n\n# get the index of the latest created date from the list and use that to get the deployment_id\nlist_latest_index = l_deployment_details_created_times.index(max(l_deployment_details_created_times))\ndeployment_id = l_deployment_details[list_latest_index]['metadata']['id']\nprint(\"Deployment ID of\",deployment_name,\"is\",deployment_id)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Deployment ID of utilities_payment_risk_prediction_model_deployment is c99e10a9-1f0e-47f1-a6d9-895a7a409ecf\n", "name": "stdout"}]}, {"metadata": {"id": "d7290851-303e-42d0-8c69-3fef20f5a135"}, "cell_type": "markdown", "source": "Promote the assets into the deployment space. The transformer which preps the data is promoted into the space. We also store the raw data dataset in the deployment space."}, {"metadata": {"id": "fe73870e-968e-4870-a0ef-86ee34bffc1d"}, "cell_type": "code", "source": "# get the transformer object created in training - this file was saved as .txt so that the mimetype could be recognised when creating the asset\ntransformer_asset_details = client.data_assets.create('preprocessor_transformer.joblib', file_path='preprocessor_transformer.txt')\n\ndataset_asset_details = client.data_assets.create(filename, file_path=filename)\n\ntransformer_id = transformer_asset_details['metadata']['guid']\ndataset_id = dataset_asset_details['metadata']['guid']", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "Creating data asset...\nSUCCESS\nCreating data asset...\nSUCCESS\n", "name": "stdout"}]}, {"metadata": {"id": "b2a808a3-30c0-4368-b54f-98562acb7d96"}, "cell_type": "markdown", "source": "## Create the Deployable Function\n\nFunctions can be deployed in Watson Machine Learning in the same way models can be deployed. The python client or REST API can be used to send data to the deployed function. Using the deployed function allows us to prepare the data and pass it to the model for scoring all within the deployed function.\n\nWe start off by creating the dictionary of default parameters to be passed to the function. We get the ID of the assets that have been promoted into the deployment space. We also add the model deployment ID and space ID into the dictionary."}, {"metadata": {"id": "c6e6c414-8ed7-44a2-896f-d4403c6074ea"}, "cell_type": "code", "source": "assets_dict = {'dataset_asset_id' : dataset_id, 'dataset_name' : filename, 'transformer_asset_id' : transformer_id}", "execution_count": 12, "outputs": []}, {"metadata": {"id": "d36db039-f562-4aa1-9db3-c5830c130628"}, "cell_type": "code", "source": "# create the wml_credentials again. After already creating the client using the credentials, the instance_id gets updated to 999\n# re-create the dictionary so that the correct instance_id is used\nif ibmcloud_api_key==\"\":\n    wml_credentials[\"instance_id\"] = \"openshift\"\n\nai_parms = {'wml_credentials' : wml_credentials, 'space_id' : space_id, 'assets' : assets_dict, 'model_deployment_id' : deployment_id}", "execution_count": 13, "outputs": []}, {"metadata": {"id": "bebc981f-f855-42c7-b465-1fe9428c3a30"}, "cell_type": "markdown", "source": "### Scoring Pipeline Function\n\nThe function below takes a customer ID and billing date as payload. It preps the customer raw data, loads the model, executes model scoring and generates predictions."}, {"metadata": {"id": "5d3fd4cb-2eb3-48b3-b3e3-267bdd4e902b"}, "cell_type": "code", "source": "def scoring_pipeline(parms=ai_parms):\n     \n    import pandas as pd\n    import requests\n    import os\n    import json\n    import joblib\n    \n    from ibm_watson_machine_learning import APIClient\n\n    client = APIClient(parms[\"wml_credentials\"])\n    client.set.default_space(parms['space_id'])\n\n    # call the function to download the stored dataset asset and return the path\n    dataset_path = client.data_assets.download(parms['assets']['dataset_asset_id'], parms['assets']['dataset_name'])\n    df_raw = pd.read_csv(dataset_path)\n    \n    # call the function to download the transformer joblib file and return the path\n    transformer_object_path = client.data_assets.download(parms['assets']['transformer_asset_id'], 'preprocessor_transformer.joblib')\n    fitted_preprocessor = joblib.load(transformer_object_path)\n    \n    # get the ratio of the data passed in for this month vs the previous month and vs the average of the lookback window\n    # eg how does this month's bill compare to the average for the previous month?\n    def cur_month_vs_historical_summary(df, col, customer_id_col, lookback_window):\n        # how does this month's data compare to the previous month?\n        df[col + '_PREVIOUS_MONTH'] = df.groupby(customer_id_col)[col].shift(1)\n        df['RATIO_THIS_MONTH_' + col + '_VS_LAST_MONTH'] = df[col] / df[col + '_PREVIOUS_MONTH']\n        # how does this month's data compare to the average of the lookback window?\n\n        # get the average of the lookback window\n        df[col + '_AVG_LOOKBACK_WINDOW'] = df.groupby(customer_id_col)[col].shift(1).rolling(lookback_window).mean()\n        df['RATIO_THIS_MONTH_' + col + '_VS_AVG_LOOKBACK_WINDOW'] = df[col] / df[col + '_AVG_LOOKBACK_WINDOW']\n\n        df.drop([col + '_AVG_LOOKBACK_WINDOW', col + '_PREVIOUS_MONTH'], axis=1, inplace=True)\n\n        return df\n\n    def prep_data(cust_id, scoring_billing_month, user_inputs):\n\n        target_col = user_inputs['target_col']\n        overdue_balance_col = user_inputs['overdue_balance_col'] \n        billing_date_col  = user_inputs['billing_date_col']\n        customer_id_col = user_inputs['customer_id_col'] \n        lookback_window = user_inputs['lookback_window']\n        l_cols_to_summarise = user_inputs['l_cols_to_summarise']  \n        \n        # filter by customer id\n        df_prep = df_raw[df_raw[customer_id_col] == cust_id]\n\n        # create the column for billing month\n        df_prep['BILLING_MONTH'] = df_prep[billing_date_col].astype(str).str[0:6]\n        df_prep['BILLING_MONTH'] = df_prep['BILLING_MONTH'].astype(int)\n        \n        # if any records exist for after the billing month, remove then\n        df_prep = df_prep[df_prep['BILLING_MONTH']<=scoring_billing_month]\n        \n        # sort by billing date\n        df_prep = df_prep.sort_values('BILLING_MONTH')\n    \n        # shift the overdue balance back 1 record per customer to create our target variable\n        # we need this variable for previous months so we can calculate if the customer missed payments in the lookback window\n        df_prep[target_col] = df_prep.groupby(customer_id_col)[overdue_balance_col].shift(-1)\n\n        # if we don't know if the customer missed their next payment we will just fill with 0\n        df_prep[target_col] = df_prep[target_col].fillna(0)\n        \n        df_prep.loc[df_prep[target_col] != 0, target_col] = 1\n    \n        # create summary variables\n        for col in l_cols_to_summarise:\n            df_prep = cur_month_vs_historical_summary(df_prep, col, customer_id_col, lookback_window)\n\n        # how many times as a cusotmer missed a payment in the lookback period?\n        df_prep['NUM_MISSED_PAYMENTS_LOOKBACK_WINDOW'] = df_prep.groupby(customer_id_col)[target_col].shift(1).rolling(lookback_window).sum()\n        # now that we've calculated all of our features based on lookbacks, we can delete the old data\n        # we are only interested in scoring the customer for the specified billing month\n        df_prep = df_prep[df_prep['BILLING_MONTH']==scoring_billing_month]\n        # drop the target column\n        df_prep.drop(target_col, axis=1, inplace=True)\n        # create the variable for the billing month\n        df_prep['BILLING_MONTH_NUMBER'] = df_prep['BILLING_MONTH'].astype(str).str[4:6]\n        # use transformer to prep the data\n        X_postprocess = fitted_preprocessor.transform(df_prep)\n        \n        return X_postprocess\n    \n    def score(payload):\n        import json\n        \n        scoring_billing_date = payload['input_data'][0]['values'][0][1]\n        # convert into int with year and month\n        scoring_billing_month = int(scoring_billing_date[3:7] + scoring_billing_date[0:2])\n        \n        cust_id = payload['input_data'][0]['values'][0][0]\n        \n        # we stored the metadata dictionary when we deployed the model, we retrieve it in the following line of code\n        metadata_dict = client.deployments.get_details(parms['model_deployment_id'])['entity']['custom']  \n        \n        probability_threshold = metadata_dict['probability_threshold']\n        pre_balancing_target_density = metadata_dict['training_data_pre_balancing_target_density']\n        post_balancing_target_density = metadata_dict['training_data_post_balancing_target_density']\n        \n        try:\n            prepped_data = prep_data(cust_id, scoring_billing_month, metadata_dict['user_inputs'])\n        except:\n            return {\"predictions\" : [{'values' : 'Data prep filtered out customer data. Unable to score. Check that the billing date is valid for the input data(e.g. 06-2019).'}]}\n            \n        if prepped_data is None:\n            return {\"predictions\" : [{'values' : 'Data prep filtered out customer data. Unable to score. Check that the billing date is valid for the input data.'}]}\n        elif prepped_data.shape[0] == 0:\n            return {\"predictions\" : [{'values' : 'Data prep filtered out customer data. Unable to score. Check that the billing date is valid for the input data.'}]}\n        else:\n            scoring_payload = {\"input_data\":  [{ \"values\" : prepped_data.tolist()}]}\n            predictions = client.deployments.score(parms['model_deployment_id'], scoring_payload)\n            \n            # we need to adjust the probabilities because we trained on the balanced dataset\n            # also update the predicted class returned based on our threshold\n            # by default the predicted class is based on 0.5 probability, we changed this based on ROC curve\n            for idx, val in enumerate(predictions['predictions'][0]['values']):\n                # adjust the probabilities\n                predictions['predictions'][0]['values'][idx][1][1] = 1/(1+(1/pre_balancing_target_density-1)/(1/post_balancing_target_density-1)*(1/predictions['predictions'][0]['values'][idx][1][1]-1))\n                predictions['predictions'][0]['values'][idx][1][0] = 1 - predictions['predictions'][0]['values'][idx][1][1]\n                if predictions['predictions'][0]['values'][idx][1][1] >= probability_threshold:\n\n                    predictions['predictions'][0]['values'][idx][0] = 1\n                else:\n\n                    predictions['predictions'][0]['values'][idx][0] = 0\n                    \n            predictions[\"Payment Risk\"]=str(round(predictions['predictions'][0]['values'][idx][1][1]*100,2))+\"%\"\n                \n        \n        return {\"predictions\" : [{'values' : predictions}]}\n            \n    return score\n\n\n# scoring_pipeline()({\"input_data\":[{\"fields\":[\"CUSTOMER ID\",\"Billing Date(%mm-%YYYY)\"],\"values\":[[212,\"06-2019\"]]}]})\n", "execution_count": 14, "outputs": []}, {"metadata": {"id": "da7cc8ef-424a-4f6d-8584-5f70488d321c"}, "cell_type": "markdown", "source": "### Deploy the Function\n\nYou can specify the name of the function and deployment in the code below. As we have previously seen, we use tags in the metadata to allow us to programmatically identify the deployed function."}, {"metadata": {"id": "d37ed9c8-8026-4ffd-bed0-4c7d247bfe30"}, "cell_type": "code", "source": "# store the function and deploy it \nfunction_name = 'utilities_payment_risk_prediction_scoring_pipeline_function'\nfunction_deployment_name = 'utilities_payment_risk_prediction_scoring_pipeline_function_deployment'", "execution_count": 15, "outputs": []}, {"metadata": {"id": "61d927bf-2d38-4970-b11c-f38f248d83b7"}, "cell_type": "markdown", "source": "We use tags, input data schemas, output data schemas and software specifications in the metadata to store the function. Input data schemas provide an easy option to input data to score in the deployment space. Example metatadata to store the function can be viewed using `client.repository.FunctionMetaNames.get_example_values()`. Similarly, example metatadata to deploy the function can be viewed using `client.deployments.ConfigurationMetaNames.get_example_values()`.\n\nThe Software Specification refers to the runtime used in the Notebook, WML training and WML deployment. We use the `default_py3.7` software specification to store the function. We get the ID of the software specification and include it in the metadata when storing the function. Available Software specifications can be retrieved using `client.software_specifications.list()`."}, {"metadata": {"id": "117b85b4-cb6a-4570-814e-f04a3f93bd72"}, "cell_type": "code", "source": "software_spec_id = client.software_specifications.get_id_by_name(\"default_py3.7\")", "execution_count": 16, "outputs": []}, {"metadata": {"scrolled": false, "id": "bdae474a-3aeb-42ec-ae73-219af6b7e5ac"}, "cell_type": "code", "source": "# add the metadata for the function and deployment    \nmeta_data = {\n    client.repository.FunctionMetaNames.NAME : function_name,\n    client.repository.FunctionMetaNames.TAGS : ['utilities_payment_risk_prediction_scoring_pipeline_function_tag'],\n    client.repository.FunctionMetaNames.INPUT_DATA_SCHEMAS:[{'id': '1','type': 'struct','fields': [{'name': 'CUSTOMER ID', 'type': 'int'},{'name': 'Billing Date', 'type': 'MM-YYYY'}]}],\n    client.repository.FunctionMetaNames.OUTPUT_DATA_SCHEMAS: [{'id': '1','type': 'struct','fields': [{'name': 'Payment_Risk','type': 'double'}]}],\n    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: software_spec_id\n}\n\nfunction_details = client.repository.store_function(meta_props=meta_data, function=scoring_pipeline)\n\nfunction_id = function_details[\"metadata\"][\"id\"]\n\nmeta_props = {\n    client.deployments.ConfigurationMetaNames.NAME: function_deployment_name,\n    client.deployments.ConfigurationMetaNames.TAGS : ['utilities_payment_risk_prediction_scoring_pipeline_function_deployment_tag'],\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\n# deploy the stored model\nfunction_deployment_details = client.deployments.create(artifact_uid=function_id, meta_props=meta_props)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '753b8073-0f76-437c-9cf6-07fba281262b' started\n\n#######################################################################################\n\n\ninitializing.........\nready\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='83fabc06-4ef8-48b9-98d7-9d8946378ba3'\n------------------------------------------------------------------------------------------------\n\n\n", "name": "stdout"}]}, {"metadata": {"id": "ec6dda86-4a23-46c7-a36f-ba6e0224da65"}, "cell_type": "markdown", "source": "### Score New Data\n\nGet the guid of the deployed function, create the payload and use the python client to score the data. The deployed function returns the classification prediction along with the probabilities.\n\nThe payload contains two values. The first is the billing date for scoring. This is the date that a bill would be created; the amount and usage to be billed for are known at this point. We pass it in in the **month number - year** format. The second value contains the ID of the customer who we would like to make the prediction for."}, {"metadata": {"id": "b15eb3e9-8967-4bda-80fe-2d26c6381a77"}, "cell_type": "code", "source": "scoring_deployment_id = client.deployments.get_uid(function_deployment_details)\n\n# date should be %m-%Y format\npayload = [{\"fields\":[\"CUSTOMER ID\",\"Billing Date(%mm-%YYYY)\"],\"values\":[[217,\"06-2019\"]]}]\n\npayload_metadata = {client.deployments.ScoringMetaNames.INPUT_DATA: payload}\n# score\nfunct_output = client.deployments.score(scoring_deployment_id, payload_metadata)\nfunct_output", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "{'predictions': [{'values': {'predictions': [{'fields': ['prediction',\n       'probability'],\n      'values': [[1, [0.274550050596425, 0.725449949403575]]]}],\n    'Payment Risk': '72.54%'}}]}"}, "metadata": {}}]}, {"metadata": {"id": "def91844-ca6d-44a1-b32f-7085b2501a6b"}, "cell_type": "markdown", "source": "**The R Shiny Dashboard invokes this scoring pipeline for visualizing the results.**<br>\n**Follow the instructions from Readme to launch the R-Shiny dashboard.**\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\nSample Materials, provided under <a href=\"https://github.com/IBM/Industry-Accelerators/blob/master/CPD%20SaaS/LICENSE\" target=\"_blank\" rel=\"noopener noreferrer\">license.</a> <br>\nLicensed Materials - Property of IBM. <br>\n\u00a9 Copyright IBM Corp. 2020, 2021. All Rights Reserved. <br>\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}