{"cells": [{"metadata": {"id": "d0860e89-b110-4a8e-95ea-fcd0efc483ef"}, "cell_type": "markdown", "source": "# Industry Accelerators - Customer Life Event Prediction Models"}, {"metadata": {"id": "dd4bd356-560f-4aae-bcc5-095dce2bb6ca"}, "cell_type": "markdown", "source": "## Introduction\nIn this notebook we'll be going through an end-to-end project to load in long form transactional type data and prepare the data into a wide format. Our long form data contains a feature named `EVENT_TYPE_ID` which contains events that the customer has experienced. In this project, we're specifically looking for the events with the prefix `LFE_`. These events are significant <b>Life Events</b> that have been experienced by the client. We're going to be specifically targeting two life events for this project: `LFE_RELOCATION` and `LFE_HOME_PURCHASE`. Therefore, we are going to build some machine learning models to help predict the likelihood of these two events occurring for our clients - which is in the next notebook (called 2-model-training.ipynb).\n\nAdditionally the user also has the option to incorporate `Census` data generated probabilities for migration, birth, marriage and birth into the dataset that is used for modeling.  \n\nBefore executing this notebook on IBM Cloud :<br>\n1) When you import this project on an IBM Cloud environment, a project access token should be inserted at the top of this notebook as a code cell. <br>\nIf you do not see the cell above, Insert a project token: Click on **More -> Insert project token** in the top-right menu section and run the cell. <br>\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n2) You can then step through the notebook execution cell by cell, by selecting Shift-Enter. Or you can execute the entire notebook by selecting **Cell -> Run All** from the menu.<br>\n"}, {"metadata": {}, "cell_type": "code", "source": "try:\n    project\nexcept NameError:\n    # READING AND WRITING PROJECT ASSETS\n    import project_lib\n    project = project_lib.Project() ", "execution_count": 3, "outputs": []}, {"metadata": {"id": "3dc45fa9-b456-424c-8dda-b038f54b0139"}, "cell_type": "markdown", "source": "## Load Event Data\nFor this project, we will be loading in the long form data called `event.csv`. You can execute this notebook with the sample data provided with the project. We will use `project_lib` to read the data."}, {"metadata": {"id": "0dc11df0-639f-486b-87e2-4f65c615f5d1"}, "cell_type": "markdown", "source": "**Sample Materials, provided under license. <br>\nLicensed Materials - Property of IBM. <br>\n\u00a9 Copyright IBM Corp. 2019, 2020. All Rights Reserved. <br>\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.<br>**"}, {"metadata": {"id": "0fc374f1-911c-41a3-a31f-f109874b8f5c"}, "cell_type": "code", "source": "!pip install chart_studio\nimport pandas as pd\nimport time\npd.set_option('display.max_columns', 500)\nfrom chart_studio.plotly import iplot\nimport plotly as py\npy.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport pickle", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting chart_studio\n  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64 kB 5.4 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: plotly in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from chart_studio) (4.8.2)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from chart_studio) (2.24.0)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from chart_studio) (1.15.0)\nRequirement already satisfied: retrying>=1.3.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from chart_studio) (1.3.3)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->chart_studio) (1.25.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->chart_studio) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->chart_studio) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->chart_studio) (3.0.4)\nInstalling collected packages: chart-studio\nSuccessfully installed chart-studio-1.1.0\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/html": "        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "}, "metadata": {}}]}, {"metadata": {"id": "b7a0d34f-dfc1-4d76-ae7e-4b6ecb99edc7"}, "cell_type": "code", "source": "# Read event data from a CSV file.\nevents = pd.read_csv(project.get_file('event.csv'), parse_dates=['EVENT_DATE'], dayfirst=True)\n    \n# display the event data\nprint('\\nEvent Data:')\ndisplay(events.head())\nprint(\"{} rows, {} columns\".format(*events.shape))", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "\nEvent Data:\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "   CUSTOMER_ID EVENT_DATE       EVENT_TYPE_ID\n0         1103 2013-02-02  INT_OTHER_PHYSICAL\n1         1103 2013-02-02    XCT_MORTGAGE_NEW\n2         1769 2013-02-03  INT_OTHER_PHYSICAL\n3         1769 2013-02-03    XCT_MORTGAGE_NEW\n4         1879 2013-02-15    XCT_MORTGAGE_NEW", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSTOMER_ID</th>\n      <th>EVENT_DATE</th>\n      <th>EVENT_TYPE_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1103</td>\n      <td>2013-02-02</td>\n      <td>INT_OTHER_PHYSICAL</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1103</td>\n      <td>2013-02-02</td>\n      <td>XCT_MORTGAGE_NEW</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1769</td>\n      <td>2013-02-03</td>\n      <td>INT_OTHER_PHYSICAL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1769</td>\n      <td>2013-02-03</td>\n      <td>XCT_MORTGAGE_NEW</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1879</td>\n      <td>2013-02-15</td>\n      <td>XCT_MORTGAGE_NEW</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}, {"output_type": "stream", "text": "103426 rows, 3 columns\n", "name": "stdout"}]}, {"metadata": {"id": "207368f1-34fa-40e8-b892-a2698b3a9498"}, "cell_type": "markdown", "source": "The `events.csv` data which we loaded has only 3 columns and 103,426 rows. The three main columns to focus on are `CUSTOMER_ID`, `EVENT_DATE`, and `EVENT_TYPE_ID`. This data represents a list of the client experiencing specific events on a given date. We'll take this data and make counts of the events while focusing on the two target life events: <b>home purchase and relocation,</b>."}, {"metadata": {"id": "3e0e9042-e3cf-4374-9918-f021b9e6d7e8"}, "cell_type": "markdown", "source": "### Display Distinct Event Types in Data"}, {"metadata": {"id": "564fb15c-e9c5-4090-83ce-cf7c95cc0d6f"}, "cell_type": "markdown", "source": "All of the unique events can be found in the dataset under the feature `EVENT_TYPE_ID`. The wide format dataset will be created from this list of unique events. Additionally, this creates flags and counts on whether a client experienced the event."}, {"metadata": {"id": "a1060cf5-c63d-40ac-811d-d81025bb103d"}, "cell_type": "code", "source": "events['EVENT_TYPE_ID'].unique()", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "array(['INT_OTHER_PHYSICAL', 'XCT_MORTGAGE_NEW',\n       'MENTION_LFE_HOME_PURCHASE', 'XCT_EQ_SELL', 'XFER_FUNDS_OUT_LARGE',\n       'LFE_RELOCATION', 'LFE_HOME_PURCHASE', 'ACNT_SEC_OPEN_*',\n       'BIRTHDAY30', 'INT_LOGIN_WEB', 'BIRTHDAY77', 'BIRTHDAY36',\n       'BIRTHDAY51', 'BIRTHDAY59', 'BIRTHDAY22', 'XCT_EQ_BUY',\n       'BIRTHDAY55', 'BIRTHDAY47', 'BIRTHDAY58', 'BIRTHDAY43',\n       'BIRTHDAY35', 'BIRTHDAY67', 'BIRTHDAY27', 'BIRTHDAY32',\n       'BIRTHDAY63', 'BIRTHDAY24', 'BIRTHDAY34', 'BIRTHDAY80',\n       'BIRTHDAY21', 'BIRTHDAY82', 'BIRTHDAY29', 'BIRTHDAY81',\n       'BIRTHDAY73', 'BIRTHDAY42', 'BIRTHDAY53', 'BIRTHDAY70',\n       'BIRTHDAY28', 'BIRTHDAY40', 'BIRTHDAY62', 'BIRTHDAY41',\n       'BIRTHDAY74', 'BIRTHDAY69', 'BIRTHDAY64', 'BIRTHDAY44',\n       'BIRTHDAY71', 'BIRTHDAY76', 'BIRTHDAY23', 'BIRTHDAY68',\n       'BIRTHDAY38', 'BIRTHDAY79', 'BIRTHDAY78', 'BIRTHDAY39',\n       'BIRTHDAY37', 'BIRTHDAY45', 'BIRTHDAY54', 'BIRTHDAY25',\n       'BIRTHDAY56', 'BIRTHDAY60', 'BIRTHDAY49', 'BIRTHDAY48',\n       'BIRTHDAY72', 'BIRTHDAY46', 'BIRTHDAY61', 'BIRTHDAY26',\n       'BIRTHDAY57', 'BIRTHDAY33', 'BIRTHDAY66', 'BIRTHDAY75',\n       'BIRTHDAY50', 'BIRTHDAY52', 'BIRTHDAY83', 'BIRTHDAY31',\n       'BIRTHDAY65', 'BIRTHDAY84', 'ACNT_SEC_CLOSE_*'], dtype=object)"}, "metadata": {}}]}, {"metadata": {"id": "77f13cb2-1323-470d-bb32-5295c2103a39"}, "cell_type": "markdown", "source": "### Select Life Event Types to Predict"}, {"metadata": {"id": "ca2e2158-b88b-4aab-936f-f3ab5822c948"}, "cell_type": "markdown", "source": "As mentioned above, the focus will be on predicting the two life events `LFE_RELOCATION` and `LFE_HOME_PURCHASE` so we'll filter the `EVENT_TYPE_ID` feature to only those events with the life event prefix of `LFE_`."}, {"metadata": {"id": "38c02470-c971-4c68-a9cb-488c01cda78e"}, "cell_type": "code", "source": "# prediction_types = ['LFE_HOME_PURCHASE','LFE_RELOCATION']\nprediction_types = [event_type for event_type in list(events['EVENT_TYPE_ID'].unique()) if event_type[:4] == 'LFE_']\n\nprint(\"\\nEvent Types to Predict:\")\ndisplay(prediction_types)\nprint()", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "\nEvent Types to Predict:\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "['LFE_RELOCATION', 'LFE_HOME_PURCHASE']"}, "metadata": {}}, {"output_type": "stream", "text": "\n", "name": "stdout"}]}, {"metadata": {"id": "40276261-0600-4dae-8376-87ee65291745"}, "cell_type": "markdown", "source": "## User Inputs and Data Prep\n\nSee `/project_data/data_asset/life_event_prep.py` for details of data preparation\n\nThis script generates the dataset which is used as input for model training and scoring purposes. Given a list of events which customers experienced, this script transforms this long form dataset into a wide format, which can be used for modelling.\n\n#### Data Cleaning\nA number of functions are carried out throughout the code for cleaning the data. <br>\n\n\u2022\tAny customer who does not have at least observation_window + forecast_horizon consecutive months of historical data are filtered out of the training dataset <br>\n\u2022\tCustomers who don\u2019t have any events in their observation_window are also removed from the training dataset <br>\n\u2022\tSimilarly, when scoring, any customer who had no events in the observation window is filtered out <br>\n\u2022\tAny life event that doesn\u2019t have at least \u2018life_event_minimum_target_count\u2019 (default 100) unique customers experiencing it is removed and not used as a target variable <br>\n\u2022\tAny column with more than 10% nulls is dropped from the training dataset <br>\n\u2022\tAny columns with a constant value are removed from the training dataset <br>\n\u2022\tThe final scoring dataset is engineered to ensure it has the same columns and order as the training dataset <br>\n\n#### User Inputs for `LifeEventPrep`:\n- **target_event_type_ids :**  A list of life events which we are trying to predict. Single or multiple events can be handled.\n- **train_or_score :** Specifies whether we are prepping the data for training or for scoring. Training data includes the target variable while scoring dataset does not.\n- **training_start_date :** The start date that we start counting events from for training. Variable is a string.\n- **training_end_date :** Cut off date for training events. Any events occurring after this date will not be included in training data. Again, the variable is a string.\n- **forecast_horizon :** The window of time that we want to predict in. This is the number of months after the observation month in which the event can occur.\n- **observation_window :** The lookback period from the observation month. We use the count of number of events which occurred in this window as input variables. Again, this variable is in months.\n- **life_event_minimum_target_count :** To include a particular life event target variable, the variable must have at least this number of unique customers associated with it.\n- **cols_to_drop**: Columns to be dropped due to known irrelevance to target (e.g. ID column)\n- **b_use_census_data**: Boolean variable to allow the user to specify whether they would like to use the supplied census data or not.\n\n\n#### Census Data (optional)\n\nIf you would like to use probabilities generated from USA Census data along with the life events, the `prep_census_data.py` script in the `/project_data/data_asset/` folder should be called by setting `b_use_census_data` to `True`. This script maps each customer in the events data to their most similar customer type in census data, based on customer gender, age range, income, marital status, location, education and employment status. The census data can be found in the `/project_data/data_asset/census_probabilities.csv` file and  contains the following fields:\n\n- **LOCATION :** Location(State) of the Customer\n- **MARITAL_STATUS :**\tMarital status of the customer\n- **EDUCATION :** Customer's Highest level of Education \n- **GENDER :** Gender of the Customer\n- **EMPLOYMENT :** Customer's Employment status\n- **INCOME :** Customer's annual income\n- **AGE\t:** Customer's Age\n- **MIGRATION_PROB :**\tProbability of a Customer to relocate\n- **BIRTH_PROB :**\tProbability of a customer to give birth\n- **MARRIAGE_PROB :** Probability of a customer to get married\n- **DIVORCE_PROB :** Probability of a customer to get divorced\n\nIf you use different customer data, you need to edit the `prep_census_data.py` script and generate the relevant mappings between the new customer details and census data.\n\n*Disclaimer: The census data used in the accelerator is for showcasing the capability of the accelerator. It might not reflect the actual census data.*"}, {"metadata": {"id": "f5e6e273-cd4c-4cb8-a89c-8c7a98e0f1fb"}, "cell_type": "code", "source": "b_use_census_data = False", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%writefile life_event_prep.py\n\n\"\"\"\nSample Materials, provided under license.\nLicensed Materials - Property of IBM\n\u00a9 Copyright IBM Corp. 2019. All Rights Reserved.\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport sys\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nimport json\nimport os\n\nclass LifeEventPrep():\n\n    def __init__(self, target_event_type_ids, b_use_census_data, train_or_score='train', training_start_date=\"2010-01-01\", training_end_date=\"2017-08-01\", forecast_horizon=3,\n                 observation_window=4, scoring_end_date=datetime.datetime.today(), life_event_minimum_target_count=100, norepeat_months=4, cols_to_drop=['CUSTOMER_ID']):\n\n        self.train_or_score = train_or_score\n        self.target_event_type_ids = target_event_type_ids\n        self.b_use_census_data = b_use_census_data\n        self.forecast_horizon = forecast_horizon\n        self.observation_window = observation_window\n        self.training_start_date = training_start_date\n        self.training_end_date = training_end_date\n        self.scoring_end_date = scoring_end_date\n        self.life_event_minimum_target_count = life_event_minimum_target_count\n        self.norepeat_months = norepeat_months\n        self.cols_to_drop = cols_to_drop\n        self.latency_start = 1\n        self.perc_positive_cutoff = 1.0\n\n        # if a string for a particular date for end of scoring(obs month) is passed, convert to datetime\n        if self.train_or_score == 'score':\n            if isinstance(self.scoring_end_date, str):\n                self.scoring_end_date = datetime.datetime.strptime(self.scoring_end_date, '%Y-%m-%d')\n\n\n        if self.train_or_score == 'train':\n        # create a dictionary with all values for user inputs. We will save this out and use it for scoring\n        # to ensure that the user inputs are consistent across train and score notebooks\n        # exclude variables that won't be used for scoring\n          self.user_inputs_dict = { 'target_event_type_ids' : target_event_type_ids, 'b_use_census_data' : b_use_census_data, 'forecast_horizon' : forecast_horizon,\n              'observation_window' : observation_window, 'life_event_minimum_target_count' : life_event_minimum_target_count, 'norepeat_months' : norepeat_months, 'cols_to_drop' : cols_to_drop}\n\n    # Some functions to handle adding, subtracting, randomly selecting dates that are in YYYYMM format\n    def udf_n_months(self, dateMin, dateMax):\n        month_dif = (relativedelta(datetime.datetime.strptime(str(dateMax), '%Y%m'),\n                        datetime.datetime.strptime(str(dateMin), '%Y%m')).months +\n                    relativedelta(datetime.datetime.strptime(str(dateMax), '%Y%m'),\n                        datetime.datetime.strptime(str(dateMin), '%Y%m')).years * 12)\n        return month_dif\n\n    # function returns a random date in [dateMin, dateMax]\n    def udf_rand_date_in_range(self, dateMin, dateMax, randnumber):\n        rand_month_in_range = (datetime.datetime.strptime(str(dateMin), '%Y%m') +\n                                        (relativedelta(months=int(np.floor(randnumber *(relativedelta(\n                                                        datetime.datetime.strptime(str(dateMax), '%Y%m'),\n                                                        datetime.datetime.strptime(str(dateMin), '%Y%m')).months +\n                                                    relativedelta(\n                                                        datetime.datetime.strptime(str(dateMax), '%Y%m'),\n                                                        datetime.datetime.strptime(str(dateMin),'%Y%m')).years * 12 +1))))))\n\n        return rand_month_in_range.strftime('%Y%m')\n\n    # function to add a specified number of months to a date in YYYYMM format\n    def udf_add_months(self, YEAR_MONTH, months_to_add):\n        new_date = int((datetime.datetime.strptime(str(YEAR_MONTH), '%Y%m') + relativedelta(months=months_to_add)).strftime('%Y%m'))\n        return new_date\n\n    # selects the observation month\n    def udf_sub_rand_latency(self, YEAR_MONTH, randnumber, latency_start, forecast_horizon):\n        rand_obs_month = (datetime.datetime.strptime(str(YEAR_MONTH), '%Y%m')\n                            - relativedelta(months=latency_start + int(randnumber * (forecast_horizon - latency_start + 1)))).strftime('%Y%m')\n        return rand_obs_month\n\n    # Prep functions\n    def prepare_single_event_type(self, target_event_type_id, events, cym_cnt, train_or_score):\n        # ensure the dataframe has data in it - can be empty if scoring low number of cases and they were filtered out based on dates\n        if cym_cnt.shape[0] > 0:\n            # This function preps the data for one specified target event\n            # create a temporary target variable, which is 1 if the specific life event passed to the function\n            # happened in that month, otherwise 0\n            print('\\nPrepping data for ' + target_event_type_id)\n            target_col = 'E_' + target_event_type_id\n            # make sure that the target event exists as a column in the dataset, if not, add it with all 0's (should only really happen when scoring)\n            if target_col not in list(cym_cnt.columns):\n                cym_cnt[target_col] = 0\n\n            cym_cnt['TARGET'] = 0\n            cym_cnt.loc[cym_cnt[target_col]>0, 'TARGET'] = 1\n\n            # Call the select_customer_observation_month function - returns 1 record per customer\n            # with the target variable, the observation month, the start month of the observation period\n            # and the end month of the forecast horizon month\n            df_cust_target = self.select_customer_observation_month(cym_cnt, train_or_score)\n\n            # filter out customers who have no event data in the observation window\n            # join the target df with cym_cnt, the df with customer and YYYYMM and column per event\n            print('Number of customers before removing those with no event data in observation window : ' + str(df_cust_target.shape[0]))\n            if train_or_score=='train':\n                print('Number of customers target=1 before above filtering : ' + str(df_cust_target[df_cust_target['TARGET']==1].shape[0]))\n\n            # remove the temp target var in cym_cnt as we now have our final target\n            cym_cnt.drop('TARGET', axis=1, inplace=True)\n            df_cust_target_cym_cnt = cym_cnt.merge(df_cust_target, on='CUSTOMER_ID', how='inner')\n            df_cust_target_cym_cnt = df_cust_target_cym_cnt[(df_cust_target_cym_cnt['YEAR_MONTH'].astype(int)>=df_cust_target_cym_cnt['OBS_MONTH_MIN_OW'].astype(int))\n                                    & (df_cust_target_cym_cnt['YEAR_MONTH'].astype(int)<=df_cust_target_cym_cnt['OBS_MONTH'].astype(int))]\n\n            # update the target df to include only those customers who have had events in the observation window\n            df_cust_target = df_cust_target[df_cust_target['CUSTOMER_ID'].isin(list(df_cust_target_cym_cnt['CUSTOMER_ID'].unique()))]\n\n            print('Number of customers after removing those with no event data in observation window : ' + str(df_cust_target.shape[0]))\n\n            if df_cust_target.shape[0] == 0:\n              print('Customers had no events in observation window.', file=sys.stderr)\n              return None\n\n            if train_or_score=='train':\n                print('Number of customers target=1 after above filtering : ' + str(df_cust_target[df_cust_target['TARGET']==1].shape[0]))\n\n            # not all customers have had an event in their observation month\n            # Therefore they wouldn't have a record in the data for the observation month\n            # We add the record in and fill all events with 0\n            # I think this helps for later in the code\n            # Get the observation month for each customer, join to the wide df (cym_cnt) on customer and YYYYMM,\n            # Use a right outer join so if the observation month isn't in the wide df it will be included after join\n\n            df_temp = cym_cnt.merge(df_cust_target[['CUSTOMER_ID', 'OBS_MONTH']], left_on=['CUSTOMER_ID', 'YEAR_MONTH'],\n                                            right_on=['CUSTOMER_ID', 'OBS_MONTH'], how='right')\n            # only take where the result is null, ie. where the observerd month wasn't in the wide (cym_cnt) table\n            df_temp = df_temp[df_temp['YEAR_MONTH'].isnull()]\n            # update YEAR_MONTH to be the OBS_MONTH and drop OBS_MONTH, fill na's with 0\n            df_temp['YEAR_MONTH'] = df_temp['OBS_MONTH']\n            df_temp.drop('OBS_MONTH', axis=1, inplace=True)\n            df_temp.fillna(0, inplace=True)\n\n            # join back to target df so we can add the relevant columns from that df\n            # then append onto the df_cust_target_cym_cnt df\n            # we then just have a df with customer, YYYYMM and column for each event,\n            # but always including a record for the observation month for a customer\n            df_temp = df_temp.merge(df_cust_target, on='CUSTOMER_ID', how='inner')\n            df_cust_target_cym_cnt = pd.concat([df_cust_target_cym_cnt, df_temp], sort=False)\n\n            # more filtering to remove edge cases\n            # for scoring, remove customers who have experienced the life event in the previous norepeat_months months\n            # where start of the observation window is before the start of our data, remove the records\n            # where the end of the forecase period is after the end of our data, remove the records\n            YM_min = df_cust_target_cym_cnt['YEAR_MONTH'].min()\n            YM_max = df_cust_target_cym_cnt['YEAR_MONTH'].max()\n\n            #print('Number of customers before filtering : ' + str(df_cust_target_cym_cnt['CUSTOMER_ID'].nunique()))\n            #print('Number of customers target=1 before filtering : ' + str(df_cust_target_cym_cnt[df_cust_target_cym_cnt['TARGET']==1]['CUSTOMER_ID'].nunique()))\n\n            df_cust_target_cym_cnt = df_cust_target_cym_cnt[df_cust_target_cym_cnt['OBS_MONTH_MIN_OW']>=YM_min]\n            if train_or_score == 'train':\n                df_cust_target_cym_cnt = df_cust_target_cym_cnt[df_cust_target_cym_cnt['OBS_MONTH_PLS_LATEND']<=YM_max]\n            elif train_or_score == 'score':\n                norepeat_months = self.norepeat_months\n                # we don't want to score customers who have experienced the life event in the previous norepeat_months\n                start_norepeat_period = self.udf_add_months(YM_max, 1-norepeat_months)\n\n                customers_norepeat = df_cust_target_cym_cnt[(df_cust_target_cym_cnt['TARGET']>0) &\n                                                (df_cust_target_cym_cnt['TARGET_MONTH']<start_norepeat_period)]\n\n                customers_norepeat = pd.DataFrame(customers_norepeat['CUSTOMER_ID'].drop_duplicates())\n                customers_norepeat['LIFE_EVENT_B4_NOREP_PERIOD'] = 1\n\n                df_cust_target_cym_cnt = df_cust_target_cym_cnt.merge(customers_norepeat, on='CUSTOMER_ID', how='left')\n\n                # Keep records where the target is 0 (haven't experienced the life event),\n                # or target is 1 and life_event_b4_norep_period is 1 (the customer experienced the life event but it was\n                # more than norepeat_months ago)\n                df_cust_target_cym_cnt = df_cust_target_cym_cnt[(df_cust_target_cym_cnt['TARGET']==0) | ((df_cust_target_cym_cnt['TARGET']==1) &\n                                                                        (df_cust_target_cym_cnt['LIFE_EVENT_B4_NOREP_PERIOD']==1))]\n\n                df_cust_target_cym_cnt.drop(['LIFE_EVENT_B4_NOREP_PERIOD'], axis=1, inplace=True)\n                if df_cust_target_cym_cnt.shape[0] == 0:\n                    print('Note: All customers filtered out as they experienced the life event within ' +\n                        str(norepeat_months) + ' months (norepeat_months) of the observation date', file=sys.stderr)\n\n\n            #print('Number of customers after filtering : ' + str(df_cust_target_cym_cnt['CUSTOMER_ID'].nunique()))\n            #print('Number of customers target=1 after filtering : ' + str(df_cust_target_cym_cnt[df_cust_target_cym_cnt['TARGET']==1]['CUSTOMER_ID'].nunique()))\n\n            # get data into AMT format, one line of data per customer\n            # we will create variables that are a count of each event per customer over their observation window (end with '_OW')\n            # We also create variables for the count of each event in the actual observation month\n\n            # remove columns we don't need anymore\n            df_cust_target_cym_cnt.drop(['TARGET_MONTH', 'OBS_MONTH_MIN_OW', 'OBS_MONTH_PLS_LATEND'], axis=1, inplace=True)\n\n            # count the number of occurences of each event over the observation window\n            # drop target as it is summed up, correct target is later\n            df_per_cust_ow = df_cust_target_cym_cnt.groupby(['CUSTOMER_ID', 'OBS_MONTH']).sum().reset_index()\n            df_per_cust_ow.drop(['YEAR_MONTH', 'TARGET'], axis=1, inplace=True)\n            # add a variable for total number of events over observarion window\n\n            for col in df_per_cust_ow.columns:\n                if col.startswith('E_'):\n                    new_col_name = col + '_OW'\n                    df_per_cust_ow.rename(columns={col:new_col_name}, inplace=True)\n\n            # get the number of occurences of each event in the observation window\n            df_per_cust_om = df_cust_target_cym_cnt[df_cust_target_cym_cnt['OBS_MONTH']==df_cust_target_cym_cnt['YEAR_MONTH']].copy()\n            df_per_cust_om.drop('YEAR_MONTH', axis=1, inplace=True)\n\n            for col in df_per_cust_om.columns:\n                if col.startswith('E_'):\n                    new_col_name = col + '_OM'\n                    df_per_cust_om.rename(columns={col:new_col_name}, inplace=True)\n\n            df_per_cust = df_per_cust_ow.merge(df_per_cust_om, on=['CUSTOMER_ID', 'OBS_MONTH'], how='inner')\n\n            # add a variable for observation month.\n            df_per_cust['MONTH'] = df_per_cust['OBS_MONTH'].astype(str).str[4:].astype(int)\n\n            # get total number of events per customers in observation window and in observation month\n            events_ow_cols = list(df_per_cust.loc[:, df_per_cust.columns.str.endswith('_OW')].columns)\n            events_om_cols = list(df_per_cust.loc[:, (~(df_per_cust.columns.str.endswith('_OW')) & (df_per_cust.columns.str.startswith('E_')))].columns)\n\n            df_per_cust['TOT_NB_OF_EVENTS_OW'] = df_per_cust[events_ow_cols].sum(axis=1)\n            df_per_cust['TOT_NB_OF_EVENTS_OM'] = df_per_cust[events_om_cols].sum(axis=1)\n\n            if train_or_score == 'train':\n                # cleaning - move target variable to end\n                cols = list(df_per_cust)\n                cols.insert(len(cols), cols.pop(cols.index('TARGET')))\n                df_per_cust = df_per_cust.loc[:, cols]\n            elif train_or_score == 'score':\n                df_per_cust.drop('TARGET', axis=1, inplace=True)\n\n            return df_per_cust\n\n        else:\n            print('Error: No customers were passed to the function. Stopping.', file=sys.stderr)\n            #sys.exit('Error: No customers were passed to the function. Stopping.')\n            return None\n\n    def select_customer_observation_month(self, cym_cnt,  train_or_score):\n        # Creates the observation month for each customer and the target variable.\n        # Also works out first month in observation window and end of forecast horizon period\n        # First creates a target month for each customer, TARGET_MONTH, the month that the life event occurs\n        # for the customer. If a customer hasn't experienced a life event, they are given a random TARGET_MONTH,\n        # which is a month between the first event and last event month for that customer\n        # For scoring, the observation month is the 'scoring_end_date' variable\n\n        observation_window = self.observation_window\n        forecast_horizon = self.forecast_horizon\n        latency_start = self.latency_start\n\n        print('Using observation_window = ' + str(observation_window))\n        print('Using forecast_horizon = ' + str(forecast_horizon))\n\n        # create df with just customer_id, year_month and target(as created in prepare_single_event_type function)\n        # df_cust_month_target was cym_tgt in original code\n        df_cust_month_target = cym_cnt[['CUSTOMER_ID', 'YEAR_MONTH', 'TARGET']]\n\n        if train_or_score == 'train':\n            # Directly from codebase:\n            ######################################\n            # Definition of the TARGET_MONTH:\n            # occurence of target over histoical months\n            # C1: 0 ------------------------- 0\n            # C2: ----------------- 0 1 0 -----\n            # C3: ------- 0 1 0 --- 0 1 0 -----\n            #\n            # C1 has no occurence of the event,\n            # C2 has exactly one\n            # C3 had two occurences\n\n            # C1. For customers who didn't experience a life event give them a random TARGET_MONTH\n            # within the period that they were active\n            # Note this can select a target_month that is the first month a customer is seen\n            # that customer will have no historical event data and will be removed later in the code\n            df_negatives = df_cust_month_target.groupby(['CUSTOMER_ID']).agg({'YEAR_MONTH':['min', 'max'], 'TARGET': 'sum'}).reset_index()\n            df_negatives.columns = df_negatives.columns.get_level_values(1)\n            df_negatives.columns = ['CUSTOMER_ID', 'CUST_YM_MIN', 'CUST_YM_MAX', 'CUST_TARGET']\n            df_negatives = df_negatives[df_negatives['CUST_TARGET']==0]\n            df_negatives['RAND1'] = np.random.rand(df_negatives.shape[0])\n            df_negatives['TARGET_MONTH'] = df_negatives.apply(lambda x: self.udf_rand_date_in_range(int(x['CUST_YM_MIN']), int(x['CUST_YM_MAX']), x['RAND1']), axis=1)\n            df_negatives = df_negatives[['CUSTOMER_ID', 'TARGET_MONTH']]\n            df_negatives['TARGET'] = 0\n\n            # C2. Customers who experienced exactly 1 life event\n            # assign a specified % as positive examples\n            # should this % parameter be configurable?\n            # remaing are set to negative, selecting a random month at least norepeat_months after the event\n\n            perc_positive_cutoff = self.perc_positive_cutoff\n\n            # Create a df with target month for each customer and a random number column which is used\n            # to specify if the record should be used as a positive or negative example\n            df_cust_target_one_occ = df_cust_month_target[df_cust_month_target['TARGET']>0].groupby('CUSTOMER_ID').agg({'YEAR_MONTH':'min', 'TARGET': 'sum'}).reset_index()\n            df_cust_target_one_occ = df_cust_target_one_occ[df_cust_target_one_occ['TARGET']==1]\n            df_cust_target_one_occ.rename(columns={'YEAR_MONTH':'TARGET_MONTH', 'TARGET':'TARGET_COUNT'}, inplace=True)\n            df_cust_target_one_occ['RAND1'] = np.random.rand(df_cust_target_one_occ.shape[0])\n\n            # Take all records with random number less than cutoff as positive examples\n            df_cust_target_one_occ_pos = df_cust_target_one_occ[df_cust_target_one_occ['RAND1']<=perc_positive_cutoff][['CUSTOMER_ID', 'TARGET_MONTH']]\n            df_cust_target_one_occ_pos['TARGET'] = 1\n\n            # All records greater than the cutoff are negative examples\n            # For each customer, find the starting point for their TARGET_MONTH\n            # This has to be between norepeat_months after the event and the date of their last event\n            # if the cutoff is set to 1 it means that we don't set any of these records to 0\n\n            if perc_positive_cutoff < 1.0:\n                df_cust_target_one_occ_neg = df_cust_target_one_occ[df_cust_target_one_occ['RAND1']>perc_positive_cutoff][['CUSTOMER_ID', 'TARGET_MONTH']]\n                # specify a new (temp) TARGET_MONTH that is norepeat_months after the event occurred\n                df_cust_target_one_occ_neg['TARGET_MONTH'] = df_cust_target_one_occ_neg.apply(lambda x: self.udf_add_months(int(x['TARGET_MONTH']), self.norepeat_months), axis=1)\n                # Select a random month between the new TARGET_MONTH and the last time the customer is seen\n                # join back to df_cust_month_target which has a record for every customer and month\n                df_cust_target_one_occ_neg = df_cust_target_one_occ_neg.merge(df_cust_month_target, on='CUSTOMER_ID', how='inner')\n                # filter to include only months >= the new target month\n                df_cust_target_one_occ_neg[df_cust_target_one_occ_neg['YEAR_MONTH']>=df_cust_target_one_occ_neg['TARGET_MONTH']]\n                # I changed this, original called for a random month between first event after new target_month and last event\n                # I'm changing to say the customer can have a random target month between new target month and last event\n                df_cust_target_one_occ_neg = df_cust_target_one_occ_neg.groupby(['CUSTOMER_ID', 'TARGET_MONTH'])['YEAR_MONTH'].max().reset_index()\n                df_cust_target_one_occ_neg.rename(columns={'TARGET_MONTH':'CUST_YM_MIN', 'YEAR_MONTH':'CUST_YM_MAX'}, inplace=True)\n                df_cust_target_one_occ_neg['RAND2'] = np.random.rand(df_cust_target_one_occ_neg.shape[0])\n                # Call the function to select a random target month between TARGET_MONTH and last event month\n                df_cust_target_one_occ_neg['TARGET_MONTH'] = df_cust_target_one_occ_neg.apply(lambda x: self.udf_rand_date_in_range(int(x['CUST_YM_MIN']), int(x['CUST_YM_MAX']), x['RAND2']), axis=1)\n                # select relevant columns and set the target value to 0\n                df_cust_target_one_occ_neg = df_cust_target_one_occ_neg[['CUSTOMER_ID', 'TARGET_MONTH']]\n                df_cust_target_one_occ_neg['TARGET'] = 0\n\n            # C3. Customers who experienced the life event multiple times\n            # We just take the first time they experienced the event as the TARGET_MONTH\n\n            # filter to only include months where target=1\n            df_cust_target_multi_occ = df_cust_month_target[df_cust_month_target['TARGET']>=1]\n            # Get the min of YEAR_MONTH per customer to find month of occurence of first life event\n            # Sum up the target so we can filter to inlcude only customers who have had multiple life events\n            df_cust_target_multi_occ = df_cust_target_multi_occ.groupby('CUSTOMER_ID').agg({'YEAR_MONTH':'min', 'TARGET':'sum'}).reset_index()\n            df_cust_target_multi_occ.rename(columns={'YEAR_MONTH':'TARGET_MONTH'}, inplace=True)\n            df_cust_target_multi_occ = df_cust_target_multi_occ[df_cust_target_multi_occ['TARGET']>1]\n            # Filter to only include columns we need and set target to 1\n            df_cust_target_multi_occ = df_cust_target_multi_occ[['CUSTOMER_ID', 'TARGET_MONTH']]\n            df_cust_target_multi_occ['TARGET'] = 1\n\n            print('Training data has #Target> 1 customers: ' + str(df_cust_target_multi_occ.shape[0]))\n            print('Training data has #Target==1 customers: ' + str(df_cust_target_one_occ.shape[0]))\n            print('   Of those, we set ' + str(df_cust_target_one_occ_pos.shape[0]) + ' to positive')\n            if perc_positive_cutoff < 1.0:\n                print('   and we set ' + str(df_cust_target_one_occ_neg.shape[0]) + ' to negative')\n            else:\n                print('   and we set 0 to negative')\n            print('Training data has #Target==0 customers: ' + str(df_negatives.shape[0]))\n\n            # this was cdates variable in original\n            if perc_positive_cutoff < 1.0:\n                df_cust_target = pd.concat([df_negatives, df_cust_target_one_occ_neg, df_cust_target_one_occ_pos, df_cust_target_multi_occ])\n            else:\n                df_cust_target = pd.concat([df_negatives, df_cust_target_one_occ_pos, df_cust_target_multi_occ])\n            print('Number of records : ' + str(df_cust_target.shape[0]))\n            print('Number of unique customers : ' + str(df_cust_target['CUSTOMER_ID'].nunique()))\n\n            if df_cust_target['CUSTOMER_ID'].nunique() != df_cust_target.shape[0]:\n                print('Something went wrong. We have more than 1 row per customer')\n\n            # I haven't included hidden feature about boosting minority class\n\n            # select the observation month for each customer\n            # the month must be within the forecast_horizon of the target_month\n            # For example, if the life event occurred in 201809, the observation month must be between\n            # 201806 and 201809 if the forecast_horizon is 3 months\n            df_cust_target['RAND2'] = np.random.rand(df_cust_target.shape[0])\n            df_cust_target['OBS_MONTH'] = df_cust_target.apply(lambda x: self.udf_sub_rand_latency(int(x['TARGET_MONTH']), x['RAND2'], latency_start, forecast_horizon), axis=1)\n\n            # Now that we have the observation month we get the first month of our observation window (OBS_MONTH_MIN_OW)\n            # Events which occurred over the observation window will be used as variables in AMT\n            # Note that the observation month is included in the observation window\n            df_cust_target['OBS_MONTH_MIN_OW'] = df_cust_target.apply(lambda x: self.udf_add_months(int(x['OBS_MONTH']), (1-observation_window)), axis=1)\n\n            # We also calculate the end month in the forecasting period (OBS_MONTH_PLS_LATEND)\n            df_cust_target['OBS_MONTH_PLS_LATEND'] = df_cust_target.apply(lambda x: self.udf_add_months(int(x['OBS_MONTH']), forecast_horizon), axis=1)\n\n            df_cust_target.drop('RAND2', axis=1, inplace=True)\n\n            # set the months to ints instead of objects\n            df_cust_target['OBS_MONTH'] = df_cust_target['OBS_MONTH'].astype(int)\n            df_cust_target['TARGET_MONTH'] = df_cust_target['TARGET_MONTH'].astype(int)\n\n        elif train_or_score =='score':\n\n            df_cust_target = df_cust_month_target.groupby('CUSTOMER_ID')['TARGET'].max().reset_index()\n            df_cust_target['OBS_MONTH'] = pd.to_datetime(self.scoring_end_date.date())\n            df_cust_target['OBS_MONTH'] = df_cust_target['OBS_MONTH'].dt.strftime('%Y%m').astype(int)\n            df_cust_target['OBS_MONTH_MIN_OW'] = df_cust_target.apply(lambda x: self.udf_add_months(int(x['OBS_MONTH']), (1-observation_window)), axis=1)\n            df_cust_target['OBS_MONTH_PLS_LATEND'] = df_cust_target.apply(lambda x: self.udf_add_months(int(x['OBS_MONTH']), forecast_horizon), axis=1)\n\n            # for those customers who have experienced the life event, we want to know when they last experienced it\n            df_month_last_lfe_event = df_cust_month_target[df_cust_month_target['TARGET']>0].groupby('CUSTOMER_ID')['YEAR_MONTH'].max().reset_index()\n            df_month_last_lfe_event.rename(columns={'YEAR_MONTH':'TARGET_MONTH'}, inplace=True)\n            df_cust_target = df_cust_target.merge(df_month_last_lfe_event, on='CUSTOMER_ID', how='left')\n            df_cust_target['TARGET_MONTH'] = df_cust_target['TARGET_MONTH'].fillna(0)\n            df_cust_target['TARGET_MONTH'] = df_cust_target['TARGET_MONTH'].astype(int)\n\n        return df_cust_target\n\n    def prep_data(self, df_raw, train_or_score):\n        np.random.seed(42)\n        # just in case any caps are used\n        train_or_score = train_or_score.lower()\n        # hidden inputs\n        latency_start = self.latency_start\n\n        print('Before removing dates that are not in training period : ' + str(df_raw.shape))\n        # remove any dates that are not in our training period\n        if train_or_score == 'train':\n            df_raw = df_raw[(df_raw['EVENT_DATE']>=datetime.datetime.strptime(self.training_start_date, '%Y-%m-%d'))\n                    & (df_raw['EVENT_DATE']<=datetime.datetime.strptime(self.training_end_date, '%Y-%m-%d'))]\n        else:\n            # otherwise use same start period but all data to end of scoring period\n            df_raw = df_raw[(df_raw['EVENT_DATE']>=datetime.datetime.strptime(self.training_start_date, '%Y-%m-%d'))\n                    & (df_raw['EVENT_DATE']<=self.scoring_end_date)]\n        print('After removing dates that are not in training period : ' + str(df_raw.shape) + '\\n')\n\n        # create a df with 1 record per customer, get date of first and last event\n        # filter to include only those who have enough months of data\n        # enough months = (observation + forecast) for training data\n        # enough months = observation window for scoring data\n        # For scoring, if we haven't seen the customer in the observation window, we filter them out here\n\n        print('Number of customers before checking for enough history : ' + str(df_raw['CUSTOMER_ID'].nunique()))\n\n        # prevent the code from going further if all customers have been filtered out\n        if df_raw['CUSTOMER_ID'].nunique() == 0:\n          print('Customer had no events in the effective date period', file=sys.stderr)\n\n        else:\n          if train_or_score == 'train':\n              n_months = self.forecast_horizon + self.observation_window\n\n              customers_with_enough_history = df_raw.groupby('CUSTOMER_ID')['EVENT_DATE'].agg([max, min]).reset_index()\n              customers_with_enough_history.columns = ['CUSTOMER_ID', 'MAX_DATE', 'MIN_DATE']\n\n              # Convert to yyyymm and add new column for number of months\n              # filter to exclude customers who don't have enough months of data\n              customers_with_enough_history['MAX_DATE'] = customers_with_enough_history['MAX_DATE'].dt.strftime('%Y%m').astype(int)\n              customers_with_enough_history['MIN_DATE'] = customers_with_enough_history['MIN_DATE'].dt.strftime('%Y%m').astype(int)\n              customers_with_enough_history['N_MONTHS'] = customers_with_enough_history.apply(lambda x: self.udf_n_months(x['MIN_DATE'], x['MAX_DATE']), axis=1)\n              customers_with_enough_history = customers_with_enough_history[customers_with_enough_history['N_MONTHS']>n_months]\n\n          elif train_or_score == 'score':\n              n_months = self.observation_window\n\n              customers_with_enough_history = df_raw.groupby('CUSTOMER_ID')['EVENT_DATE'].max().reset_index()\n              customers_with_enough_history.columns = ['CUSTOMER_ID', 'MAX_DATE']\n              # add a new column for effective date\n              customers_with_enough_history['EFF_DATE_LATEST'] = pd.to_datetime(self.scoring_end_date.date())\n              # Convert to yyyymm and add new column for number of months\n              # filter to exclude customers who haven't had an event in the observation periods\n              customers_with_enough_history['MAX_DATE'] = customers_with_enough_history['MAX_DATE'].dt.strftime('%Y%m').astype(int)\n              customers_with_enough_history['EFF_DATE_LATEST'] = customers_with_enough_history['EFF_DATE_LATEST'].dt.strftime('%Y%m').astype(int)\n              customers_with_enough_history['N_MONTHS'] = customers_with_enough_history.apply(lambda x: self.udf_n_months(x['MAX_DATE'], x['EFF_DATE_LATEST']), axis=1)\n              customers_with_enough_history = customers_with_enough_history[customers_with_enough_history['N_MONTHS']<=n_months]\n              if customers_with_enough_history.shape[0] == 0:\n                  print('Note: No customer for scoring had any event within the observation window and all have been filtered out', file=sys.stderr)\n\n          print('Number of customers after  checking for enough history : ' + str(customers_with_enough_history.shape[0]) + '\\n')\n\n          df_events = df_raw.merge(customers_with_enough_history, on='CUSTOMER_ID', how='inner')\n          print('Total number of events in the data : ' + str(df_events.shape[0]) + '\\n')\n          # get a list of distinct events\n          events = list(df_events['EVENT_TYPE_ID'].unique())\n\n          # get a count of number of occurences of each event by customer and month (yyyymm)\n          # pivot to give one record per customer and month (yyyymm) with each event having a column\n          df_events['YEAR_MONTH'] = df_events['EVENT_DATE'].dt.strftime('%Y%m').astype(int)\n          df_events = df_events.groupby(['CUSTOMER_ID', 'YEAR_MONTH', 'EVENT_TYPE_ID']).size().reset_index()\n          df_events.rename(columns={0:'count'}, inplace=True)\n\n          cym_cnt = pd.pivot_table(df_events, index=['CUSTOMER_ID', 'YEAR_MONTH'], columns='EVENT_TYPE_ID', values='count').reset_index()\n          cym_cnt.fillna(0, inplace=True)\n\n          if cym_cnt.shape[0] == 0:\n              print('Note: All customers were filtered out\\n', file=sys.stderr)\n\n          # check to make sure target events are in the events table\n          # any target event that doesn't appear in the events table is removed\n          # This should only be carried out for training\n          if train_or_score == 'train':\n              for target_event in self.target_event_type_ids:\n                  if target_event not in events:\n                      self.target_event_type_ids.remove(target_event)\n                      print(target_event + ' does not appear in events table and has been removed')\n\n              if len(self.target_event_type_ids) == 0:\n                  print('Note: event_type_ids from target_event_type_ids not found in events table', file=sys.stderr)\n\n              # if there are less than the threshold number of customers associated with the target event, remove the event\n              # get a count of number of unique customers associated with each target event\n              # any below the threshold are removed from the target_event_type_ids list\n              df_target_cust_count = pd.DataFrame(df_events[df_events['EVENT_TYPE_ID'].isin(self.target_event_type_ids)].groupby('EVENT_TYPE_ID')['CUSTOMER_ID'].nunique().reset_index())\n              df_target_cust_count.rename(columns={'CUSTOMER_ID':'customer_count'}, inplace=True)\n              events_below_threshold = list(df_target_cust_count[df_target_cust_count['customer_count']<self.life_event_minimum_target_count]['EVENT_TYPE_ID'])\n              target_event_type_ids = [x for x in self.target_event_type_ids if x not in events_below_threshold]\n              print('\\n' + str(len(self.target_event_type_ids)) + ' Target ID(s) left after removing target events below threshold (' + str(self.life_event_minimum_target_count) + ' customers)')\n\n          # rename event columns to include 'E_'\n          for e in events:\n              cym_cnt.rename(columns={e:'E_' + e}, inplace=True)\n\n        result_map = {}\n        for event_type_id in self.target_event_type_ids:\n            if df_raw['CUSTOMER_ID'].nunique() > 0:\n              #Call the prepare_single_event_type function\n              result_map[event_type_id] = self.prepare_single_event_type(event_type_id, events, cym_cnt, train_or_score)\n            else:\n              result_map[event_type_id] = None\n\n        if result_map[event_type_id] is not None:\n          # store the columns names that are used as input into each model\n          training_cols = {}\n          for event_type_id in self.target_event_type_ids:\n              # prep training data, remove columns where nulls make up over 10%\n              # drop constant columns (eg all 0's)\n\n              # drop obs_month column\n              result_map[event_type_id].drop('OBS_MONTH', axis=1, inplace=True)\n\n              if train_or_score == 'train':\n                  columns_required = ['CUSTOMER_ID', 'TARGET', 'MONTH']\n                  numeric_cols = []\n                  for col in result_map[event_type_id].columns:\n                      if is_numeric_dtype(result_map[event_type_id][col].dtype):\n                          numeric_cols.append(col)\n\n                  numeric_cols = set(numeric_cols) - set(columns_required)\n                  print(result_map[event_type_id].shape)\n                  # loop through columns and check for constants or missing vals\n                  for col in numeric_cols:\n                      # drop cols where min=max ie constants\n                      curr_col = result_map[event_type_id][col]\n                      if curr_col.min() == curr_col.max():\n                          result_map[event_type_id].drop(col, axis=1, inplace=True)\n                      # drop column if it is 10% or more null values\n                      elif (curr_col.isna().sum()/curr_col.shape[0]) > 0.1:\n                          result_map[event_type_id].drop(col, axis=1, inplace=True)\n\n              for col in self.cols_to_drop:\n                  result_map[event_type_id].drop(col, axis=1, inplace=True)\n\n              training_cols[event_type_id] = list(result_map[event_type_id].columns)\n\n          # if training, use json to save out the columns that were used for training\n          if train_or_score == 'train':\n              self.user_inputs_dict['cols_used_for_training'] = training_cols\n\n              # save the user inputs and the columns used for building models\n              with open('training_user_inputs_and_prepped_column_names.json', 'w') as f:\n                  json.dump(self.user_inputs_dict, f)\n\n        return result_map", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Writing life_event_prep.py\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "%%writefile prep_census_data.py\n\n# Copyright 2017, 2018 IBM. IPLA licensed Sample Materials.\n# this function is called if the user selects to use the supplied census data (b_use_census_data variable)\n# it reads in the census and customer data\n# the census and customer data are matched based on age, marital status, education, employment status,\n# income, location and gender\n# the function does cleaning to align category names in each column between customer and census datasets\n# returns the prepped dataset along with marriage, migration, birth and divorce probabilities from census data\nimport pandas as pd\nimport os \nimport numpy as np\n\nclass census_data():\n  def prep_census(self,census_data,customer_data,prepped_data,train_or_score):\n\n      # read in the census data and the customer data\n      df_census_probabilities = census_data.copy()#pd.read_csv('/project_data/data_asset/Census Migration Birth Marriage and Divorce Probabilities.csv')\n      \n      df_customers = customer_data.copy()#pd.read_csv('/project_data/data_asset/customer.csv')\n\n      # to join the census data to customer data we map our customer categories to their most similar category in the census data \n  \n      # age ranges in customer data: 23 to 30, 30 to 40, 40 to 55, 55 to 65, 65 and over   \n      # age ranges in census data: 18-24, 25-29, 30-34, 35-39, 40-44, 45-54, 55-64, 65-74, 75+, unknown\n      # update the census categories to be the same as in the customer data\n      age_dict = {'18-24':'18-24', '25-29':'23 to 30', '30-34':'30 to 40', '35-39':'30 to 40', '40-44':'40 to 55', '45-54':'40 to 55', '55-64':'55 to 65', '65-74':'65 and over', '75+':'65 and over', 'Unknown':'Unknown'}\n      df_census_probabilities['AGE'] = df_census_probabilities['AGE'].map(age_dict)\n  \n      # marital status in customer data: Married, Divorced, 'Single'\n      # marrital status in census data: Married, Divorced or Separated, Single, Widoed, Unknown\n      # update census 'Divorced or Separated' category to 'Divorced'\n      # All other categories can remain the same\n      df_census_probabilities['MARITAL_STATUS'] = df_census_probabilities['MARITAL_STATUS'].replace({'Divorced or Separated':'Divorced'})\n  \n      # education in customer data: High School, College, Professional, University, PhD\n      # education in census data: Grade 11 or Lower, High School, University, Professional Degree, Doctorate Degree, Unknown\n      # update 'Professional Degree' category in census data to 'Professional' \n      # update 'Doctorate Degree' category in census data to 'PhD'\n      df_census_probabilities['EDUCATION'] = df_census_probabilities['EDUCATION'].replace({'Professional Degree':'Professional',\n                                                                                    'Doctorate Degree':'PhD'})\n      # update the 'College' category in customer data to 'University'\n      df_customers['EDUCATION_LEVEL'] = df_customers['EDUCATION_LEVEL'].replace({'College':'University'})\n  \n      # employment status in customer data: Employed, Selfemployed, Homemaker, Retired, Unemployed\n      # employment status in census data: Employed, 'Not in Labor Force', Unemployed, Unknown\n      # update selfemployed category in customer data to employed\n      # update homemaker category in customer data to Not in Labor Force\n      # update retired category in customer data to Not in Labor Force\n      df_customers['EMPLOYMENT_STATUS'] = df_customers['EMPLOYMENT_STATUS'].replace({'Selfemployed':'Employed'})\n      df_customers['EMPLOYMENT_STATUS'] = df_customers['EMPLOYMENT_STATUS'].replace({'Homemaker':'Not in Labor Force'})\n      df_customers['EMPLOYMENT_STATUS'] = df_customers['EMPLOYMENT_STATUS'].replace({'Retired':'Not in Labor Force'})\n  \n      # income in customer data is numerical\n      # income in census data: Under 15k, 15k-35k, 35k-75k, 75k-125k, 125k-200k, 200K+, Unknown,\n      # bin income up in the customer data\n      bins = [0, 15000, 35000, 75000, 125000, 200000, 9999999999]\n      labels = ['Under 15k', '15k-35k', '35k-75k', '75k-125k', '125k-200k', '200K+']\n      df_customers['ANNUAL_INCOME'] = pd.cut(df_customers['ANNUAL_INCOME'], bins, labels=labels)\n  \n      # states should match between customer and census (where customer data is in USA)\n      # set everything else in customer data to 'Unknown' to align with census\n      df_customers['LOCATION'] = df_customers['ADDRESS_HOME_STATE']\n      df_customers.loc[(~df_customers['ADDRESS_HOME_STATE'].isin(df_census_probabilities['LOCATION'].unique())), 'LOCATION'] = 'Unknown'\n  \n      # gender ranges in our customer data are the same as census (ex 'unknown')\n  \n      # because of how we grouped above, we can have duplicate records over location, age, marital status, education, employment,\n      # gender and income, but with different probabilities\n      # to combat this we group by these factors and take an average of the probabilities\n      df_census_probabilities = df_census_probabilities.groupby(['LOCATION', 'MARITAL_STATUS', 'EDUCATION', 'GENDER', 'EMPLOYMENT',\n             'INCOME', 'AGE'])[['MIGRATION_PROB', 'BIRTH_PROB', 'MARRIAGE_PROB', 'DIVORCE_PROB']].mean().reset_index()\n      \n      \n      ##########  Code to plot census Data on training notebook \n      if train_or_score=='train':\n\n        cols_to_plot=[\"LOCATION\",\"MARITAL_STATUS\",\"EDUCATION\",\"GENDER\",\"EMPLOYMENT\",\"INCOME\",\"AGE\"]\n        for col in cols_to_plot:\n          df_to_plot=df_census_probabilities.groupby(col).mean().reset_index()\n      \n          #df_to_plot[\"BIRTH_PROB\"]=df_to_plot[\"BIRTH_PROB\"]*10\n          #df_to_plot[\"MARRIAGE_PROB\"]=df_to_plot[\"MARRIAGE_PROB\"]*10\n          #df_to_plot[\"DIVORCE_PROB\"]=df_to_plot[\"DIVORCE_PROB\"]*10\n          self.plot_census(df_to_plot,col)\n        \n        \n      # join customer and census data on all 7 fields to get the probabilities from census\n      # first filter the customer data to return one record per customer\n      df_customers = pd.merge(df_customers, df_customers.groupby('CUSTOMER_ID')['EFFECTIVE_DATE'].max().reset_index(), how='inner', on=['CUSTOMER_ID', 'EFFECTIVE_DATE'])\n  \n      # get the records that we can match on all 7 criteria\n      # mapping above should ensure that all customer records will get a match so we can use an inner join\n      df_census_probabilities = pd.merge(df_customers, df_census_probabilities, how='inner', left_on=['AGE_RANGE', 'MARITAL_STATUS', \n                                                                      'EDUCATION_LEVEL', 'EMPLOYMENT_STATUS',\n                                                                      'LOCATION', 'ANNUAL_INCOME', 'GENDER'],\n                                                          right_on=['AGE', 'MARITAL_STATUS', \n                                                                      'EDUCATION', 'EMPLOYMENT',\n                                                                      'LOCATION', 'INCOME', 'GENDER'])\n  \n      df_census_probabilities = df_census_probabilities[['CUSTOMER_ID', 'MIGRATION_PROB', 'BIRTH_PROB', 'MARRIAGE_PROB', 'DIVORCE_PROB']]\n      \n      # replace any missing values with the mean for that column\n      df_census_probabilities = df_census_probabilities.fillna(df_census_probabilities.mean())\n      \n      # loop through the dictionary of prepped data and append the probabilities to the prepped data\n      for event_type, df in prepped_data.items():\n          prepped_data[event_type] = pd.merge(prepped_data[event_type], df_census_probabilities, on='CUSTOMER_ID')\n          prepped_data[event_type].drop('CUSTOMER_ID', axis=1, inplace=True)\n  \n      return prepped_data\n  \n  def plot_census(self,s,column):\n    \n    from chart_studio.plotly import iplot\n    import plotly as py\n    import plotly.graph_objs as go\n    py.offline.init_notebook_mode(connected=True)\n\n    data = [\n        go.Bar(\n            x=s[column],\n            y=s[\"MIGRATION_PROB\"],\n            name=\"MIGRATION_PROB\"\n        ),\n        go.Bar(\n            x=s[column],\n            y=s[\"BIRTH_PROB\"],\n            name=\"BIRTH_PROB\"\n        ),\n        go.Bar(\n            x=s[column],\n            y=s[\"MARRIAGE_PROB\"],\n            name=\"MARRIAGE_PROB\"\n        ),\n        go.Bar(\n            x=s[column],\n            y=s[\"DIVORCE_PROB\"],\n            name=\"DIVORCE_PROB\"\n        )\n\n    ]\n    \n    layout = go.Layout(\n        barmode='group',\n        title='Average Census data Probabilities by '+column\n    )\n\n    fig = dict(data = data, layout = layout)\n    py.offline.iplot(fig)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "Overwriting prep_census_data.py\n", "name": "stdout"}]}, {"metadata": {"id": "e957fe9c-7b9f-497c-9996-ab25baa20162"}, "cell_type": "code", "source": "# Copy files into the Notebook filesystem\nfiles = ['training_user_inputs_and_prepped_column_names.json', 'event.csv', 'census_probabilities.csv', 'customer.csv']\nfor item in files:\n    f = open(item, 'w+b')\n    f.write(project.get_file(item).getbuffer())\n    f.close()\n    \nif b_use_census_data:\n    cols_to_drop=[]\nelse:\n    cols_to_drop=['CUSTOMER_ID']\n    \nfrom life_event_prep import LifeEventPrep\n\nlfe_prep = LifeEventPrep(target_event_type_ids=prediction_types,\n                         b_use_census_data=b_use_census_data,\n                         train_or_score='train',\n                         training_start_date=\"2010-01-01\",\n                         training_end_date=\"2017-08-01\",\n                         forecast_horizon=3,\n                         observation_window=4,\n                         life_event_minimum_target_count=100,\n                         cols_to_drop=cols_to_drop)\n\n# Prepare Home Purchase and Relocation Data\nprepped_data = lfe_prep.prep_data(events, 'train')", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "Before removing dates that are not in training period : (103426, 3)\nAfter removing dates that are not in training period : (45776, 3)\n\nNumber of customers before checking for enough history : 960\nNumber of customers after  checking for enough history : 789\n\nTotal number of events in the data : 44849\n\n\n2 Target ID(s) left after removing target events below threshold (100 customers)\n\nPrepping data for LFE_RELOCATION\nUsing observation_window = 4\nUsing forecast_horizon = 3\nTraining data has #Target> 1 customers: 0\nTraining data has #Target==1 customers: 575\n   Of those, we set 575 to positive\n   and we set 0 to negative\nTraining data has #Target==0 customers: 214\nNumber of records : 789\nNumber of unique customers : 789\nNumber of customers before removing those with no event data in observation window : 789\nNumber of customers target=1 before above filtering : 575\nNumber of customers after removing those with no event data in observation window : 489\nNumber of customers target=1 after above filtering : 342\n\nPrepping data for LFE_HOME_PURCHASE\nUsing observation_window = 4\nUsing forecast_horizon = 3\nTraining data has #Target> 1 customers: 0\nTraining data has #Target==1 customers: 468\n   Of those, we set 468 to positive\n   and we set 0 to negative\nTraining data has #Target==0 customers: 321\nNumber of records : 789\nNumber of unique customers : 789\nNumber of customers before removing those with no event data in observation window : 789\nNumber of customers target=1 before above filtering : 468\nNumber of customers after removing those with no event data in observation window : 521\nNumber of customers target=1 after above filtering : 314\n(452, 151)\n(479, 151)\n", "name": "stdout"}]}, {"metadata": {"id": "105f516e-1597-43ec-813b-8ee3e2be73da"}, "cell_type": "markdown", "source": "If you want to use the census data then call the prep function to add the probabilities to the data and display the census data plots showing the average probabilities by **`Location, Marital_status, Education, Gender, Employment, Income and Age`**."}, {"metadata": {"id": "6d86846f-fc29-494f-9d69-06a6ca6e3f98", "scrolled": false}, "cell_type": "code", "source": "census_df = pd.read_csv(project.get_file('census_probabilities.csv'))\n\ncustomer_data = pd.read_csv(project.get_file('customer.csv'))\n\n# if the user has selected to use the census data call the function to prep the census data and add the probabilities to the prepped data\n\nif b_use_census_data:\n    \n    from prep_census_data import census_data\n    census=census_data()\n    prepped_data=census.prep_census(census_df,customer_data,prepped_data,'train')", "execution_count": 12, "outputs": []}, {"metadata": {"id": "a2ed6da9-fc2b-46dc-98e4-bca596565aa0"}, "cell_type": "markdown", "source": "### Display Prepared Data\n\nThe final dataset contains one record per customer, with variables based on counts of the number of events that a customer had within a specified timeframe, the observation window. All event-related variables are prefixed with 'E_'. \n\nFor each event, we get a count of the number of times the customer experienced the event in the observation window. These variables are suffixed with \"\\_OW\". We also create variables for the number of times each customer experienced each event in the observation month (end month in observation window). These variables are suffixed with \"\\_OM\". \n\nIf the b_census_flag is set to True, each customer is matched to census data based on **`Location, Age, Education, Income, Profession and Gender to add MIGRATION_PROB, BIRTH_PROB, MARRIAGE_PROB and DIVORCE_PROB columns to the prepped dataset`**.\n\nThe dataset also contains a target variable indicating whether that customer experienced the life event or not within a particular timeframe, the forecast horizon.\n"}, {"metadata": {"id": "3c2b9cf2-b8c8-4d4b-baf2-5d1ef86d6357"}, "cell_type": "code", "source": "for event_type, df in prepped_data.items():\n    print('\\nTraining Data for '+event_type+':')\n    display(df.head())\n    print(\"{} rows, {} columns\\n\".format(*df.shape))\n    ", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "\nTraining Data for LFE_RELOCATION:\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "   E_ACNT_SEC_OPEN_*_OW  E_BIRTHDAY24_OW  E_BIRTHDAY26_OW  E_BIRTHDAY27_OW  \\\n0                   0.0              0.0              0.0              0.0   \n1                   0.0              0.0              0.0              0.0   \n2                   0.0              0.0              0.0              0.0   \n3                   0.0              0.0              0.0              0.0   \n4                   0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY28_OW  E_BIRTHDAY29_OW  E_BIRTHDAY32_OW  E_BIRTHDAY35_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY36_OW  E_BIRTHDAY39_OW  E_BIRTHDAY40_OW  E_BIRTHDAY42_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY51_OW  E_BIRTHDAY52_OW  E_BIRTHDAY54_OW  E_BIRTHDAY55_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY59_OW  E_BIRTHDAY68_OW  E_BIRTHDAY71_OW  E_BIRTHDAY73_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY75_OW  E_BIRTHDAY76_OW  E_BIRTHDAY77_OW  E_BIRTHDAY79_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY80_OW  E_BIRTHDAY83_OW  E_INT_LOGIN_WEB_OW  \\\n0              0.0              0.0                 0.0   \n1              0.0              0.0                 8.0   \n2              0.0              0.0                 0.0   \n3              0.0              0.0                 0.0   \n4              0.0              0.0                 0.0   \n\n   E_INT_OTHER_PHYSICAL_OW  E_MENTION_LFE_HOME_PURCHASE_OW  E_XCT_EQ_BUY_OW  \\\n0                      0.0                             0.0              0.0   \n1                      0.0                             0.0              6.0   \n2                      0.0                             0.0              0.0   \n3                      1.0                             0.0              0.0   \n4                      0.0                             0.0              0.0   \n\n   E_XCT_EQ_SELL_OW  E_XCT_MORTGAGE_NEW_OW  E_XFER_FUNDS_OUT_LARGE_OW  \\\n0               0.0                    0.0                        1.0   \n1               2.0                    0.0                        0.0   \n2               1.0                    0.0                        1.0   \n3               0.0                    1.0                        0.0   \n4               0.0                    0.0                        1.0   \n\n   E_ACNT_SEC_OPEN_*_OM  E_BIRTHDAY24_OM  E_BIRTHDAY27_OM  E_BIRTHDAY28_OM  \\\n0                   0.0              0.0              0.0              0.0   \n1                   0.0              0.0              0.0              0.0   \n2                   0.0              0.0              0.0              0.0   \n3                   0.0              0.0              0.0              0.0   \n4                   0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY39_OM  E_BIRTHDAY55_OM  E_BIRTHDAY68_OM  E_BIRTHDAY73_OM  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY77_OM  E_INT_LOGIN_WEB_OM  E_INT_OTHER_PHYSICAL_OM  \\\n0              0.0                 0.0                      0.0   \n1              0.0                 2.0                      0.0   \n2              0.0                 0.0                      0.0   \n3              0.0                 0.0                      0.0   \n4              0.0                 0.0                      0.0   \n\n   E_MENTION_LFE_HOME_PURCHASE_OM  E_XCT_EQ_BUY_OM  E_XCT_EQ_SELL_OM  \\\n0                             0.0              0.0               0.0   \n1                             0.0              0.0               0.0   \n2                             0.0              0.0               0.0   \n3                             0.0              0.0               0.0   \n4                             0.0              0.0               0.0   \n\n   E_XCT_MORTGAGE_NEW_OM  E_XFER_FUNDS_OUT_LARGE_OM  MONTH  \\\n0                    0.0                        1.0      9   \n1                    0.0                        0.0      1   \n2                    0.0                        1.0      8   \n3                    0.0                        0.0      5   \n4                    0.0                        1.0      9   \n\n   TOT_NB_OF_EVENTS_OW  TOT_NB_OF_EVENTS_OM  TARGET  \n0                  1.0                  1.0       0  \n1                 16.0                  2.0       0  \n2                  2.0                  1.0       1  \n3                  2.0                  0.0       1  \n4                  1.0                  1.0       1  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>E_ACNT_SEC_OPEN_*_OW</th>\n      <th>E_BIRTHDAY24_OW</th>\n      <th>E_BIRTHDAY26_OW</th>\n      <th>E_BIRTHDAY27_OW</th>\n      <th>E_BIRTHDAY28_OW</th>\n      <th>E_BIRTHDAY29_OW</th>\n      <th>E_BIRTHDAY32_OW</th>\n      <th>E_BIRTHDAY35_OW</th>\n      <th>E_BIRTHDAY36_OW</th>\n      <th>E_BIRTHDAY39_OW</th>\n      <th>E_BIRTHDAY40_OW</th>\n      <th>E_BIRTHDAY42_OW</th>\n      <th>E_BIRTHDAY51_OW</th>\n      <th>E_BIRTHDAY52_OW</th>\n      <th>E_BIRTHDAY54_OW</th>\n      <th>E_BIRTHDAY55_OW</th>\n      <th>E_BIRTHDAY59_OW</th>\n      <th>E_BIRTHDAY68_OW</th>\n      <th>E_BIRTHDAY71_OW</th>\n      <th>E_BIRTHDAY73_OW</th>\n      <th>E_BIRTHDAY75_OW</th>\n      <th>E_BIRTHDAY76_OW</th>\n      <th>E_BIRTHDAY77_OW</th>\n      <th>E_BIRTHDAY79_OW</th>\n      <th>E_BIRTHDAY80_OW</th>\n      <th>E_BIRTHDAY83_OW</th>\n      <th>E_INT_LOGIN_WEB_OW</th>\n      <th>E_INT_OTHER_PHYSICAL_OW</th>\n      <th>E_MENTION_LFE_HOME_PURCHASE_OW</th>\n      <th>E_XCT_EQ_BUY_OW</th>\n      <th>E_XCT_EQ_SELL_OW</th>\n      <th>E_XCT_MORTGAGE_NEW_OW</th>\n      <th>E_XFER_FUNDS_OUT_LARGE_OW</th>\n      <th>E_ACNT_SEC_OPEN_*_OM</th>\n      <th>E_BIRTHDAY24_OM</th>\n      <th>E_BIRTHDAY27_OM</th>\n      <th>E_BIRTHDAY28_OM</th>\n      <th>E_BIRTHDAY39_OM</th>\n      <th>E_BIRTHDAY55_OM</th>\n      <th>E_BIRTHDAY68_OM</th>\n      <th>E_BIRTHDAY73_OM</th>\n      <th>E_BIRTHDAY77_OM</th>\n      <th>E_INT_LOGIN_WEB_OM</th>\n      <th>E_INT_OTHER_PHYSICAL_OM</th>\n      <th>E_MENTION_LFE_HOME_PURCHASE_OM</th>\n      <th>E_XCT_EQ_BUY_OM</th>\n      <th>E_XCT_EQ_SELL_OM</th>\n      <th>E_XCT_MORTGAGE_NEW_OM</th>\n      <th>E_XFER_FUNDS_OUT_LARGE_OM</th>\n      <th>MONTH</th>\n      <th>TOT_NB_OF_EVENTS_OW</th>\n      <th>TOT_NB_OF_EVENTS_OM</th>\n      <th>TARGET</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>16.0</td>\n      <td>2.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}, {"output_type": "stream", "text": "452 rows, 53 columns\n\n\nTraining Data for LFE_HOME_PURCHASE:\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "   E_ACNT_SEC_OPEN_*_OW  E_BIRTHDAY26_OW  E_BIRTHDAY27_OW  E_BIRTHDAY28_OW  \\\n0                   0.0              0.0              0.0              0.0   \n1                   0.0              0.0              0.0              0.0   \n2                   0.0              0.0              0.0              0.0   \n3                   0.0              0.0              0.0              0.0   \n4                   0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY32_OW  E_BIRTHDAY35_OW  E_BIRTHDAY36_OW  E_BIRTHDAY39_OW  \\\n0              0.0              0.0              1.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY40_OW  E_BIRTHDAY42_OW  E_BIRTHDAY43_OW  E_BIRTHDAY44_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY50_OW  E_BIRTHDAY52_OW  E_BIRTHDAY55_OW  E_BIRTHDAY59_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY61_OW  E_BIRTHDAY62_OW  E_BIRTHDAY68_OW  E_BIRTHDAY69_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY70_OW  E_BIRTHDAY71_OW  E_BIRTHDAY72_OW  E_BIRTHDAY73_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY74_OW  E_BIRTHDAY75_OW  E_BIRTHDAY77_OW  E_BIRTHDAY79_OW  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY80_OW  E_BIRTHDAY81_OW  E_BIRTHDAY82_OW  E_INT_LOGIN_WEB_OW  \\\n0              0.0              0.0              0.0                 8.0   \n1              0.0              0.0              0.0                 0.0   \n2              0.0              0.0              0.0                 8.0   \n3              0.0              0.0              0.0                 0.0   \n4              0.0              0.0              0.0                 0.0   \n\n   E_INT_OTHER_PHYSICAL_OW  E_LFE_RELOCATION_OW  \\\n0                      0.0                  0.0   \n1                      0.0                  1.0   \n2                      0.0                  0.0   \n3                      0.0                  0.0   \n4                      0.0                  0.0   \n\n   E_MENTION_LFE_HOME_PURCHASE_OW  E_XCT_EQ_BUY_OW  E_XCT_EQ_SELL_OW  \\\n0                             0.0              4.0               2.0   \n1                             0.0              0.0               0.0   \n2                             0.0              8.0               2.0   \n3                             0.0              0.0               1.0   \n4                             0.0              0.0               0.0   \n\n   E_XCT_MORTGAGE_NEW_OW  E_XFER_FUNDS_OUT_LARGE_OW  E_ACNT_SEC_OPEN_*_OM  \\\n0                    0.0                        0.0                   0.0   \n1                    0.0                        0.0                   0.0   \n2                    0.0                        0.0                   0.0   \n3                    0.0                        1.0                   0.0   \n4                    0.0                        1.0                   0.0   \n\n   E_BIRTHDAY27_OM  E_BIRTHDAY35_OM  E_BIRTHDAY39_OM  E_BIRTHDAY44_OM  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY55_OM  E_BIRTHDAY62_OM  E_BIRTHDAY69_OM  E_BIRTHDAY70_OM  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY72_OM  E_BIRTHDAY74_OM  E_BIRTHDAY75_OM  E_BIRTHDAY77_OM  \\\n0              0.0              0.0              0.0              0.0   \n1              0.0              0.0              0.0              0.0   \n2              0.0              0.0              0.0              0.0   \n3              0.0              0.0              0.0              0.0   \n4              0.0              0.0              0.0              0.0   \n\n   E_BIRTHDAY79_OM  E_BIRTHDAY82_OM  E_INT_LOGIN_WEB_OM  \\\n0              0.0              0.0                 2.0   \n1              0.0              0.0                 0.0   \n2              0.0              0.0                 2.0   \n3              0.0              0.0                 0.0   \n4              0.0              0.0                 0.0   \n\n   E_INT_OTHER_PHYSICAL_OM  E_LFE_RELOCATION_OM  \\\n0                      0.0                  0.0   \n1                      0.0                  0.0   \n2                      0.0                  0.0   \n3                      0.0                  0.0   \n4                      0.0                  0.0   \n\n   E_MENTION_LFE_HOME_PURCHASE_OM  E_XCT_EQ_BUY_OM  E_XCT_EQ_SELL_OM  \\\n0                             0.0              1.0               0.0   \n1                             0.0              0.0               0.0   \n2                             0.0              2.0               0.0   \n3                             0.0              0.0               0.0   \n4                             0.0              0.0               0.0   \n\n   E_XCT_MORTGAGE_NEW_OM  E_XFER_FUNDS_OUT_LARGE_OM  MONTH  \\\n0                    0.0                        0.0      5   \n1                    0.0                        0.0      7   \n2                    0.0                        0.0      7   \n3                    0.0                        0.0      9   \n4                    0.0                        0.0     10   \n\n   TOT_NB_OF_EVENTS_OW  TOT_NB_OF_EVENTS_OM  TARGET  \n0                 15.0                  3.0       0  \n1                  1.0                  0.0       0  \n2                 18.0                  4.0       0  \n3                  2.0                  0.0       1  \n4                  1.0                  0.0       1  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>E_ACNT_SEC_OPEN_*_OW</th>\n      <th>E_BIRTHDAY26_OW</th>\n      <th>E_BIRTHDAY27_OW</th>\n      <th>E_BIRTHDAY28_OW</th>\n      <th>E_BIRTHDAY32_OW</th>\n      <th>E_BIRTHDAY35_OW</th>\n      <th>E_BIRTHDAY36_OW</th>\n      <th>E_BIRTHDAY39_OW</th>\n      <th>E_BIRTHDAY40_OW</th>\n      <th>E_BIRTHDAY42_OW</th>\n      <th>E_BIRTHDAY43_OW</th>\n      <th>E_BIRTHDAY44_OW</th>\n      <th>E_BIRTHDAY50_OW</th>\n      <th>E_BIRTHDAY52_OW</th>\n      <th>E_BIRTHDAY55_OW</th>\n      <th>E_BIRTHDAY59_OW</th>\n      <th>E_BIRTHDAY61_OW</th>\n      <th>E_BIRTHDAY62_OW</th>\n      <th>E_BIRTHDAY68_OW</th>\n      <th>E_BIRTHDAY69_OW</th>\n      <th>E_BIRTHDAY70_OW</th>\n      <th>E_BIRTHDAY71_OW</th>\n      <th>E_BIRTHDAY72_OW</th>\n      <th>E_BIRTHDAY73_OW</th>\n      <th>E_BIRTHDAY74_OW</th>\n      <th>E_BIRTHDAY75_OW</th>\n      <th>E_BIRTHDAY77_OW</th>\n      <th>E_BIRTHDAY79_OW</th>\n      <th>E_BIRTHDAY80_OW</th>\n      <th>E_BIRTHDAY81_OW</th>\n      <th>E_BIRTHDAY82_OW</th>\n      <th>E_INT_LOGIN_WEB_OW</th>\n      <th>E_INT_OTHER_PHYSICAL_OW</th>\n      <th>E_LFE_RELOCATION_OW</th>\n      <th>E_MENTION_LFE_HOME_PURCHASE_OW</th>\n      <th>E_XCT_EQ_BUY_OW</th>\n      <th>E_XCT_EQ_SELL_OW</th>\n      <th>E_XCT_MORTGAGE_NEW_OW</th>\n      <th>E_XFER_FUNDS_OUT_LARGE_OW</th>\n      <th>E_ACNT_SEC_OPEN_*_OM</th>\n      <th>E_BIRTHDAY27_OM</th>\n      <th>E_BIRTHDAY35_OM</th>\n      <th>E_BIRTHDAY39_OM</th>\n      <th>E_BIRTHDAY44_OM</th>\n      <th>E_BIRTHDAY55_OM</th>\n      <th>E_BIRTHDAY62_OM</th>\n      <th>E_BIRTHDAY69_OM</th>\n      <th>E_BIRTHDAY70_OM</th>\n      <th>E_BIRTHDAY72_OM</th>\n      <th>E_BIRTHDAY74_OM</th>\n      <th>E_BIRTHDAY75_OM</th>\n      <th>E_BIRTHDAY77_OM</th>\n      <th>E_BIRTHDAY79_OM</th>\n      <th>E_BIRTHDAY82_OM</th>\n      <th>E_INT_LOGIN_WEB_OM</th>\n      <th>E_INT_OTHER_PHYSICAL_OM</th>\n      <th>E_LFE_RELOCATION_OM</th>\n      <th>E_MENTION_LFE_HOME_PURCHASE_OM</th>\n      <th>E_XCT_EQ_BUY_OM</th>\n      <th>E_XCT_EQ_SELL_OM</th>\n      <th>E_XCT_MORTGAGE_NEW_OM</th>\n      <th>E_XFER_FUNDS_OUT_LARGE_OM</th>\n      <th>MONTH</th>\n      <th>TOT_NB_OF_EVENTS_OW</th>\n      <th>TOT_NB_OF_EVENTS_OM</th>\n      <th>TARGET</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>15.0</td>\n      <td>3.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>18.0</td>\n      <td>4.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}, {"output_type": "stream", "text": "479 rows, 66 columns\n\n", "name": "stdout"}]}, {"metadata": {"id": "bc0fc7e4-748d-473c-a5e1-b5e11db4c2c0"}, "cell_type": "markdown", "source": "### Save Prepared Dictionary as Pickle File\nSince the prepped data is a dictionary, we must save it as a pickle file in order to transfer the data to the model training notebook. This data will be called `prepared_data.pkl`"}, {"metadata": {"id": "34fe9999-87fb-49b2-bc35-c46c476c8584"}, "cell_type": "code", "source": "#function saving dictionary to pickle file on cp4d\ndef save_dict(obj, name ):\n    with open(name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n        print('SUCCESSFULLY SAVED')\n        f.close()\n\nsave_dict(prepped_data, 'prepared_data')", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "SUCCESSFULLY SAVED\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "We have now finished preparing the dataset and saved out the prepped data for modelling. See notebook `2-model-training` for the next step."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\nSample Materials, provided under <a href=\"https://github.com/IBM/Industry-Accelerators/blob/master/CPD%20SaaS/LICENSE\" target=\"_blank\" rel=\"noopener noreferrer\">license.</a> <br>\nLicensed Materials - Property of IBM. <br>\n\u00a9 Copyright IBM Corp. 2019, 2021. All Rights Reserved. <br>\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}