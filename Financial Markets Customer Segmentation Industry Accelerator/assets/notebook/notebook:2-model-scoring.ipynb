{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Industry Accelerators - Financial Markets Customer Segmentation Model"}, {"metadata": {}, "cell_type": "markdown", "source": "## Introduction\n\nNow that we have built the machine learning pipeline, stored and deployed it using <a href=\"http://ibm-wml-api-pyclient.mybluemix.net\" target=\"_blank\" rel=\"noopener noreferrer\">ibm-watson-machine-learning</a>[]() , we can use the pipeline to ingest new data, prep it and score it. \n"}, {"metadata": {}, "cell_type": "markdown", "source": "\nBefore executing this notebook on IBM Cloud, you need to:<br>\n1) When you import this project on an IBM Cloud environment, a project access token should be inserted at the top of this notebook as a code cell. <br>\nIf you do not see the cell above, Insert a project token: Click on **More -> Insert project token** in the top-right menu section and run the cell <br>\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n2) Provide your IBM Cloud API key in the subsequent cell<br>\n3) You can then step through the notebook execution cell by cell, by selecting Shift-Enter. Or you can execute the entire notebook by selecting **Cell -> Run All** from the menu.<br>\n"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Insert IBM Cloud API key\n\nYour Cloud API key can be generated by going to the [API Keys section of the Cloud console](https://cloud.ibm.com/iam/apikeys). From that page, scroll down to the API Keys section, and click Create an IBM Cloud API key. Give your key a name and click Create, then copy the created key and paste it below. \n\nIf you are running this notebook on cloud pak for data on-prem, leave the ibmcloud_api_key field blank."}, {"metadata": {}, "cell_type": "code", "source": "ibmcloud_api_key = ''", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "try:\n    project\nexcept NameError:\n    # READING AND WRITING PROJECT ASSETS\n    import project_lib\n    project = project_lib.Project() ", "execution_count": 24, "outputs": []}, {"metadata": {"collapsed": true, "id": "e9dad1ed-3fa9-4766-890d-90eb480722f8"}, "cell_type": "markdown", "source": "## Create and Test Scoring Pipeline "}, {"metadata": {"id": "6ea2c023-ea63-405a-a245-96aba7610685"}, "cell_type": "markdown", "source": "In this notebook we will:\n\n- Programmatically get the ID's for the deployment space and model deployment that were created in the **2-model-training** notebook.\n- Promote assets required for scoring new data into the deployment space.\n- Create a deployable function which will take raw data for scoring, prep it into the format required for the models and score it.\n- Deploy the function.\n- Create the required payload, invoke the deployed function and return clusters.\n"}, {"metadata": {"id": "689d0e8b-b79c-4789-847a-db9d23ed6a67"}, "cell_type": "code", "source": "import os\nimport pandas as pd\nimport datetime\nfrom ibm_watson_machine_learning import APIClient\n\nif ibmcloud_api_key != '':\n    wml_credentials = {\n        \"apikey\": ibmcloud_api_key,\n        \"url\": 'https://' + os.environ['RUNTIME_ENV_REGION'] + '.ml.cloud.ibm.com'\n    }\nelse:\n    token = os.environ['USER_ACCESS_TOKEN']\n    wml_credentials = {\n        \"token\": token,\n        \"instance_id\" : \"openshift\",\n        \"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n        \"version\": \"3.5\"\n     }\nclient = APIClient(wml_credentials)", "execution_count": 25, "outputs": []}, {"metadata": {"id": "e65551ba-4dd6-4026-980a-de312bd3f517"}, "cell_type": "markdown", "source": "### User Inputs\n\nEnter the  csv file with raw data to be scored.  The file will be downloaded to the local path to promote it to deployment space later."}, {"metadata": {"id": "fa79f83a-e983-47e8-9743-b5c95c0b92e2"}, "cell_type": "code", "source": "# specify the location of the csv file with raw data that we would like to score for\nfilename = 'customer_full_summary_latest.csv'\nf = open(filename, 'w+b')\nf.write(project.get_file(filename).getbuffer())\nf.close()", "execution_count": 26, "outputs": []}, {"metadata": {"id": "a3c96f6b-3274-49cb-b0bc-6faafc22d27e"}, "cell_type": "markdown", "source": "### Set up Deployment Space, Deployments and Assets\n\nThe following code programmatically gets the deployment space and the model deployment details which were created in **2-model-training**. \nWe use the space name and deployment names that were used when creating the deployments as specified below. If multiple deployments within the selected space have the same name, the most recently created deployment is used.\n\nAlternatively, you can manually enter the space and deployment id's.\n\nThe code also promotes some assets into the deployment space, specifically, the dataset with raw data for scoring, the python script file which is used for prepping the data, the metadata that was stored when prepping the data and the PCA object that was created during training. By promoting these assets into the deployment space, they are available and can be accessed by the deployed function. "}, {"metadata": {"id": "26358342-2632-4601-a55c-8034768cfbf3"}, "cell_type": "code", "source": "space_name = 'Customer Segmentation Space'\nmodel_name = 'customer_segmentation_model'\ndeployment_name = 'customer_segmentation_model_deployment'", "execution_count": 27, "outputs": []}, {"metadata": {"id": "00b1a383-66b3-4e83-a6f7-3094efc190bd"}, "cell_type": "markdown", "source": "Get the space we are working in, which is found using the name that were hardcoded in **2-model-training**. If you like to use a different space manually set the **space_id**.\n\nSet the space as the default space for working."}, {"metadata": {"id": "d1a6a9a2-d924-4f31-b1c0-5fb1174962b6"}, "cell_type": "code", "source": "l_space_details = []\nl_space_details_created_times = []\nfor space_details in client.spaces.get_details()['resources']:\n    if space_details['entity']['name'] == space_name:\n        space_id=space_details['metadata']['id']\n\n# set this space as default space\nclient.set.default_space(space_id)", "execution_count": 28, "outputs": [{"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "'SUCCESS'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "l_deployment_details = []\nl_deployment_details_created_times = []\n\nfor deployment in client.deployments.get_details()['resources']:\n        \n\n        if deployment['entity']['name'] == deployment_name:            \n                l_deployment_details.append(deployment)\n                l_deployment_details_created_times.append(datetime.datetime.strptime(deployment['metadata']['created_at'],  '%Y-%m-%dT%H:%M:%S.%fZ'))\n                \n                \n\n# get the index of the latest created date from the list and use that to get the deployment_id\nlist_latest_index = l_deployment_details_created_times.index(max(l_deployment_details_created_times))\ndeployment_id = l_deployment_details[list_latest_index]['metadata']['id']\nprint(\"Deployment ID of\",deployment_name,\"is\",deployment_id)", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "Deployment ID of customer_segmentation_model_deployment is 504ec2f9-5ebc-44ef-ab66-73fc19e2eeff\n", "name": "stdout"}]}, {"metadata": {"id": "52690d6e235045fa816e2974c3a5e494"}, "cell_type": "markdown", "source": "### Promote Assets to Deployment space"}, {"metadata": {"id": "31daa5e3-595c-43bb-9d7d-d05dbdc7ab8b"}, "cell_type": "markdown", "source": "Promote the assets into the deployment space. We will use the prep script for getting the raw data into the format required for scoring. We also need the prep metadata that was saved as json during the prep for training, this ensures that the user inputs specified for prepping the data for training are the same as the ones used for scoring. Also store the PCA object used in training and the raw data dataset in the deployment space.\n\nWe add these assets into the deployment space.  Also store the raw data dataset in the deployment space."}, {"metadata": {"id": "02f84e95-539a-4724-ae16-cf27622675c1"}, "cell_type": "code", "source": "# we will use the prep script for getting the raw data into the format required for scoring\n# we also need the prep metadata that was saved as json during the prep for training - this ensures that the user inputs specified for prepping the data for training are the same as the ones used for scoring.\n# we need to add these files into the deployment space\nasset_details_json = client.data_assets.create('training_data_metadata.json', file_path='training_data_metadata.json')\nasset_details_script = client.data_assets.create('customer_segmentation_prep.py', file_path='customer_segmentation_prep.py')\n\n# get the pca object created in training - this file was saved as .txt so that the mimetype could be recognised when creating the asset\nasset_details_pca = client.data_assets.create('pca.joblib', file_path='pca.txt')\n\nasset_details_dataset = client.data_assets.create(filename, file_path=filename)", "execution_count": null, "outputs": [{"output_type": "stream", "text": "Creating data asset...\nSUCCESS\nCreating data asset...\nSUCCESS\nCreating data asset...\nSUCCESS\nCreating data asset...\n", "name": "stdout"}]}, {"metadata": {"id": "08c03cbd-8041-4287-87f6-c19bf1316a0b"}, "cell_type": "code", "source": "client.data_assets.list()", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "--------------------------------  ----------  -------  ------------------------------------\nNAME                              ASSET_TYPE  SIZE     ASSET_ID\ncustomer_full_summary_latest.csv  data_asset  1607327  bb16a02b-7944-49bc-b230-e032f84cc549\npca.joblib                        data_asset  6857     ce86f8af-940c-4c7b-886f-942619968f55\ntraining_data_metadata.json       data_asset  3404     7a3dde2f-d037-41ab-8caf-e91acb2b841f\ncustomer_segmentation_prep.py     data_asset  17871    1cd9260b-d0e4-4d5f-b77d-d0e5c8f55406\n--------------------------------  ----------  -------  ------------------------------------\n", "name": "stdout"}]}, {"metadata": {"id": "90851230-863c-4ae6-beae-d12636df8726"}, "cell_type": "markdown", "source": "### Create the Deployable Function\n\nFunctions can be deployed in Watson Machine Learning in the same way models can be deployed. The python client or REST API can be used to send data to the deployed function. Using the deployed function allows us to prepare the data and pass it to the model for scoring all within the deployed function.\n\nWe start off by creating the dictionary of default parameters to be passed to the function. We get the ID's of all assets that have been promoted into the deployment space. We also add the model deployment ID and space ID information into the dictionary.\n"}, {"metadata": {"id": "2b3292cf-091c-46e0-8795-93c678f6c190"}, "cell_type": "code", "source": "# get the assets that were stored in the space - in this version of the package we need to manually assign the id\nmetadata_id = asset_details_json['metadata']['guid']\nprep_id = asset_details_script['metadata']['guid']\npca_id = asset_details_pca['metadata']['guid']\ndataset_id = asset_details_dataset['metadata']['guid']", "execution_count": 11, "outputs": []}, {"metadata": {"id": "274f74f9-b9cb-4c65-a648-c79cf4380673"}, "cell_type": "code", "source": "assets_dict = {'dataset_asset_id' : dataset_id, 'metadata_asset_id' : metadata_id, 'pca_asset_id' : pca_id,\n                   'prep_script_asset_id' : prep_id, 'dataset_name' : filename}", "execution_count": 12, "outputs": []}, {"metadata": {"id": "3d4b22e5-6c20-4088-a470-2d900698dd39"}, "cell_type": "code", "source": "# create the wml_credentials again. After already creating the client using the credentials, the instance_id gets updated to 999\n# re-create the dictionary so that the correct instance_id is used\n#wml_credentials[\"instance_id\"] = \"openshift\"\n    \nai_parms = {'wml_credentials' : wml_credentials,'space_id' : space_id, 'assets' : assets_dict, 'model_deployment_id' : deployment_id}", "execution_count": 13, "outputs": []}, {"metadata": {"id": "41afa621-5f19-4944-97c6-1e79becbb571"}, "cell_type": "markdown", "source": "### Scoring Pipeline Function\n\nThe function below takes new customers to be scored as a payload. It preps the customer raw data, loads the model, executes the model scoring and assigns each customer to a cluster. \n\nThe following rules are required to make a valid deployable function:\n\n* The deployable function must include a nested function named \"score\".\n* The score function accepts a list.\n* The list must include an array with the name \"values\".\n* The score function must return an array with the name \"predictions\", with a list as the value, which in turn contains an array with the name \"values\". Example: ```{\"predictions\" : [{'values' : }]}```\n* We pass these into the function:\n\n - Default parameters\n - Credentials and space detail\n - Details of the assets that were promoted into the space\n - Model deployment guid\n\n* The assets are downloaded into the deployment space and imported as variables. The raw data to be scored is then prepared and the function calls the model deployment endpoint to score and return predictions. "}, {"metadata": {"id": "0d20abf9-d348-47af-970f-4a8a6b791c5e"}, "cell_type": "code", "source": "def scoring_pipeline(parms=ai_parms):\n     \n    import pandas as pd\n    import requests\n    import os\n    import json\n    import joblib\n   \n    from ibm_watson_machine_learning import APIClient\n    client = APIClient(parms[\"wml_credentials\"])\n    client.set.default_space(parms['space_id'])\n    \n    # call the function to download the stored dataset asset and return the path\n    dataset_path = client.data_assets.download(parms['assets']['dataset_asset_id'], parms['assets']['dataset_name'])\n    df_raw = pd.read_csv(dataset_path, infer_datetime_format=True, \n                             parse_dates=['CUSTOMER_RELATIONSHIP_START_DATE', 'CUSTOMER_SUMMARY_END_DATE', 'CUSTOMER_SUMMARY_START_DATE'])\n    \n    \n    # call the function to download the prep script and return the path\n    prep_script_path = client.data_assets.download(parms['assets']['prep_script_asset_id'], 'prep_data_script.py')\n    # remove the rest of path and .py at end of file name to get the name of the script for importing\n    script_name = os.path.basename(prep_script_path).replace('.py', '')\n    \n    \n    # call the function to download the pca joblib file and return the path\n    pca_object_path = client.data_assets.download(parms['assets']['pca_asset_id'], 'pca.joblib')\n    pca = joblib.load(pca_object_path)\n\n    # call the function to download the prep metadata and return the path\n    metadata_path = client.data_assets.download(parms['assets']['metadata_asset_id'], 'user_inputs.json')   \n    \n    def prep(cust_ids, scoring_date):\n        import requests\n        import os\n        # import the prep script that we downloaded into the deployment space\n        prep_data_script = __import__(script_name)\n        \n        \n        with open(metadata_path, 'r') as f:\n            metadata = json.load(f)\n        \n        # create new variables for all elements in the dictionary. A new variable is created for each key\n        globals().update(metadata)\n        \n        input_df = df_raw[df_raw[granularity_key].isin(cust_ids)]\n        \n        # call the script to prep the data\n        scoring_prep = prep_data_script.CustomerSegmentationPrep('score', granularity_key=granularity_key,\n                                            customer_start_date=customer_start_date,\n                                            customer_end_date=customer_end_date,\n                                            status_attribute=status_attribute,\n                                            status_flag_active=status_flag_active,\n                                            date_customer_joined=date_customer_joined,\n                                            columns_required=columns_required,\n                                            default_attributes=default_attributes,\n                                            risk_tolerance_list=risk_tolerance_list,\n                                            investment_objective_list=investment_objective_list,\n                                            effective_date=scoring_date,\n                                            std_multiplier=std_multiplier,\n                                            max_num_cat_cardinality=max_num_cat_cardinality,\n                                            nulls_threshold=nulls_threshold)\n        prepped_data = scoring_prep.prep_data(input_df, 'score')\n    \n        if prepped_data is None:\n            print(\"Data prep filtered out customer data. Unable to score.\", file=sys.stderr)\n            return None\n\n        # handle empty data\n        if prepped_data.shape[0] == 0:\n            print(\"Data prep filtered out customer data. Unable to score.\", file=sys.stderr)\n            return None\n    \n        # the dataset contains a mix of continuous variables and categorical variables\n        # categorical variables are converted into dummy variables\n        # continuous variables are standardised using z-score\n\n        categorical_cols = list(prepped_data.select_dtypes(include=[object]).columns)\n        # create dummy variables for categorical variables and drop original\n        for col in categorical_cols:\n            prepped_data = pd.concat([prepped_data, pd.get_dummies(prepped_data[col], prefix=col, drop_first=True)], axis=1)\n            prepped_data.drop(col, axis=1, inplace=True)\n        \n        # since we're using distance based clustering, we need to scale numeric variables\n        # z score was used in training. We stored the mean and standard deviations used in standardscaler in the metadata, use these to standardise new data\n        for i in range(0, len(cols_to_standardise)):\n            current_col = cols_to_standardise[i]\n            current_col_mean = scaler_means[i]\n            current_col_standard_dev = scaler_standard_dev[i]\n            # scale the variable \n            prepped_data[current_col] = (prepped_data[current_col] - current_col_mean) / current_col_standard_dev\n        \n        # if a column does not exist in scoring but is in training, add the column to scoring dataset\n        for col in cols_used_for_training:\n            if col not in list(prepped_data.columns):\n                prepped_data[col] = 0\n\n        # if a column exists in scoring but not in training, delete it from scoring dataset\n        for col in list(prepped_data.columns):\n            if col not in cols_used_for_training:\n                prepped_data.drop(col, axis=1, inplace=True)\n\n        # make sure order of scoring columns is same as training dataset\n        prepped_data = prepped_data[cols_used_for_training]\n        \n        # get the pca object that was loaded in the outer function  and apply to the prepped dataset\n        prepped_data = pd.DataFrame(pca.transform(prepped_data))\n        \n        return prepped_data\n    \n    def score(payload):\n        import json\n        \n        scoring_date = payload['input_data'][0]['values']\n        cust_ids = payload['input_data'][0]['cust_id']\n        \n        prepped_data = prep(cust_ids, scoring_date)\n        \n        if prepped_data is None:\n            return {\"predictions\" : [{'values' : 'Data prep filtered out customer data. Unable to score.'}]}\n        elif prepped_data.shape[0] == 0:\n            return {\"predictions\" : [{'values' : 'Data prep filtered out customer data. Unable to score.'}]}\n        else:\n            \n            scoring_payload = {\"input_data\":  [{ \"values\" : prepped_data.values.tolist()}]}\n            \n            response_scoring = client.deployments.score(parms['model_deployment_id'], scoring_payload)\n            \n        \n        result = []\n        # increment each clsuter by 1 so that it starts at 1 instead of 0\n        for cluster_num in response_scoring['predictions'][0]['values']:\n            result.append(cluster_num[0] + 1)\n\n        return {\"predictions\" : [{'values' : result}]}\n        \n    return score", "execution_count": 14, "outputs": []}, {"metadata": {"id": "3d1abca7-7587-43dc-bb3e-f122b0da89bc"}, "cell_type": "markdown", "source": "### Deploy the Function\n\nThe user can specify the name of the function and deployment in the code below. "}, {"metadata": {"id": "2a046bfc-1fc9-48aa-8f3f-703022139eaf"}, "cell_type": "code", "source": "# store the function and deploy it \nfunction_name = 'customer_segmentation_scoring_pipeline_function'\nfunction_deployment_name = 'customer_segmentation_scoring_pipeline_function_deployment'", "execution_count": 15, "outputs": []}, {"metadata": {"id": "c8d0e02c-17f0-444c-81e5-24101e86acb4"}, "cell_type": "markdown", "source": "### Get the ID of software specification to be used with the function\n\nWe use tags, input data schemas, output data schemas and software specifications in the metadata to store the function. Input data schemas provide an easy option to input data to score in the deployment space. Example to create a metatadata to store the function can be viewed using `client.repository.FunctionMetaNames.get_example_values()`.\nSimilarly, example to create a metatadata to deploy the function can be viewed using `client.deployments.ConfigurationMetaNames.get_example_values()` <br>\nThe Software Specification refers to the runtime used in the Notebook, WML training and WML deployment. We use the software specification `default_py3.7` to store the function. We get the ID of the software specification and include it in the metadata when storing the function. Available Software specifications can be retrieved using `client.software_specifications.list()`.\n"}, {"metadata": {"id": "e4d685a6-e1e9-4d47-968b-bbc7400fa3b7"}, "cell_type": "code", "source": "software_spec_id = client.software_specifications.get_id_by_name(\"default_py3.7\")", "execution_count": 16, "outputs": []}, {"metadata": {"id": "cbcfe714-3b39-4767-8742-dfd1b2629546"}, "cell_type": "code", "source": "# add the metadata for the function and deployment    \nmeta_data = {\n    client.repository.FunctionMetaNames.NAME : function_name,\n    client.repository.FunctionMetaNames.TAGS : ['customer_segmentation_scoring_pipeline_function_tag'],\n    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: software_spec_id,    \n}\n\nfunction_details = client.repository.store_function(meta_props=meta_data, function=scoring_pipeline)\n\nfunction_id = function_details[\"metadata\"][\"id\"]\n\nmeta_props = {\n    client.deployments.ConfigurationMetaNames.NAME: function_deployment_name,\n    client.deployments.ConfigurationMetaNames.TAGS : ['customer_segmentation_scoring_pipeline_function_deployment_tag'],\n    client.deployments.ConfigurationMetaNames.ONLINE: {},\n    \n}\n\n# deploy the stored model\nfunction_deployment_details = client.deployments.create(artifact_uid=function_id, meta_props=meta_props)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '6460d1bd-2574-4c12-8883-a1d4e5dacc38' started\n\n#######################################################################################\n\n\ninitializing........\nready\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='66dbc2f5-f572-49de-afdd-95bbf51fc59d'\n------------------------------------------------------------------------------------------------\n\n\n", "name": "stdout"}]}, {"metadata": {"id": "c395608e-e1dd-4120-bf45-14e896ec570a"}, "cell_type": "markdown", "source": "### Score New Data\n\nGet the guid of the deployed function, create the payload and use the python client to score the data. The deployed function returns the assigned clusters for each customer to be scored. \n\nThe payload contains two values. The first is the effective date for scoring. This is the date that the clustering is computed. The second value is a list of customer ID's who we would like to make the prediction for. "}, {"metadata": {"id": "1c6e3589-37a5-4022-83cf-ee77d0d33846"}, "cell_type": "code", "source": "scoring_deployment_id = client.deployments.get_uid(function_deployment_details)\nclient.deployments.get_details(scoring_deployment_id)", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "{'entity': {'asset': {'id': '6460d1bd-2574-4c12-8883-a1d4e5dacc38'},\n  'custom': {},\n  'deployed_asset_type': 'function',\n  'hardware_spec': {'id': 'Not_Applicable', 'name': 'XS', 'num_nodes': 1},\n  'name': 'customer_segmentation_scoring_pipeline_function_deployment',\n  'online': {},\n  'space_id': '8b134edd-9809-4e60-94ff-cca6afc7789f',\n  'status': {'online_url': {'url': 'https://us-south.ml.cloud.ibm.com/ml/v4/deployments/66dbc2f5-f572-49de-afdd-95bbf51fc59d/predictions'},\n   'state': 'ready'}},\n 'metadata': {'created_at': '2021-04-13T14:45:39.244Z',\n  'id': '66dbc2f5-f572-49de-afdd-95bbf51fc59d',\n  'modified_at': '2021-04-13T14:45:39.244Z',\n  'name': 'customer_segmentation_scoring_pipeline_function_deployment',\n  'owner': 'IBMid-664001TADX',\n  'space_id': '8b134edd-9809-4e60-94ff-cca6afc7789f',\n  'tags': ['customer_segmentation_scoring_pipeline_function_deployment_tag']}}"}, "metadata": {}}]}, {"metadata": {"id": "5bcb05fe-5421-4650-af2a-7926bd5ad179"}, "cell_type": "code", "source": "cust_ids = [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008]\n\npayload = [{'values' : \"2018-09-30\", 'cust_id' : cust_ids}]\n\npayload_metadata = {client.deployments.ScoringMetaNames.INPUT_DATA: payload}\n# score\nfunct_output = client.deployments.score(scoring_deployment_id, payload_metadata)\nfunct_output", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "{'predictions': [{'values': [6, 1, 5, 5, 7, 4, 5, 4, 3]}]}"}, "metadata": {}}]}, {"metadata": {"id": "98be1415b4ae4a3aafd70725deea6a8a"}, "cell_type": "markdown", "source": "**The R Shiny Dashboard invokes this scoring pipeline for visualizing the results. Follow the instructions from Readme to launch R-Shiny dashboard.**"}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\nThis project contains Sample Materials, provided under this <a href=\"https://github.com/IBM/Industry-Accelerators/blob/master/CPD%20SaaS/LICENSE\" target=\"_blank\" rel=\"noopener noreferrer\">license</a>. <br/>\nLicensed Materials - Property of IBM. <br/>\n\u00a9 Copyright IBM Corp. 2019, 2020, 2021. All Rights Reserved. <br/>\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.<br/>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}