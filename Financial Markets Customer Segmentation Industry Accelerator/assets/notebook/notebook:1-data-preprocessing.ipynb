{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Industry Accelerators - Financial Markets Customer Segmentation Model"}, {"metadata": {}, "cell_type": "markdown", "source": "## Data Preprocessing"}, {"metadata": {}, "cell_type": "markdown", "source": "## Introduction\n\n\nIn this notebook we will be going through an end-to-end project to load in long form transactional type data, prepare the data into a wide format. The summary and demographic information is analyzed at a client level. The model input data structure is a wide form data structure (multiple rows per client), organized by **`Customer ID`** as the key field. We will use the function **`CustomerSegmentationPrep()`** to prepare the data."}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "Before executing this notebook on IBM Cloud :<br>\n1) When you import this project on an IBM Cloud environment, a project access token should be inserted at the top of this notebook as a code cell. <br>\nIf you do not see the cell above, Insert a project token: Click on **More -> Insert project token** in the top-right menu section and run the cell <br>\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n2) You can then step through the notebook execution cell by cell, by selecting Shift-Enter. Or you can execute the entire notebook by selecting **Cell -> Run All** from the menu.<br>\n"}, {"metadata": {}, "cell_type": "code", "source": "try:\n    project\nexcept NameError:\n    # READING AND WRITING PROJECT ASSETS\n    import project_lib\n    project = project_lib.Project() ", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Load Customer Segmentation Data\n\nFor this project we will be loading the csv file called **customer_full_summary_latest.csv**. The file is located in the `data_assets`. We use **project_lib** library to fetch and save the files associated with the project.\n\nThe easiest way to load in data is to use the <b>Find and Add Data</b> icon in the upper right hand corner. Once selected you will see a sidebar come out with options to load from either Files or Connections.\n\nIf you loaded your dataset into a Watson Studio analytics project, like a CSV file, then select Files and you should be able to find your dataset name. From there you can click the <b>Insert to code</b> and select to either insert a pandas dataframe or a spark dataframe. Once you make the selection you'll see python code inserted into the notebook cell with either Pandas or PySpark code for reading in your data. Now you're ready to explore and manipulate your dataset. "}, {"metadata": {}, "cell_type": "markdown", "source": "In the cell below we import the python libraries that we will use throughout the notebook."}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 1000)\nimport json\nimport importlib\nimport warnings\nimport sys\nimport time\nimport os\nimport pickle \nimport shutil\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.preprocessing import StandardScaler\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(0)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## User Inputs and Data Prep\n\n\n### Data Prep\n\n**`CustomerSegmentationPrep()`** function is used for data preparation.\n\nThe function generates the dataset that is used for clustering. We take a wide form dataset with customer details, filter to include only columns that are relevant, complete data cleaning and produce a dataframe suitable for clustering. \n\n\n### User Inputs\n\n**effective_date :**  This is the date that the segmentation is computed. All input data should be before this date.<br>\n**train_or_score :**  Specify whether we are prepping the data for training or scoring. Should always be 'train' in this notebook.<br>\n\n**granularity_key :** Specifies the customer ID column.<br>\n**customer_start_date :** Column with the start of the summary month of customer data.<br> \n**customer_end_date :** As above, but last day of the summary month.<br>\n**status_attribute :** Column which indicates whether the customer is active or inactive and is used to define churn. Churned customers are removed from the dataset.<br>\n**status_flag_active :** The name of the variable in the status_attribute that indicates that the customer has churned, in this case it is 'Inactive'.<br>\n**date_customer_joined :** Specifies the column where the customer join date is recorded. This variable is used to calculate customer tenure.<br>\n\n**columns_required :** A list of default columns required, includes ID column and date columns.<br>\n**default_attributes :** A list of the variables that we would like to use for the segmentation.<br>\n**risk_tolerance_list :** A list of the risk categories for the customer's accounts. 'High', 'Low' etc.<br> \n**investment_objective_list :** A list of the investment objective categories for the customer's accounts. 'Security', 'Income' etc.<br>\n\nThe last three user input variables are used for data cleaning.<br>\n**std_multiplier :** This variable is used to identify outlier values. This number is multiplied by the variable standard deviation. Any value above this is defined as an outlier and the value is capped at this number multiplied by the standard deviation.<br>\n**max_num_cat_cardinality :** This variable defines the maximum cardinality for categorical variables. Any categorical variable with more categories than this maximum is removed from the dataset.<br> \n**nulls_threshold :** This threshold is used to identify columns with many null values. Any column with percentage of nulls greater than this threshold will be removed from the dataset.<br>\n\nThe user can use the default inputs as listed below or can choose their own. The user inputs will be stored and the same inputs will be applied automatically at scoring time. \n\n\n### Data Cleaning\n\n\u2022\tAny customer who attrited in the dataset is removed. Only active customers are used for clustering.<br>\n\u2022\tWe take the most recent record for each customer.<br>\n\u2022\tAny columns in the dataset that have a single constant value are removed.<br>\n\u2022\tAny column with more than 10% null values is removed.<br>\n\u2022\tHigh cardinality categorical columns are removed.<br>\n\u2022\tNumerical outliers are cleaned. <br>\n\u2022\tRemaining missing values are filled with 'Unknown' for categorical and the average of the column for numerical. "}, {"metadata": {}, "cell_type": "code", "source": "%%writefile customer_segmentation_prep.py\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nimport sys\nimport json\nimport os\n\nclass CustomerSegmentationPrep():\n    \n    def __init__(self, train_or_score, \n        granularity_key='CUSTOMER_CUSTOMER_ID',\n        customer_start_date='CUSTOMER_SUMMARY_START_DATE',\n        customer_end_date='CUSTOMER_SUMMARY_END_DATE',\n        status_attribute='CUSTOMER_STATUS',\n        status_flag_active='Active',\n        date_customer_joined='CUSTOMER_RELATIONSHIP_START_DATE',\n        columns_required=['CUSTOMER_CUSTOMER_ID', 'CUSTOMER_STATUS', 'CUSTOMER_SUMMARY_START_DATE', 'CUSTOMER_SUMMARY_END_DATE',\n                        'CUSTOMER_EFFECTIVE_DATE',  'CUSTOMER_SYSTEM_LOAD_TIMESTAMP'], \n        default_attributes=['CUSTOMER_GENDER', 'CUSTOMER_AGE_RANGE', 'CUSTOMER_EDUCATION_LEVEL',\n                                'CUSTOMER_EMPLOYMENT_STATUS', 'CUSTOMER_MARITAL_STATUS', 'CUSTOMER_NUMBER_OF_DEPENDENT_CHILDREN',\n                                'CUSTOMER_URBAN_CODE', 'CUSTOMER_ANNUAL_INCOME', 'CUSTOMER_RELATIONSHIP_START_DATE',\n                                'CUSTOMER_SUMMARY_FUNDS_UNDER_MANAGEMENT', 'CUSTOMER_SUMMARY_RETURN_SINCE_INCEPTION',\n                                'CUSTOMER_SUMMARY_RETURN_LAST_QUARTER', 'CUSTOMER_SUMMARY_ASSETS',\n                                'CUSTOMER_SUMMARY_NUMBER_OF_ACTIVE_ACCOUNTS', 'CUSTOMER_SUMMARY_NUMBER_OF_EMAILS',\n                                'CUSTOMER_SUMMARY_NUMBER_OF_LOGINS', 'CUSTOMER_SUMMARY_NUMBER_OF_CALLS',\n                                'CUSTOMER_SUMMARY_TOTAL_NUMBER_OF_BUY_TRADES', 'CUSTOMER_SUMMARY_TOTAL_NUMBER_OF_SELL_TRADES',\n                                'CUSTOMER_SUMMARY_TOTAL_AMOUNT_OF_ALL_FEES'] , \n        risk_tolerance_list = [], investment_objective_list = [], effective_date = '2018-09-30', std_multiplier=5, max_num_cat_cardinality=10,\n        nulls_threshold=0.1):\n            \n        self.train_or_score = train_or_score\n        self.columns_required = columns_required\n        self.default_attributes = default_attributes \n        self.granularity_key = granularity_key\n        self.date_customer_joined = date_customer_joined \n        self.customer_end_date = customer_end_date  \n        self.customer_start_date = customer_start_date \n        self.risk_tolerance_list = risk_tolerance_list\n        self.investment_objective_list = investment_objective_list \n        self.effective_date = effective_date\n        self.status_attribute = status_attribute\n        self.status_flag_active = status_flag_active\n        self.std_multiplier = std_multiplier\n        self.max_num_cat_cardinality = max_num_cat_cardinality\n        self.nulls_threshold = nulls_threshold\n\n        # if effective date is a date convert it to a string for consistency\n        if isinstance(self.effective_date, datetime.datetime):\n            self.effective_date = datetime.datetime.strftime(self.effective_date, '%Y-%m-%d')\n            \n        if self.train_or_score == 'train':\n            # create a dictionary with all values for user inputs. We will save this out and use it for scoring\n            # to ensure that the user inputs are consistent across train and score notebooks\n            # exclude variables that won't be used for scoring\n            self.user_inputs_dict = { 'columns_required' : columns_required, 'default_attributes' : default_attributes,\n                'granularity_key' : granularity_key, 'date_customer_joined' : date_customer_joined, \n                'customer_end_date' : customer_end_date, 'customer_start_date' : customer_start_date,\n                'risk_tolerance_list' : risk_tolerance_list, 'investment_objective_list' : investment_objective_list,\n                'effective_date' : effective_date, 'status_attribute' : status_attribute,\n                'status_flag_active' : status_flag_active, 'std_multiplier' : std_multiplier,\n                'max_num_cat_cardinality' : max_num_cat_cardinality, 'nulls_threshold' : nulls_threshold }\n\n    # function to get the difference between 2 dates returned in months\n    def udf_n_months(self, date1, date2):\n        month_dif = (relativedelta(date1, date2).months + \n                relativedelta(date1, date2).years * 12)\n        return month_dif\n\n    # function to fill in any missing data for customer join date\n    # if only some records are missing for the customer and we have the join date in other records use that\n    # Otherwise, use the earliest customer summary start date\n    def fill_date_customer_joined(self, df):\n        nb_cust_date_customer_joined_filled = df[df[self.date_customer_joined].isnull()][self.granularity_key].nunique()\n\n        if nb_cust_date_customer_joined_filled > 0:\n            print('Filling date_customer_joined for ' + str(nb_cust_date_customer_joined_filled) + ' customers')\n            # get a list of the customers who are missing start dates\n            cust_date_cust_joined_missing = list(df[df[self.date_customer_joined].isnull()][self.granularity_key].unique())\n\n            # check to see if any of the start date records for the customer are filled in\n            # use this if it's available\n            df_new_start_date = df[df[self.granularity_key].isin(cust_date_cust_joined_missing)].groupby(self.granularity_key)[self.date_customer_joined].min().reset_index()\n            df_new_start_date = df_new_start_date[df_new_start_date[self.date_customer_joined].notnull()]\n            df_new_start_date.rename(columns={self.date_customer_joined: 'MIN_START_DATE'}, inplace=True)\n            if df_new_start_date.shape[0] > 0:\n                df = df.merge(df_new_start_date, on=self.granularity_key, how='left')\n                df[self.date_customer_joined].fillna(df['MIN_START_DATE'], inplace=True)\n                # since these customers are not now missing start dates, remove them from the list\n                cust_date_cust_joined_missing = list(set(cust_date_cust_joined_missing) - set(df_new_start_date[self.granularity_key].unique()))\n                # drop the min_start_date var\n                df.drop('MIN_START_DATE', axis=1, inplace=True)\n\n            if len(cust_date_cust_joined_missing) > 0:\n                # get the earliest customer summary start date for each customer who is missing a start date\n                df_new_start_date = df[df[self.granularity_key].isin(cust_date_cust_joined_missing)].groupby(self.granularity_key)[self.customer_start_date].min().reset_index()\n                df_new_start_date.rename(columns={self.customer_start_date:'NEW_START_DATE'}, inplace=True)\n                # join back to original df and update \n                df = df.merge(df_new_start_date, on=self.granularity_key, how='left')\n                df[self.date_customer_joined].fillna(df['NEW_START_DATE'], inplace=True)\n                df.drop('NEW_START_DATE', axis=1, inplace=True)\n\n        return df\n\n    # this function returns a list of dynamic attributes from lists provided\n    # User provides a list of risk and investment objective types\n    # the function gets the column names for the counts of accounts of each type\n    def dynamic_attributes_from_list(self):\n        dynamic_attributes = []\n    \n        if len(self.risk_tolerance_list) > 0:\n            for risk in self.risk_tolerance_list:\n                col_name = 'NUM_ACCOUNTS_WITH_RISK_TOLERANCE_' + risk.upper().replace(\" \", \"_\")\n                dynamic_attributes.append(col_name)\n    \n        if len(self.investment_objective_list) > 0:\n            for objective in self.investment_objective_list:\n                col_name = 'NUM_ACCOUNTS_WITH_INVESTMENT_OBJECTIVE_' + objective.upper().replace(\" \", \"_\")\n                dynamic_attributes.append(col_name)\n    \n        return dynamic_attributes\n\n    # this function filters the dataframe to only include the columns that are specified\n    def filter_attributes(self, df, columns_required, default_attributes):\n    \n        # the attributes we will use are the required ones plus ones specitied in defualt_attributes\n        working_attributes = columns_required + default_attributes\n        # check to make sure we don't have duplicate columns names\n        working_attributes = list(set(working_attributes))\n        #check to make sure that the attributes are in the original dataframe\n        if set(working_attributes) - set(df.columns) == 0:\n            print('Invalid column names, no column names in columns_required or default_attributes lists are contained in the dataframe')\n    \n        # check to see if any columns passed in the list are not actually in the dataframe, print them to screen\n        # and remove from the list of working_attributes\n        cols_passed_but_not_in_df = [attribute for attribute in working_attributes if attribute not in df.columns]\n        if len(cols_passed_but_not_in_df) > 0:\n            print(str(len(cols_passed_but_not_in_df)) + ' columns were passed but are not contained in the data. :' + str(cols_passed_but_not_in_df))\n            working_attributes = [col for col in working_attributes if col not in cols_passed_but_not_in_df]\n        \n        df = df[working_attributes]\n        return df\n\n    # This function does some data cleaning by removing columns that have constant or missing values\n    # All numeric data that has only 1 value is removed\n    # For categorical variables, we drop columns that have only 1 unique value\n    # For categoricals, we drop columns that have a cardinality greater than or equal to max_num_cat_cardinality\n    # If drop_count_column_distinct is True, we drop columns that have null values above the specified threshold, nulls_threshold\n    def drop_dataframe_columns(self, df, max_num_cat_cardinality = 10, nulls_threshold = 0.1, keep=[], drop_count_column_distinct=False):\n\n        print('Before cleaning, we had ' + str(df.shape[1]) + ' columns.')\n        # get the numeric columns\n        numeric_cols = list(df.select_dtypes(include=[np.number]).columns)\n        # remove the columns that are required from the list\n        numeric_cols = list(set(numeric_cols) - set(keep))\n\n        # drop all numeric columns that just contain a constant value, min=max\n        # record cols that we are dropping and remove after iterating over the list\n        # don't remove in list as I think it causes issues when iterating over it\n        cols_to_remove = []\n        for col in numeric_cols:\n            curr_col = df[col]\n            if curr_col.max() == curr_col.min():\n                df.drop(col, axis=1, inplace=True)\n                # remove the column from our list of numerical variables\n                cols_to_remove.append(col)\n\n        numeric_cols = list(set(numeric_cols) - set(cols_to_remove))\n\n        # get the string and datetime columns\n        string_cols = list(df.select_dtypes(include=[object]).columns)\n        # remove the columns that are required from the list\n        string_cols = list(set(string_cols) - set(keep))\n        datetime_cols = list(df.select_dtypes(include=[np.datetime64]).columns)\n        # remove the columns that are required from the list\n        datetime_cols = list(set(datetime_cols) - set(keep))\n\n        # treat string and datetime cols the same for below\n        not_num_cols = string_cols + datetime_cols\n\n        # get a count of number of null values in each column,\n        # if the number of nulls is greater than a threshold percentage, drop the column\n        if drop_count_column_distinct:\n            cols_to_remove = []\n            for col in numeric_cols:\n                curr_col = df[col]\n                if (curr_col.isna().sum()/curr_col.shape[0]) > nulls_threshold:\n                    df.drop(col, axis=1, inplace=True)\n                    # add the column name to the list of attributes to remove\n                    cols_to_remove.append(col)\n\n            numeric_cols = list(set(numeric_cols) - set(cols_to_remove))  \n\n            # do the same for non-numerical columns\n            cols_to_remove = []\n            for col in not_num_cols:\n                curr_col = df[col]\n                if (curr_col.isna().sum()/curr_col.shape[0]) > nulls_threshold:\n                    df.drop(col, axis=1, inplace=True)\n                    # add the column name to the list of tho\n                    cols_to_remove.append(col)\n\n            numeric_cols = list(set(not_num_cols) - set(cols_to_remove))  \n\n        # drop categorical variables that are constant or more than cat_cardinality_threshold (10) categories\n        for col in string_cols:\n            col_cardinality = df[col].nunique()\n            if col_cardinality == 1 or col_cardinality >= max_num_cat_cardinality:\n                df.drop(col, axis=1, inplace=True)\n\n        print('After cleaning, we have ' + str(df.shape[1]) + ' columns.')\n\n        return df\n\n    # This function takes a dataframe, a list of columns and a multiplier\n    # and replaces values that are more than multiplier * standard deviations from the mean\n    def clean_outliers(self, df, column_list, multiplier=5):\n        for col in column_list:\n            col_std = df[col].std()\n            col_mean = df[col].mean()\n            df.loc[df[col] >= col_mean + (multiplier * col_std), col] = col_mean + (multiplier * col_std)\n\n        return df\n\n    def prep_data(self, df_raw, train_or_score):\n        # just in case any caps are used\n        train_or_score = train_or_score.lower()\n\n        # find the columns that are used for risk and investment objective\n        dynamic_attributes = self.dynamic_attributes_from_list()\n        # add the dynamic attributes to the already defined default attribute list\n        self.default_attributes = self.default_attributes + dynamic_attributes\n\n        # filter the dataframe to only include attributes that have been specified\n        df_prep = self.filter_attributes(df_raw, self.columns_required, self.default_attributes)\n\n        # fill missing customer join dates with the customer summary start date\n        if self.date_customer_joined in df_prep.columns:\n            df_prep = self.fill_date_customer_joined(df_prep)\n        \n        # filter to only include customers who most recent record is active. All customers who churned are removed\n        #sort by customer ID and summary END_DATE, take the latest record\n        df_prep = df_prep.sort_values(by=[self.granularity_key, self.customer_end_date])\n        df_prep = df_prep.groupby(self.granularity_key).last().reset_index()\n\n        print('Before removing inactive customers we have ' + str(df_prep[self.granularity_key].nunique()) + ' customers')\n        df_prep = df_prep[df_prep[self.status_attribute]==self.status_flag_active]\n        print('After removing inactive customers we have ' + str(df_prep[self.granularity_key].nunique()) + ' customers')\n\n        # drop some columns that we don't need\n        df_prep.drop(['CUSTOMER_STATUS', 'CUSTOMER_SYSTEM_LOAD_TIMESTAMP'], axis=1, inplace=True)\n\n        if train_or_score == 'train':\n            # drop more columns\n            # we only do this for training, as when scoring, we already know the columns dropped from training\n            df_prep = self.drop_dataframe_columns(df_prep, self.max_num_cat_cardinality, self.nulls_threshold, keep=self.columns_required, drop_count_column_distinct=True)\n\n        # Calculate the customer tenure\n        if self.date_customer_joined in df_prep.columns:\n            df_prep = df_prep[df_prep[self.date_customer_joined]<=datetime.datetime.strptime(self.effective_date, '%Y-%m-%d')]\n            if df_prep.shape[0] == 0:\n                print('Error: No data to train with', file=sys.stderr)\n            else:\n                print('Add a column for customer tenure')\n                df_prep['CUSTOMER_TENURE_IN_MONTHS'] = df_prep.apply(lambda x: self.udf_n_months(datetime.datetime.strptime(self.effective_date, '%Y-%m-%d'), x[self.date_customer_joined]), axis=1)        \n        \n        if df_prep.shape[0] == 0:\n            return None\n          \n        # drop any column that looks like a date\n        if train_or_score == 'train':\n            for col in df_prep.columns:\n                if df_prep[col].dtype == 'datetime64[ns]':\n                    df_prep.drop(col, axis=1, inplace=True)\n\n        print('Prepped data has ' + str(df_prep.shape[0]) + ' rows and ' + str(df_prep.shape[1]) + ' columns.')\n        print('Prep has data for ' + str(df_prep[self.granularity_key].nunique()) + ' customers')\n        \n        if train_or_score == 'train':\n            # get a list of columns that we would like to remove outliers for\n            # we use only float valued columns\n            float_cols = list(df_prep.select_dtypes(include=[np.float]).columns)\n            # call the function to remove outliers\n            df_prep = self.clean_outliers(df_prep, float_cols, self.std_multiplier) \n\n        # for string columns replace nulls with 'Unknown'\n        # for numerical replace with mean. If there are no values for the column to calculate a mean (can happen in scoring),\n        # fill with 0 instead\n        string_cols = list(df_prep.select_dtypes(include=[object]).columns)\n        numeric_cols = list(df_prep.select_dtypes(include=[np.number]).columns)\n\n        for col in string_cols:\n            df_prep[col].fillna('Unknown', inplace=True)\n\n        for col in numeric_cols:\n            col_mean = df_prep[col].mean()\n            # if the whole column is null (can happen when scoring, esp if just 1 customer), fill the value with 0\n            if pd.isnull(col_mean):\n                df_prep[col].fillna(0, inplace=True)\n            else:\n                df_prep[col].fillna(col_mean, inplace=True)\n        \n        if train_or_score == 'train':\n            with open('training_data_metadata.json', 'w') as f:\n                json.dump(self.user_inputs_dict, f)\n        return df_prep\n", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Writing customer_segmentation_prep.py\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Prep Variables "}, {"metadata": {}, "cell_type": "code", "source": "# User input variables\neffective_date = '2018-09-30'  # date at which the prediction was computed \ntrain_or_score = 'train'\n\ngranularity_key='CUSTOMER_CUSTOMER_ID'\ncustomer_start_date='CUSTOMER_SUMMARY_START_DATE'\ncustomer_end_date='CUSTOMER_SUMMARY_END_DATE'\nstatus_attribute='CUSTOMER_STATUS'\nstatus_flag_active='Active'\ndate_customer_joined='CUSTOMER_RELATIONSHIP_START_DATE'\n\ncolumns_required=['CUSTOMER_CUSTOMER_ID', 'CUSTOMER_STATUS', 'CUSTOMER_SUMMARY_START_DATE', 'CUSTOMER_SUMMARY_END_DATE',\n                    'CUSTOMER_EFFECTIVE_DATE',  'CUSTOMER_SYSTEM_LOAD_TIMESTAMP']\n\ndefault_attributes=['CUSTOMER_GENDER', 'CUSTOMER_AGE_RANGE', 'CUSTOMER_EDUCATION_LEVEL',\n                            'CUSTOMER_EMPLOYMENT_STATUS', 'CUSTOMER_MARITAL_STATUS', \n                            'CUSTOMER_URBAN_CODE', 'CUSTOMER_ANNUAL_INCOME', 'CUSTOMER_RELATIONSHIP_START_DATE', \n                            'CUSTOMER_SUMMARY_RETURN_LAST_QUARTER', \n                            'CUSTOMER_SUMMARY_NUMBER_OF_EMAILS',\n                            'CUSTOMER_SUMMARY_NUMBER_OF_LOGINS',\n                    'CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES',\n                           'CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY', 'CUSTOMER_CREDIT_AUTHORITY_LEVEL', 'CUSTOMER_CUSTOMER_BEHAVIOR', 'CUSTOMER_IMPORTANCE_LEVEL_CODE',\n                           'CUSTOMER_MARKET_GROUP',\n                           'CUSTOMER_PURSUIT']\nrisk_tolerance_list = []\ninvestment_objective_list = []\n\nstd_multiplier=5\nmax_num_cat_cardinality=15\nnulls_threshold=0.1", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "customer_full_summary_latest_file = project.get_file(\"customer_full_summary_latest.csv\")\ncustomer_full_summary_latest_file.seek(0)\n\ndf_raw = pd.read_csv(customer_full_summary_latest_file,\n                     parse_dates=['CUSTOMER_RELATIONSHIP_START_DATE',\n                                 'CUSTOMER_SUMMARY_END_DATE', 'CUSTOMER_SUMMARY_START_DATE'], infer_datetime_format=True)\n\nfrom customer_segmentation_prep import CustomerSegmentationPrep\n\ndata_prep = CustomerSegmentationPrep(train_or_score=train_or_score, effective_date=effective_date, granularity_key=granularity_key, customer_start_date=customer_start_date, customer_end_date=customer_end_date,\n                                        status_attribute=status_attribute, status_flag_active=status_flag_active, date_customer_joined=date_customer_joined, columns_required=columns_required, default_attributes=default_attributes,\n                                        risk_tolerance_list=risk_tolerance_list, investment_objective_list=investment_objective_list, std_multiplier=std_multiplier, max_num_cat_cardinality=max_num_cat_cardinality, nulls_threshold=nulls_threshold)\n\ndf_prepped = data_prep.prep_data(df_raw, train_or_score)", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "Before removing inactive customers we have 1000 customers\nAfter removing inactive customers we have 838 customers\nBefore cleaning, we had 22 columns.\nAfter cleaning, we have 19 columns.\nAdd a column for customer tenure\nPrepped data has 838 rows and 17 columns.\nPrep has data for 838 customers\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Preview prepped data\ndf_prepped.head()", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "   CUSTOMER_CUSTOMER_ID CUSTOMER_MARITAL_STATUS CUSTOMER_URBAN_CODE  \\\n0                  1000                 Married                City   \n1                  1001                Divorced               Urban   \n2                  1002                 Married               Urban   \n3                  1003                 Married               Urban   \n4                  1004                 Married                City   \n\n   CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES CUSTOMER_GENDER  \\\n0                                     1757.13            Male   \n1                                    17935.79          Female   \n2                                     1221.06          Female   \n3                                     1176.59          Female   \n4                                    14452.36            Male   \n\n  CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY CUSTOMER_EMPLOYMENT_STATUS  \\\n0                             Recreation                   Employed   \n1                          Uncategorized               Selfemployed   \n2                                 Travel                  Homemaker   \n3                                 Travel                  Homemaker   \n4                                   Food                   Employed   \n\n  CUSTOMER_CUSTOMER_BEHAVIOR CUSTOMER_MARKET_GROUP  \\\n0                   Moderate          Accumulating   \n1                 Aggressive               Gifting   \n2                     Growth          Accumulating   \n3                     Growth          Accumulating   \n4                   Moderate          Accumulating   \n\n  CUSTOMER_CREDIT_AUTHORITY_LEVEL CUSTOMER_AGE_RANGE     CUSTOMER_PURSUIT  \\\n0                          Medium           30 to 40  Capital Acquisition   \n1                       Very High        65 and over  Retirement Planning   \n2                        Very Low           55 to 65   Increase Net Worth   \n3                        Very Low        65 and over   Increase Net Worth   \n4                          Medium           40 to 55      Estate Planning   \n\n  CUSTOMER_IMPORTANCE_LEVEL_CODE CUSTOMER_EFFECTIVE_DATE  \\\n0                   Low priority              2018-01-02   \n1                Normal priority              2017-11-29   \n2                  High priority              2017-08-28   \n3                  High priority              2018-01-17   \n4                   Low priority              2018-01-03   \n\n  CUSTOMER_EDUCATION_LEVEL  CUSTOMER_ANNUAL_INCOME  CUSTOMER_TENURE_IN_MONTHS  \n0                  College                325000.0                          8  \n1             Professional                280000.0                         10  \n2                      PhD                130000.0                         13  \n3                      PhD                120000.0                          8  \n4                  College                350000.0                          8  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSTOMER_CUSTOMER_ID</th>\n      <th>CUSTOMER_MARITAL_STATUS</th>\n      <th>CUSTOMER_URBAN_CODE</th>\n      <th>CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES</th>\n      <th>CUSTOMER_GENDER</th>\n      <th>CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY</th>\n      <th>CUSTOMER_EMPLOYMENT_STATUS</th>\n      <th>CUSTOMER_CUSTOMER_BEHAVIOR</th>\n      <th>CUSTOMER_MARKET_GROUP</th>\n      <th>CUSTOMER_CREDIT_AUTHORITY_LEVEL</th>\n      <th>CUSTOMER_AGE_RANGE</th>\n      <th>CUSTOMER_PURSUIT</th>\n      <th>CUSTOMER_IMPORTANCE_LEVEL_CODE</th>\n      <th>CUSTOMER_EFFECTIVE_DATE</th>\n      <th>CUSTOMER_EDUCATION_LEVEL</th>\n      <th>CUSTOMER_ANNUAL_INCOME</th>\n      <th>CUSTOMER_TENURE_IN_MONTHS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000</td>\n      <td>Married</td>\n      <td>City</td>\n      <td>1757.13</td>\n      <td>Male</td>\n      <td>Recreation</td>\n      <td>Employed</td>\n      <td>Moderate</td>\n      <td>Accumulating</td>\n      <td>Medium</td>\n      <td>30 to 40</td>\n      <td>Capital Acquisition</td>\n      <td>Low priority</td>\n      <td>2018-01-02</td>\n      <td>College</td>\n      <td>325000.0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001</td>\n      <td>Divorced</td>\n      <td>Urban</td>\n      <td>17935.79</td>\n      <td>Female</td>\n      <td>Uncategorized</td>\n      <td>Selfemployed</td>\n      <td>Aggressive</td>\n      <td>Gifting</td>\n      <td>Very High</td>\n      <td>65 and over</td>\n      <td>Retirement Planning</td>\n      <td>Normal priority</td>\n      <td>2017-11-29</td>\n      <td>Professional</td>\n      <td>280000.0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1002</td>\n      <td>Married</td>\n      <td>Urban</td>\n      <td>1221.06</td>\n      <td>Female</td>\n      <td>Travel</td>\n      <td>Homemaker</td>\n      <td>Growth</td>\n      <td>Accumulating</td>\n      <td>Very Low</td>\n      <td>55 to 65</td>\n      <td>Increase Net Worth</td>\n      <td>High priority</td>\n      <td>2017-08-28</td>\n      <td>PhD</td>\n      <td>130000.0</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1003</td>\n      <td>Married</td>\n      <td>Urban</td>\n      <td>1176.59</td>\n      <td>Female</td>\n      <td>Travel</td>\n      <td>Homemaker</td>\n      <td>Growth</td>\n      <td>Accumulating</td>\n      <td>Very Low</td>\n      <td>65 and over</td>\n      <td>Increase Net Worth</td>\n      <td>High priority</td>\n      <td>2018-01-17</td>\n      <td>PhD</td>\n      <td>120000.0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1004</td>\n      <td>Married</td>\n      <td>City</td>\n      <td>14452.36</td>\n      <td>Male</td>\n      <td>Food</td>\n      <td>Employed</td>\n      <td>Moderate</td>\n      <td>Accumulating</td>\n      <td>Medium</td>\n      <td>40 to 55</td>\n      <td>Estate Planning</td>\n      <td>Low priority</td>\n      <td>2018-01-03</td>\n      <td>College</td>\n      <td>350000.0</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now that the data is prepared we need to continue with a few more data preparation steps before we can do clustering. First is to simply remove the columns `CUSTOMER_CUSTOMER_ID` and `CUSTOMER_EFFECTIVE_DATE` since they're not needed for segmentation."}, {"metadata": {}, "cell_type": "code", "source": "# Drop columns not needed for segmentation\ndf_prepped.drop(['CUSTOMER_CUSTOMER_ID', 'CUSTOMER_EFFECTIVE_DATE'], axis=1, inplace=True)", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Display Prepared Data\n\nNow that the data is ready for analysis, we will take a quick look at the dataset to ensure that everything is as expected."}, {"metadata": {}, "cell_type": "code", "source": "# Preview prepped data with standardized numeric values\nprint('\\nTraining Data for Customer Segmentation use case:')\ndisplay(df_prepped.head())\nprint(\"{} rows, {} columns\\n\".format(*df_prepped.shape))", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "\nTraining Data for Customer Segmentation use case:\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "  CUSTOMER_MARITAL_STATUS CUSTOMER_URBAN_CODE  \\\n0                 Married                City   \n1                Divorced               Urban   \n2                 Married               Urban   \n3                 Married               Urban   \n4                 Married                City   \n\n   CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES CUSTOMER_GENDER  \\\n0                                     1757.13            Male   \n1                                    17935.79          Female   \n2                                     1221.06          Female   \n3                                     1176.59          Female   \n4                                    14452.36            Male   \n\n  CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY CUSTOMER_EMPLOYMENT_STATUS  \\\n0                             Recreation                   Employed   \n1                          Uncategorized               Selfemployed   \n2                                 Travel                  Homemaker   \n3                                 Travel                  Homemaker   \n4                                   Food                   Employed   \n\n  CUSTOMER_CUSTOMER_BEHAVIOR CUSTOMER_MARKET_GROUP  \\\n0                   Moderate          Accumulating   \n1                 Aggressive               Gifting   \n2                     Growth          Accumulating   \n3                     Growth          Accumulating   \n4                   Moderate          Accumulating   \n\n  CUSTOMER_CREDIT_AUTHORITY_LEVEL CUSTOMER_AGE_RANGE     CUSTOMER_PURSUIT  \\\n0                          Medium           30 to 40  Capital Acquisition   \n1                       Very High        65 and over  Retirement Planning   \n2                        Very Low           55 to 65   Increase Net Worth   \n3                        Very Low        65 and over   Increase Net Worth   \n4                          Medium           40 to 55      Estate Planning   \n\n  CUSTOMER_IMPORTANCE_LEVEL_CODE CUSTOMER_EDUCATION_LEVEL  \\\n0                   Low priority                  College   \n1                Normal priority             Professional   \n2                  High priority                      PhD   \n3                  High priority                      PhD   \n4                   Low priority                  College   \n\n   CUSTOMER_ANNUAL_INCOME  CUSTOMER_TENURE_IN_MONTHS  \n0                325000.0                          8  \n1                280000.0                         10  \n2                130000.0                         13  \n3                120000.0                          8  \n4                350000.0                          8  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSTOMER_MARITAL_STATUS</th>\n      <th>CUSTOMER_URBAN_CODE</th>\n      <th>CUSTOMER_SUMMARY_AMOUNT_OF_MANAGEMENT_FEES</th>\n      <th>CUSTOMER_GENDER</th>\n      <th>CUSTOMER_SUMMARY_TOP_SPENDING_CATEGORY</th>\n      <th>CUSTOMER_EMPLOYMENT_STATUS</th>\n      <th>CUSTOMER_CUSTOMER_BEHAVIOR</th>\n      <th>CUSTOMER_MARKET_GROUP</th>\n      <th>CUSTOMER_CREDIT_AUTHORITY_LEVEL</th>\n      <th>CUSTOMER_AGE_RANGE</th>\n      <th>CUSTOMER_PURSUIT</th>\n      <th>CUSTOMER_IMPORTANCE_LEVEL_CODE</th>\n      <th>CUSTOMER_EDUCATION_LEVEL</th>\n      <th>CUSTOMER_ANNUAL_INCOME</th>\n      <th>CUSTOMER_TENURE_IN_MONTHS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Married</td>\n      <td>City</td>\n      <td>1757.13</td>\n      <td>Male</td>\n      <td>Recreation</td>\n      <td>Employed</td>\n      <td>Moderate</td>\n      <td>Accumulating</td>\n      <td>Medium</td>\n      <td>30 to 40</td>\n      <td>Capital Acquisition</td>\n      <td>Low priority</td>\n      <td>College</td>\n      <td>325000.0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Divorced</td>\n      <td>Urban</td>\n      <td>17935.79</td>\n      <td>Female</td>\n      <td>Uncategorized</td>\n      <td>Selfemployed</td>\n      <td>Aggressive</td>\n      <td>Gifting</td>\n      <td>Very High</td>\n      <td>65 and over</td>\n      <td>Retirement Planning</td>\n      <td>Normal priority</td>\n      <td>Professional</td>\n      <td>280000.0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Married</td>\n      <td>Urban</td>\n      <td>1221.06</td>\n      <td>Female</td>\n      <td>Travel</td>\n      <td>Homemaker</td>\n      <td>Growth</td>\n      <td>Accumulating</td>\n      <td>Very Low</td>\n      <td>55 to 65</td>\n      <td>Increase Net Worth</td>\n      <td>High priority</td>\n      <td>PhD</td>\n      <td>130000.0</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Married</td>\n      <td>Urban</td>\n      <td>1176.59</td>\n      <td>Female</td>\n      <td>Travel</td>\n      <td>Homemaker</td>\n      <td>Growth</td>\n      <td>Accumulating</td>\n      <td>Very Low</td>\n      <td>65 and over</td>\n      <td>Increase Net Worth</td>\n      <td>High priority</td>\n      <td>PhD</td>\n      <td>120000.0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Married</td>\n      <td>City</td>\n      <td>14452.36</td>\n      <td>Male</td>\n      <td>Food</td>\n      <td>Employed</td>\n      <td>Moderate</td>\n      <td>Accumulating</td>\n      <td>Medium</td>\n      <td>40 to 55</td>\n      <td>Estate Planning</td>\n      <td>Low priority</td>\n      <td>College</td>\n      <td>350000.0</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}, {"output_type": "stream", "text": "838 rows, 15 columns\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Save Prepared data\n\nWe can save the prepared data in order to transfer the data to the model training notebook. This data will be called `training_data.csv`"}, {"metadata": {}, "cell_type": "code", "source": "project.save_data('training_data.csv', df_prepped.to_csv(index=False), overwrite=True)", "execution_count": 15, "outputs": [{"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "{'file_name': 'training_data.csv',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'financialmarketscustomersegmentat-donotdelete-pr-pufwjklnffdfmt',\n 'asset_id': '07eb2d32-ed72-4232-9ea1-b19dd1fa69fc'}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we have finished preparing the dataset and saved out the prepped data for modelling. See notebook **`2-model-training`** for the next step."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\nThis project contains Sample Materials, provided under this <a href=\"https://github.com/IBM/Industry-Accelerators/blob/master/CPD%20SaaS/LICENSE\" target=\"_blank\" rel=\"noopener noreferrer\">license</a>. <br/>\nLicensed Materials - Property of IBM. <br/>\n\u00a9 Copyright IBM Corp. 2019, 2020, 2021. All Rights Reserved. <br/>\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.<br/>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}