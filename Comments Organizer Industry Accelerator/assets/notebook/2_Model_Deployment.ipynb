{"cells": [{"metadata": {"id": "967040dd-558e-4ee8-946a-12e4c0e684f0"}, "cell_type": "markdown", "source": "**Sample Materials, provided under license. <br>\nLicensed Materials - Property of IBM. <br>\n\u00a9 Copyright IBM Corp. 2019, 2020. All Rights Reserved. <br>\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>**"}, {"metadata": {"id": "0a934a5f-8b30-41b9-a436-a65115511010"}, "cell_type": "markdown", "source": "# Import required packages"}, {"metadata": {"id": "0bc83056-1651-46f1-a4c5-608ad7060c4f"}, "cell_type": "code", "source": "import json\nimport os\nimport datetime\nfrom ibm_watson_machine_learning import APIClient", "execution_count": 1, "outputs": []}, {"metadata": {"id": "6ca209c5-6a5c-4d29-87f9-1699dcc3d97f"}, "cell_type": "markdown", "source": "# Table of Contents\n\n* [0. About](#about)\n* [1. Deploying Functions](#1)\n    * [1.1 Clustering](#1-1)\n        * [1.1.1 Create Function to Deploy](#1-1-1)\n        * [1.1.2 Deploy Function](#1-1-2)\n        * [1.1.3 Test Deployed Function](#1-1-3)\n    * [1.2 Sentiment Analysis](#1-2)\n        * [1.2.1 Create Function to Deploy](#1-2-1)\n        * [1.2.2 Deploy Function](#1-2-2)\n        * [1.2.3 Test Deployed Function](#1-2-3)\n* [2. View Deployed Functions](#2)"}, {"metadata": {"id": "6e7339ab-05ad-4d3f-a3ac-1b01fdb93173"}, "cell_type": "markdown", "source": "# 0. About <a class=\"anchor\" id=\"about\"></a>\n\nIn `1_Data_Exploration_and_Model_Training.ipynb`, we saw how all the functions work. This notebook will focus on using the Watson Machine Learning client to deploy functions. \n\nWe will walk through the steps on setting up Watson Machine Learning and deploying functions in the next sections. However, if you need more help:\n* For more information about Watson Machine Learning Client, see the documentation [here](http://ibm-wml-api-pyclient.mybluemix.net/)\n* For more information on deploying functions see the documentation [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-deploy-functions_local.html)."}, {"metadata": {"id": "bc65313f-92fe-489c-af64-dac37fd7170c"}, "cell_type": "markdown", "source": "# 1. Deploying Functions <a class=\"anchor\" id=\"1\"></a>"}, {"metadata": {"id": "bbe67f10-4b3e-4f9d-a75e-80e25872c059"}, "cell_type": "markdown", "source": "We will have two main functions, \n1. Clustering comments and \n2. Sentiment Analysis on comments \n\nDeployable functions should follow the structure:\n```\ndef deployable_function():\n    def score(paylod):\n        return payload['values']\n    return score\n```\n\nThe `payload` is a dictionary that must include `values` as a key. Additionally, each deployable function must return a json serializable object. So the format of the `payload` and the json response for each function is:\n\n1. Clustering Function\n    * Payload (input):\n    ```\n    {\n        \"values\": list of comments (required parameter),\n        \"number_of_clusters\": number of clusters to create \n            if this is empty then the model automatically \n            chooses the number but caps it at 5 (optional parameter)\n        \"number_of_terms\": number of top terms to display \n            for each cluster, if the number of comments is \n            less than this value then the number of comments \n            will be used (optional parameter)\n    }\n    ```\n    * Response:\n    ```\n    [\n        {\n            \"comment\": {\n                \"idx\": int,\n                \"group\": int,\n                \"topics\": [str]\n            },\n        },\n    ]\n    ```\nWhere \"idx\" is the index of the original comment. \"group\" is the group number assigned. \"topics\" are the most important terms in that group.\n\n2. Sentiment Analysis Function\n    * Payload (input):\n    ```\n    {\n        \"values\": list of comments (required parameter)\n    }\n    ```\n    * Response:\n    ```\n    [\n        {\n            \"comment\": {\n                \"idx\": int,\n                \"sentiment\": str,\n                \"sentence_sentiment\": [\n                    {\n                        \"idx\": int,\n                        \"sentence\": str,\n                        \"sentiment\": str\n                    },\n                ]\n            }\n        },\n    ]\n    ```\nWhere \"idx\" is the index of the original comment. \"sentiment\" is the sentiment of the entire comment. \"sentence_sentiment\" is for every sentence of the comment, its \"idx\" is the index of the sentence, \"sentence\" is the entire sentence string, its \"sentiment\" is the sentence's sentiment.\n"}, {"metadata": {"id": "e16c7a97-4d8d-4639-9254-bc35412a5c93"}, "cell_type": "markdown", "source": "### Setting Up Watson Machine Learning\n\nTo setup Watson Machine Learning, you can follow the steps below or see [here](http://ibm-wml-api-pyclient.mybluemix.net/#api-for-ibm-cloud-pak-for-data-ibm-watson-machine-learning-server) for more help."}, {"metadata": {"id": "d8ddaa7d-b5b0-4786-98ea-f4247b7d569a"}, "cell_type": "code", "source": "token = os.environ['USER_ACCESS_TOKEN']\n\nwml_credentials = {\n   \"token\": token,\n   \"instance_id\" : \"openshift\",\n   \"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n   \"version\": \"3.5\"\n}\n\n\nclient = APIClient(wml_credentials)", "execution_count": 2, "outputs": []}, {"metadata": {"id": "b377e51a51334d9f95e3f4929a9fd312"}, "cell_type": "markdown", "source": "#### Create the Deployment Space\nCreate a new deployment space using name of the space as specified in the user inputs cell above. The space name will be used in future to identify this space.\nIf a space with specified space_name already exists, user can either use the existing space by specifying `use_existing_space=True` or delete the existing space and create a new one by specifying `use_existing_space=False` below. By default `use_existing_space` is set to True.\n"}, {"metadata": {"id": "7ee8f3bc-5207-4a30-a17d-109182e1030b"}, "cell_type": "code", "source": "space_name = 'Comments Organizer Space'\n\nuse_existing_space=True", "execution_count": 3, "outputs": []}, {"metadata": {"id": "887a084f-4723-4ad2-b47e-59a5b5dceaa4"}, "cell_type": "code", "source": "space_uid=\"\"\nfor space in client.spaces.get_details()['resources']:\n\n    if space['entity']['name'] ==space_name:\n        print(\"Deployment space with \",space_name,\"already exists . .\")\n        space_uid=space['metadata']['id']\n        client.set.default_space(space_uid)\n        if(use_existing_space==False):\n\n            for deployment in client.deployments.get_details()['resources']:\n                print(\"Deleting deployment\",deployment['entity']['name'], \"in the space\",)\n                deployment_id=deployment['metadata']['id']\n                client.deployments.delete(deployment_id)\n            print(\"Deleting Space \",space_name,)\n            client.spaces.delete(space_uid)\n            time.sleep(5)\n        else:\n            print(\"Using the existing space\")\n            \n            \nif (space_uid==\"\" or use_existing_space==False):\n    print(\"\\nCreating a new deployment space -\",space_name)\n    # create the space and set it as default\n    space_meta_data = {\n        client.spaces.ConfigurationMetaNames.NAME : space_name\n\n        }\n\n    stored_space_details = client.spaces.store(space_meta_data)\n\n    space_uid = stored_space_details['metadata']['id']\n\n    client.set.default_space(space_uid)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "\nCreating a new deployment space - Comments Organizer Space\nSpace has been created. However some background setup activities might still be on-going. Check for 'status' field in the response. It has to show 'active' before space can be used. If its not 'active', you can monitor the state with a call to spaces.get_details(space_id)\n", "name": "stdout"}]}, {"metadata": {"id": "549f8f21-ff67-4c9a-8cfe-45fdccc2f0db"}, "cell_type": "markdown", "source": "## 1.1 Clustering <a class=\"anchor\" id=\"1-1\"></a>\nThis section will be focusing on wrapping our clustering methods and data preprocessing into a function that will be deployed. "}, {"metadata": {"id": "5a9a99f2-45f4-418e-85ee-2dd4a6c92e7e"}, "cell_type": "markdown", "source": "### 1.1.1 Create Function to Deploy<a class=\"anchor\" id=\"1-1-1\"></a>\n\nReminder that the deployed function's input and response is:\n* Payload (input):\n    \n    ```\n    {\n    values \n        {\n        \"test\": list of comments (required parameter),\n        \"number_of_clusters\": number of clusters to create \n            if this is empty then the model automatically \n            chooses the number but caps it at 5 (optional parameter)\n        \"number_of_terms\": number of top terms to display \n            for each cluster, if the number of comments is \n            less than this value then the number of comments \n            will be used (optional parameter)\n        }\n    }\n    ```\n* Response:\n    ```\n    [\n        {\n            \"comment\": {\n                \"idx\": int,\n                \"group\": int,\n                \"topics\": [str]\n            },\n        },\n    ]\n    ```"}, {"metadata": {"id": "35b68d2e-e543-4485-953a-110a8199739c"}, "cell_type": "code", "source": "def run_clustering():\n    import json\n    from collections import defaultdict\n\n    import numpy as np\n    import pandas as pd\n    from sklearn.cluster import KMeans\n    from sklearn.metrics import silhouette_score\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score\n\n    \n    def get_top_n_terms_per_cluster(km_model, terms, n=5):\n        \"\"\"\n        Gets the top terms used to cluster text\n\n        :param km_model: KMeans model\n        :param terms: list of terms from a TfidfVecotrizer object\n        :return: dictionary mapping cluster number to top n terms\n                 {cluster_number: [term1, term2,..., termn]}\n        \"\"\"\n        cluster_terms = defaultdict(list)\n\n        tfidf_values = km_model.cluster_centers_\n        order_centroids = km_model.cluster_centers_.argsort()[:, ::-1]\n        for i in range(len(order_centroids)):\n            cluster = order_centroids[i]\n            for term_idx in cluster[:n]:\n                # Term should exist in the cluster in order to include\n                if tfidf_values[i, term_idx] != 0:\n                    cluster_terms[i].append(terms[term_idx])\n\n        return cluster_terms\n    \n    def run_kmeans(number_of_clusters, tfidf_matrix):\n        \"\"\"\n        :param number_of_clusters: int\n        :param tfidf_matrix: matrix from TfidfVectorizer object\n        :return: KMeans model, list of cluster labels\n        \"\"\"\n        km_model = KMeans(n_clusters=number_of_clusters, init='k-means++')\n        km_model.fit(tfidf_matrix.toarray())\n        clusters = km_model.labels_.tolist()\n        return km_model, clusters\n    \n    def run_model(X, number_of_clusters=None, number_of_terms=5, max_number_of_groups=5):\n        \"\"\"\n        Runs the entire modeling process\n        1. create TFIDF matrix\n        2. run KMeans with TFIDF matrix\n        3. get top terms used\n\n        :return: (list of cluster assignments, \n                  dictionary mapping cluster number of terms)\n        \"\"\"\n        # First find TFIDF matrix\n        tfidf_vectorizer = TfidfVectorizer(max_df=0.75 if len(X)>1 else 1, \n                                           min_df=0.1 if len(X)>1 else 1,\n                                           stop_words='english',\n                                           use_idf=True, \n                                           ngram_range=(1,3),\n                                          )\n\n        tfidf_matrix = tfidf_vectorizer.fit_transform(X)\n        terms = tfidf_vectorizer.get_feature_names()\n\n        # Number of clusters must be > 2 for silhouette_score to work.\n        # If there are 2 or less comments, then just set to number of comments.\n        number_of_clusters = len(X) if len(X) <= 2 else number_of_clusters\n\n        if number_of_clusters:\n            # If there's a specific number of clusters specified, then run with that number.\n            km_model, best_clusters = run_kmeans(number_of_clusters, tfidf_matrix)\n            cluster_terms = get_top_n_terms_per_cluster(km_model, terms, number_of_terms)\n        else:\n            # Automatically find number of clusters with silhouette_score\n            # but have a maximum of max_number_of_groups \n            max_silhouette_score = 0\n            for k in range(2, min(max_number_of_groups, len(X))):\n                km_model, clusters = run_kmeans(k, tfidf_matrix)\n                current_silhouette_score = silhouette_score(tfidf_matrix, clusters)\n                if current_silhouette_score > max_silhouette_score:\n                    max_silhouette_score = current_silhouette_score\n                    cluster_terms = get_top_n_terms_per_cluster(km_model, terms, number_of_terms)\n                    best_clusters = clusters\n\n        return best_clusters, cluster_terms\n    \n    def format_clusters(final_labels, group_topics):\n        \"\"\"\n        return:{\n                \"comment\": {\n                    \"idx\": 1,\n                    \"group\": 2\n                    \"topics\": [\"\"]\n                }\n               }\n        \"\"\"\n        \n        result = []\n        for i in range(len(final_labels)):\n            group = int(final_labels[i])\n            comment = {\n                \"comment\": {\n                    \"idx\": i,\n                    \"group\": group,\n                    \"topics\": list(group_topics[group])\n                }\n            }\n            result.append(comment)\n        return json.dumps(result)\n\n    def score(payload):\n        '''\n        :param payload: dictionary, expects the key 'values' (list of comments).\n            optional keys are 'number_of_clusters' and 'number_of_numbers', both int\n        :return: JSON serializable object with format \n                [\n                    {\n                        \"comment\": {\n                            \"idx\": int,\n                            \"group\": int,\n                            \"topics\": [str]\n                        },\n                    },\n                ]\n        '''\n\n        comments = payload['input_data'][0]['values'].get('test') \n        n = payload['input_data'][0]['values'].get('number_of_clusters')\n        n_terms = payload['input_data'][0]['values'].get('number_of_terms', 5)  # default is 5\n        final_labels, top_terms = run_model(comments, number_of_clusters=n, number_of_terms=n_terms)\n        \n        score_response = {'predictions': [{ 'values': format_clusters(final_labels, top_terms)}]}        \n\n        return score_response\n    \n    return score", "execution_count": 5, "outputs": []}, {"metadata": {"id": "0342f887-d630-402f-9c8e-47b06500e997"}, "cell_type": "markdown", "source": "### 1.1.2 Deploy Function <a class=\"anchor\" id=\"1-1-2\"></a>"}, {"metadata": {"id": "d13b54a2-ec8b-4492-990e-8f63eddb591b"}, "cell_type": "markdown", "source": "Now that we have a deployable function, we store the function details."}, {"metadata": {"id": "e61430ec55ef46c99d522978a2457bcb"}, "cell_type": "code", "source": "software_spec_id = client.software_specifications.get_id_by_name(\"default_py3.7\")", "execution_count": 6, "outputs": []}, {"metadata": {"id": "a9a5ecb2-d4c4-4d66-bb2a-abd0e5d252aa"}, "cell_type": "code", "source": "# Store function details\n\nmeta_data = {\n    client.repository.FunctionMetaNames.NAME : 'Function for Clustering Text',\n    client.repository.FunctionMetaNames.TAGS : ['clustering_function_tag'],\n    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: software_spec_id\n\n}\n\nfunction_details = client.repository.store_function( meta_props=meta_data, function=run_clustering )", "execution_count": 7, "outputs": []}, {"metadata": {"id": "b82ed48c06964dd298e13387c6ceab2c"}, "cell_type": "code", "source": "# Get function id\nfunction_id = function_details[\"metadata\"][\"id\"]", "execution_count": 8, "outputs": []}, {"metadata": {"id": "26b02004c22440c592494813c993a3b0"}, "cell_type": "code", "source": "meta_props = {\n    client.deployments.ConfigurationMetaNames.NAME: 'Clustering Deployment',\n   client.deployments.ConfigurationMetaNames.TAGS : ['clustering_function_deployment_tag'],\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\n# deploy the function\nfunction_deployment_details = client.deployments.create(artifact_uid=function_id, meta_props=meta_props)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: 'c10d45d4-835c-46b9-802c-a0b46c34ec40' started\n\n#######################################################################################\n\n\ninitializing..\nready\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='e5436813-0de6-4d3e-99c3-d3b72d617e2e'\n------------------------------------------------------------------------------------------------\n\n\n", "name": "stdout"}]}, {"metadata": {"id": "55b39c92ada542a6846efaf835902920"}, "cell_type": "code", "source": "# Get deployment id for clustering\nclustering_deployment_id = client.deployments.get_uid(function_deployment_details)", "execution_count": 10, "outputs": []}, {"metadata": {"id": "04ac666b-36f7-48e2-a300-bcadfed548e5"}, "cell_type": "markdown", "source": "### 1.1.3 Test Deployed Function <a class=\"anchor\" id=\"1-1-3\"></a>"}, {"metadata": {"id": "84cdf849-6a15-4b20-93e9-17d8c67ccb12"}, "cell_type": "code", "source": "# Example 1\n# Input for payload to be passed into function\ninput_sentences = [\n    'Customer service was polite.',\n    'The socks are a pretty color.',\n    'The shirt I bought was green.',\n    'I think the sweater and socks were perfect.',\n    'I do not like the shoes, so ugly.',\n]\n#payload = {\"values\" : input_sentences}\n\npayload = {client.deployments.ScoringMetaNames.INPUT_DATA: [{'values': \n                                                             {\"test\": input_sentences,\n                                                              \"number_of_clusters\": 3,\n                                                              \"number_of_terms\": 2}} \n                                                           ]}\n\n\n# Send data to deployment for processing\nclient.deployments.score(clustering_deployment_id, payload)", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "{'predictions': [{'values': '[{\"comment\": {\"idx\": 0, \"group\": 0, \"topics\": [\"ugly\", \"shoes ugly\"]}}, {\"comment\": {\"idx\": 1, \"group\": 1, \"topics\": [\"socks\", \"socks pretty color\"]}}, {\"comment\": {\"idx\": 2, \"group\": 2, \"topics\": [\"bought\", \"bought green\"]}}, {\"comment\": {\"idx\": 3, \"group\": 1, \"topics\": [\"socks\", \"socks pretty color\"]}}, {\"comment\": {\"idx\": 4, \"group\": 0, \"topics\": [\"ugly\", \"shoes ugly\"]}}]'}]}"}, "metadata": {}}]}, {"metadata": {"id": "818202bd-b01d-4a80-b614-2c84f4066cb7"}, "cell_type": "code", "source": "# Example 2: Convert function string output to json format\ncomments = [\n    'I bought several items: socks, shirt, sweater. By far my most favorite was the shirt because it is so soft. However, the sweater and socks missed the mark.',\n    'My order arrived several days late. But when I contacted customer serivce they were very helpful and refunded me.',\n    'Horrible, horrible customer service, I have never met such rude people. Why is it so bad? Would not recommend at all.',\n    'Everything I ordered arrived on time and looked exactly like in the pictures! This company has high quality products.',\n    'I bought somethings on sale, they were a great deal. Will be buying more next time.'\n]\n\n\npayload = {client.deployments.ScoringMetaNames.INPUT_DATA: [{'values': \n                                                             {\"test\": comments,\n                                                              \"number_of_clusters\": 3,\n                                                              \"number_of_terms\": 2}} \n                                                           ]}\n\nlabels = client.deployments.score(clustering_deployment_id, payload)\n", "execution_count": 12, "outputs": []}, {"metadata": {"id": "0e2c8814-86b9-4738-b9df-c2cad9a6cadb"}, "cell_type": "code", "source": "labels", "execution_count": 13, "outputs": [{"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "{'predictions': [{'values': '[{\"comment\": {\"idx\": 0, \"group\": 2, \"topics\": [\"shirt\", \"sweater\"]}}, {\"comment\": {\"idx\": 1, \"group\": 0, \"topics\": [\"horrible\", \"customer\"]}}, {\"comment\": {\"idx\": 2, \"group\": 0, \"topics\": [\"horrible\", \"customer\"]}}, {\"comment\": {\"idx\": 3, \"group\": 1, \"topics\": [\"time\", \"somethings\"]}}, {\"comment\": {\"idx\": 4, \"group\": 1, \"topics\": [\"time\", \"somethings\"]}}]'}]}"}, "metadata": {}}]}, {"metadata": {"id": "ccd51b76-7cea-4a5c-a8a0-511c8aee322b"}, "cell_type": "markdown", "source": "## 1.2 Sentiment Anlaysis <a class=\"anchor\" id=\"1-2\"></a>\n\nThis section will be focusing on wrapping our sentiment analysis methods and data preprocessing into a function that will be deployed. For more help on deploying functions, see [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-deploy-functions.html)."}, {"metadata": {"id": "a831be2e-6b3a-4b42-971c-62972a41c8c1"}, "cell_type": "markdown", "source": "### 1.2.1 Create Function to Deploy <a class=\"anchor\" id=\"1-2-1\"></a>\n\nReminder that the deployed function's input and response will be:\n\n* Payload (input):\n    ```\n    {\n    values\n        {\n        \"test\": list of comments (required parameter)\n        }\n    }\n    ```\n* Response:\n    ```\n    [\n        {\n            \"comment\": {\n                \"idx\": int,\n                \"sentiment\": str,\n                \"sentence_sentiment\": [\n                    {\n                        \"idx\": int,\n                        \"sentence\": str,\n                        \"sentiment\": str\n                    },\n                ]\n            }\n        },\n    ]\n    ```"}, {"metadata": {"id": "10efe505-38b6-42f7-97cb-847a0f8ec11d"}, "cell_type": "code", "source": "def run_sentiment_analysis():\n    import subprocess\n    subprocess.check_output( \"pip install nltk --user\", stderr=subprocess.STDOUT, shell=True )\n    subprocess.check_output( \"pip install xlrd --user\", stderr=subprocess.STDOUT, shell=True )\n    \n    import json\n    import itertools\n    import os\n    import requests\n    import tarfile\n\n    import string\n    import nltk\n    import numpy as np\n    import pandas as pd\n    from nltk import word_tokenize\n    \n    nltk.download('punkt')\n    nltk.download('averaged_perceptron_tagger')\n    \n    class Adjective:\n        FAST = 'FAST_TOKENS'\n        SLOW = 'SLOW_TOKENS'\n        HIGH = 'HIGH_TOKENS'\n        LOW = 'LOW_TOKENS'\n\n    class Sign:\n        POSITIVE = 'Positive'\n        NEGATIVE = 'Negative'\n        NEUTRAL = 'Neutral'\n        \n    def download_data(url_base, version, data_file_name):\n        # Downloading the dataset\n        url = \"{}/{}/{}\".format(url_base, version, data_file_name)\n        response = requests.get(url)\n\n        # Check for errors\n        if not response.ok:\n            print(\"There are some errors when downloading {}\".format(url))\n\n        # Open tar file\n        with open(data_file_name, 'wb') as file_name:\n            file_name.write(response.content)\n    \n    def extract_data(data_directory, data_file_name):\n        # Extracting the dataset\n        with tarfile.open(data_file_name) as file_name:\n            file_name.extractall(path='./' + data_directory)\n            \n    def clean_adjective_df(df):\n        adjectives = []\n        for i in range(4):\n            adjectives.extend(df.iloc[:, i+1].dropna().tolist())\n\n        adj_category = df.iloc[0,0]\n\n        return [(adj, adj_category) for adj in adjectives]\n            \n    \n    # Download the dataset\n    sentiment_data_directory = 'data/sentiment-composition-lexicons'\n    url_base = 'https://dax-cdn.cdn.appdomain.cloud/dax-sentiment-composition-lexicons'\n    version = '1.0.2'\n    data_file_name = 'sentiment-composition-lexicons.tar.gz'\n    download_data(url_base, version, data_file_name)\n\n    # Extract the dataset\n    extract_data(sentiment_data_directory, data_file_name)\n    \n    # 1. Read unigram data\n    unigram_df = pd.read_csv(os.path.join(sentiment_data_directory, 'LEXICON_UG.txt'), sep=\" \")\n    # Add sentiment column\n    unigram_df['sentiment'] = np.where(unigram_df['SENTIMENT_SCORE'] > 0, 1, 0)  # 1 is positive, 0 is negative\n    unigram_sentiment_dict = pd.Series(unigram_df.SENTIMENT_SCORE.values,\n                               index=unigram_df.UNIGRAM.values).to_dict()\n\n    # 2. Read bigram data\n    bigrams_df = pd.read_csv(os.path.join(sentiment_data_directory, 'LEXICON_BG.txt'), sep=\" \")\n    # Add sentiment column\n    bigrams_df['sentiment'] = np.where(bigrams_df['SENTIMENT_SCORE'] > 0, 1, 0)  # 1 is positive, 0 is negative\n    bigram_sentiment_dict = pd.Series(bigrams_df.SENTIMENT_SCORE.values,\n                               index=bigrams_df.BIGRAM.str.split('-').apply(lambda l: tuple(l))).to_dict()\n\n    # 3.1 Adjective Classes\n    xls_file = pd.ExcelFile(os.path.join(sentiment_data_directory, 'ADJECTIVES.xlsx'))\n    adjective_expansion = pd.read_excel(xls_file, 'ADJECTIVE_EXPANSION').dropna(how='all').reset_index(drop=True)\n    high_low_PN = pd.read_excel(xls_file, '(HIGH,LOW)_POS_NEG', header=None)[0].values.tolist()\n    high_low_NP = pd.read_excel(xls_file, '(HIGH,LOW)_NEG_POS', header=None)[0].values.tolist()\n    fast_slow_PN = pd.read_excel(xls_file, '(FAST,SLOW)_POS_NEG', header=None)[0].values.tolist()\n    fast_slow_NP = pd.read_excel(xls_file, '(FAST,SLOW)_NEG_POS', header=None)[0].values.tolist()\n    \n    # Get tokens\n    tokens = []\n    token_rows = 5\n    for i in range(0, len(adjective_expansion), token_rows+1):\n        tokens.append(clean_adjective_df(adjective_expansion.loc[i:i+token_rows]))\n    high_tokens, low_tokens, fast_tokens, slow_tokens = tokens\n\n    adjective_class_map = dict(high_tokens + low_tokens + fast_tokens + slow_tokens)\n\n    # 3.2 Composition Classes\n    semantic_classes_file = pd.ExcelFile(os.path.join(sentiment_data_directory, 'SEMANTIC_CLASSES.xlsx'))\n    dominator_neg = pd.read_excel(semantic_classes_file, 'DOMINATOR_NEG', header=None)[0].values.tolist()\n    dominator_pos = pd.read_excel(semantic_classes_file, 'DOMINATOR_POS', header=None)[0].values.tolist()\n    propagator_pos = pd.read_excel(semantic_classes_file, 'PROPAGATOR_POS', header=None)[0].values.tolist()\n    propagator_neg = pd.read_excel(semantic_classes_file, 'PROPAGATOR_NEG', header=None)[0].values.tolist()\n    reverser_pos = pd.read_excel(semantic_classes_file, 'REVERSER_POS', header=None)[0].values.tolist()\n    reverser_neg = pd.read_excel(semantic_classes_file, 'REVERSER_NEG', header=None)[0].values.tolist()\n\n    adjective_conditions = {\n        Adjective.FAST: [(fast_slow_PN, Sign.POSITIVE), (fast_slow_NP, Sign.NEGATIVE)],\n        Adjective.SLOW: [(fast_slow_PN, Sign.NEGATIVE), (fast_slow_NP, Sign.POSITIVE)],\n        Adjective.HIGH: [(high_low_PN, Sign.POSITIVE), (high_low_NP, Sign.NEGATIVE)],\n        Adjective.LOW: [(high_low_PN, Sign.NEGATIVE), (high_low_NP, Sign.POSITIVE)],\n    }\n\n    sentiment_to_score = {\n        Sign.POSITIVE: +1,\n        Sign.NEGATIVE: -1,\n    }\n\n\n    # Simple implementation: if finds token in unigram_sentiment_dict, then adds sentiment\n    # The total sentiment for sentence is averaged\n    def calculate_unigram_sentiment(tokenized_sentence, sentiment_map=unigram_sentiment_dict):\n        sentiment_score = 0\n        for token in tokenized_sentence:\n            token_sentiment = sentiment_map.get(token)\n            if token_sentiment is None:\n                continue\n            else:\n                sentiment_score += token_sentiment\n\n        return sentiment_score\n    \n    def calculate_bigram_sentiment_sentence(sentence_bigrams):\n        bigrams = []\n        bigram_sentiment_score = 0\n        for bigram in sentence_bigrams:\n            bigram_sentiment = calculate_bigram_sentiment(bigram)\n            if bigram_sentiment is not None:\n                bigram_sentiment_score += bigram_sentiment\n                bigrams.append(bigram)\n\n        return bigram_sentiment_score, bigrams\n\n\n    def calculate_bigram_sentiment(bigram):\n        bigram_sentiment = bigram_sentiment_dict.get(bigram)\n        if bigram_sentiment:\n            # -0.02 and 0.02 allows for margin of error for neutrals\n            if bigram_sentiment < -0.02:\n                return -1\n            elif bigram_sentiment > 0.02:\n                return 1\n            else:\n                return 0\n        return None\n\n    def is_given_sentiment(sentiment, word, sentiment_map):\n        if word in sentiment_map:\n            if sentiment==Sign.NEGATIVE and sentiment_map[word] < 0:\n                return True\n            elif sentiment==Sign.POSITIVE and sentiment_map[word] > 0:\n                return True\n\n    def calculate_composition_or_adj_sentiment(bigram):\n        # Adjective\n        adjective_token = adjective_class_map.get(bigram[0])\n        if adjective_token is not None:\n            for expansions_list, sentiment_sign in adjective_conditions[adjective_token]:\n                if bigram[1] in expansions_list:\n                    return sentiment_to_score[sentiment_sign]\n\n        # Composition: Reverser\n        elif bigram[0] in reverser_pos and is_given_sentiment(Sign.NEGATIVE, bigram[1], unigram_sentiment_dict):\n            return sentiment_to_score[Sign.POSITIVE]\n        elif bigram[0] in reverser_neg and is_given_sentiment(Sign.POSITIVE, bigram[1], unigram_sentiment_dict):\n            return sentiment_to_score[Sign.NEGATIVE]\n\n        # Composition: Propagator\n        elif bigram[0] in propagator_pos and is_given_sentiment(Sign.NEGATIVE, bigram[0], unigram_sentiment_dict) and is_given_sentiment(Sign.POSITIVE, bigram[1], unigram_sentiment_dict):\n            return sentiment_to_score[Sign.POSITIVE]\n        elif bigram[0] in propagator_neg and is_given_sentiment(Sign.POSITIVE, bigram[0], unigram_sentiment_dict) and is_given_sentiment(Sign.NEGATIVE, bigram[1], unigram_sentiment_dict):\n            return sentiment_to_score[Sign.NEGATIVE]\n\n        # Composition: Dominator\n        elif bigram[0] in dominator_neg:\n            return sentiment_to_score[Sign.NEGATIVE]\n        elif bigram[0] in dominator_pos:\n            return sentiment_to_score[Sign.POSITIVE]\n\n        return None\n\n\n    def calculate_composition_or_adj_sentiment_sentence(sentence):\n        sentiment_count = 0\n        bigrams = []\n        sentence_bigrams = list(nltk.bigrams(word_tokenize(sentence.lower())))\n        for bigram in sentence_bigrams:\n            sentiment = calculate_composition_or_adj_sentiment(bigram)\n            if sentiment is not None:\n                sentiment_count += sentiment\n                bigrams.append(bigram)\n            else:\n                continue \n\n        # if sentiment_count > 0 then positive\n        return sentiment_count, bigrams\n    \n    def calculate_sentiment_combined(sentence):\n        sentiment_score = 0\n        table = str.maketrans(dict.fromkeys(string.punctuation))\n        cleaned_sentence = sentence.translate(table)  # remove punctuation\n        sentence_bigrams = list(nltk.bigrams(word_tokenize(cleaned_sentence.lower())))\n\n        for bigram in sentence_bigrams:\n            current_sentiment = calculate_bigram_sentiment(bigram)\n            if current_sentiment is None:\n                current_sentiment = calculate_composition_or_adj_sentiment(bigram)\n                if current_sentiment is None:\n                    unigram_sentiment = calculate_unigram_sentiment(bigram)\n                    if unigram_sentiment < - 0.1:\n                        current_sentiment = -1\n                    elif unigram_sentiment > 0.1:\n                        current_sentiment = 1\n                    else:\n                        current_sentiment = 0\n\n            sentiment_score += current_sentiment\n\n        return sentiment_score\n    \n    \n    def convert_score_to_sentiment(score):\n        if score < 0:\n            return Sign.NEGATIVE\n        elif score > 0:\n            return Sign.POSITIVE\n        else:\n            return Sign.NEUTRAL\n\n\n    def calculate_sentence_level_sentiment(comment_by_sentence):\n        \"\"\"\n        :param comment_by_sentence: comment broken down by sentence [sentence1, sentence2, ...]\n                                    each sentence is string.\n        :return: (overall_score, [(sentiment, sentence), ...])\n        \"\"\"\n        sentence_level_sentiment = []\n        overall_score = 0\n        for sentence in comment_by_sentence:\n            score = calculate_sentiment_combined(sentence)\n            overall_score += score\n            sentiment_sentence_pair = (convert_score_to_sentiment(score), sentence)\n            sentence_level_sentiment.append(sentiment_sentence_pair)\n        return overall_score, sentence_level_sentiment\n\n\n    def format_comment_sentiment(comments):\n        \"\"\"\n        :param comments: [comment, comment, ...]\n        :return:{\n                \"comment\": {\n                    \"idx\": int,\n                    \"sentiment\": str,\n                    \"sentence_sentiment\": [\n                        {\n                            \"idx\": int,\n                            \"sentence\": str,\n                            \"sentiment\": str\n                        },\n                    ]\n                }\n            }\n        \"\"\"\n        \n        result = []\n        for i in range(len(comments)):\n            comment_by_sentence = nltk.tokenize.sent_tokenize(comments[i])\n            overall_score, sentence_level_sentiment = calculate_sentence_level_sentiment(comment_by_sentence)\n            \n            sentence_jsons = []\n            for j in range(len(sentence_level_sentiment)):\n                sentence_json = {\n                    \"idx\": j,\n                    \"sentence\": sentence_level_sentiment[j][1],\n                    \"sentiment\": sentence_level_sentiment[j][0]\n                }\n                sentence_jsons.append(sentence_json)\n            \n            comment_json = {\n                \"comment\": {\n                    \"idx\": i,\n                    \"sentiment\": convert_score_to_sentiment(overall_score),\n                    \"sentence_sentiment\": sentence_jsons\n                }\n            }\n            \n            result.append(comment_json)\n    \n        return json.dumps(result)\n    \n    \n    def score(payload):\n        '''\n        :param payload: dictionary, expects the key 'values' (list of comments).\n        :return: JSON serializable object with format\n                [\n                    {\n                        \"comment\": {\n                            \"idx\": int,\n                            \"sentiment\": str,\n                            \"sentence_sentiment\": [\n                                {\n                                    \"idx\": int,\n                                    \"sentence\": str,\n                                    \"sentiment\": str\n                                },\n                            ]\n                        }\n                    },\n                ]\n        '''\n        comments = payload['input_data'][0]['values'].get('test') \n        score_response = {'predictions': [{ 'values': format_comment_sentiment(comments)}]}  \n        return score_response\n        \n    \n    return score", "execution_count": 14, "outputs": []}, {"metadata": {"id": "b6242cf9-b43c-4c14-8e1a-da82aa0fdac8"}, "cell_type": "code", "source": "# TODO: Uncomment to test deployable function\n# json.loads(run_sentiment_analysis()({\"values\": comments}))", "execution_count": 15, "outputs": []}, {"metadata": {"id": "97b74a3c-5a5f-4be0-9bf6-ee5bb7c23df9"}, "cell_type": "markdown", "source": "### 1.2.2 Deploy Function <a class=\"anchor\" id=\"1-2-2\"></a>"}, {"metadata": {"id": "52c9e9b70fec425fba5b64a49e6e9456"}, "cell_type": "code", "source": "# Store function details\n\nmeta_data = {\n    client.repository.FunctionMetaNames.NAME : 'Function for Sentiment Analysis',\n    client.repository.FunctionMetaNames.TAGS : ['sentiment_analysis_function_tag'],\n    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: software_spec_id\n\n}\n\nfunction_details = client.repository.store_function( meta_props=meta_data, function=run_sentiment_analysis )", "execution_count": 16, "outputs": []}, {"metadata": {"id": "f8e9923f6a904d7f806717dd1bd43ca2"}, "cell_type": "code", "source": "# Get function id\nfunction_id = function_details[\"metadata\"][\"id\"]", "execution_count": 17, "outputs": []}, {"metadata": {"id": "07cdb968478b4fe089f7c933f06f45f9"}, "cell_type": "code", "source": "meta_props = {\n    client.deployments.ConfigurationMetaNames.NAME: 'Sentiment Analysis Deployment',\n   client.deployments.ConfigurationMetaNames.TAGS : ['sentiment_analysis_function_deployment_tag'],\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\n# deploy the function\nfunction_deployment_details = client.deployments.create(artifact_uid=function_id, meta_props=meta_props)", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '19c61b69-7d69-4735-8ce5-1df006df5105' started\n\n#######################################################################################\n\n\ninitializing....\nready\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='c77d930a-7392-4569-a88c-cb8696bbef5f'\n------------------------------------------------------------------------------------------------\n\n\n", "name": "stdout"}]}, {"metadata": {"id": "63c67e73-e69a-4cba-9245-e9320f3e50ba"}, "cell_type": "code", "source": "# Get deployment id for clustering\nsentiment_deployment_id = client.deployments.get_uid(function_deployment_details)", "execution_count": 19, "outputs": []}, {"metadata": {"id": "c176d8e0-ac06-4eb6-88ba-618b439cb7f2"}, "cell_type": "markdown", "source": "### 1.2.3 Test Deployed Function <a class=\"anchor\" id=\"1-2-3\"></a>"}, {"metadata": {"id": "629bb99a-7da5-450a-a1d6-cf0e0fc0898f"}, "cell_type": "markdown", "source": "After successfully deploying, test the deployed function."}, {"metadata": {"id": "6a2402ec-a878-4fe9-aeb6-1d286ab7e901"}, "cell_type": "code", "source": "payload = {client.deployments.ScoringMetaNames.INPUT_DATA: [{'values': \n                                                             {\"test\": comments}} \n                                                           ]}\n\n\n# Send data to deployment for processing\nclient.deployments.score(sentiment_deployment_id, payload)", "execution_count": 20, "outputs": [{"output_type": "execute_result", "execution_count": 20, "data": {"text/plain": "{'predictions': [{'values': '[{\"comment\": {\"idx\": 0, \"sentiment\": \"Negative\", \"sentence_sentiment\": [{\"idx\": 0, \"sentence\": \"I bought several items: socks, shirt, sweater.\", \"sentiment\": \"Negative\"}, {\"idx\": 1, \"sentence\": \"By far my most favorite was the shirt because it is so soft.\", \"sentiment\": \"Negative\"}, {\"idx\": 2, \"sentence\": \"However, the sweater and socks missed the mark.\", \"sentiment\": \"Negative\"}]}}, {\"comment\": {\"idx\": 1, \"sentiment\": \"Positive\", \"sentence_sentiment\": [{\"idx\": 0, \"sentence\": \"My order arrived several days late.\", \"sentiment\": \"Negative\"}, {\"idx\": 1, \"sentence\": \"But when I contacted customer serivce they were very helpful and refunded me.\", \"sentiment\": \"Positive\"}]}}, {\"comment\": {\"idx\": 2, \"sentiment\": \"Negative\", \"sentence_sentiment\": [{\"idx\": 0, \"sentence\": \"Horrible, horrible customer service, I have never met such rude people.\", \"sentiment\": \"Negative\"}, {\"idx\": 1, \"sentence\": \"Why is it so bad?\", \"sentiment\": \"Negative\"}, {\"idx\": 2, \"sentence\": \"Would not recommend at all.\", \"sentiment\": \"Negative\"}]}}, {\"comment\": {\"idx\": 3, \"sentiment\": \"Neutral\", \"sentence_sentiment\": [{\"idx\": 0, \"sentence\": \"Everything I ordered arrived on time and looked exactly like in the pictures!\", \"sentiment\": \"Negative\"}, {\"idx\": 1, \"sentence\": \"This company has high quality products.\", \"sentiment\": \"Positive\"}]}}, {\"comment\": {\"idx\": 4, \"sentiment\": \"Positive\", \"sentence_sentiment\": [{\"idx\": 0, \"sentence\": \"I bought somethings on sale, they were a great deal.\", \"sentiment\": \"Positive\"}, {\"idx\": 1, \"sentence\": \"Will be buying more next time.\", \"sentiment\": \"Neutral\"}]}}]'}]}"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}